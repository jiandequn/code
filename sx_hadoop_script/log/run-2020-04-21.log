2020-04-21 16:27:35 开始备份hive待处理日志数据...
2020-04-21 16:27:35 Last login: Tue Apr 21 15:45:09 2020 from 10.10.30.233

2020-04-21 16:27:35 su hadoop

2020-04-21 16:27:35 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:27:35 [hadoop@hadoop-01 root]$ 
2020-04-21 16:27:35 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:27:35 hadoop fs -rm -R -f hdfs://192.168.15.21:90 00/data/hive/backup/

2020-04-21 16:27:36 2020-04-21 16:27:38,822 INFO fs.TrashPolicyDefault: Moved: 'hdfs://192.168.15.21:9000/data/hive/backup' to trash at: hdfs://192.168.15.21:9000/user/hadoop/.Trash/Current/data/hive/backup1587457658811

2020-04-21 16:27:37 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:27:37 2020
 3 2020-04-21 16:27:37 23
2020-04-21 16:27:37 Last login: Tue Apr 21 16:27:37 2020 from 10.10.30.233

2020-04-21 16:27:37 su hadoop

2020-04-21 16:27:37 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:27:37 [hadoop@hadoop-01 root]$ 
2020-04-21 16:27:37 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:27:37 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-23

2020-04-21 16:27:39 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:27:39 Last login: Tue Apr 21 16:27:39 2020 from 10.10.30.233

2020-04-21 16:27:39 su hadoop

2020-04-21 16:27:39 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:27:39 [hadoop@hadoop-01 root]$ 
2020-04-21 16:27:39 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:27:39 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/23/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-23

2020-04-21 16:27:46 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:27:46 Last login: Tue Apr 21 16:27:41 2020 from 10.10.30.233

2020-04-21 16:27:46 su hadoop

2020-04-21 16:27:46 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:27:46 [hadoop@hadoop-01 root]$ 
2020-04-21 16:27:46 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:27:46 cat loadLogs.hive

2020-04-21 16:27:46 cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:27:47 Last login: Tue Apr 21 16:27:48 2020 from 10.10.30.233

2020-04-21 16:27:47 su hadoop

2020-04-21 16:27:47 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:27:47 [hadoop@hadoop-01 root]$ 
2020-04-21 16:27:47 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:27:47 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-23 -d day=23 -d year=2020 -f hive/load Logs.hive

2020-04-21 16:27:47 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-21 16:27:48 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:27:50 Hive Session ID = dcde3fb3-ef20-4f62-8364-1e36b0984ef3

2020-04-21 16:27:50 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:27:56 Hive Session ID = 86e98f9a-8b27-4c04-9200-9627cb51dbc2

2020-04-21 16:27:57 OK
Time taken: 0.789 seconds

2020-04-21 16:27:58 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=23)

2020-04-21 16:28:03 OK
Time taken: 6.085 seconds

2020-04-21 16:28:04 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:04 备份hive待处理日志数据已完成。
2020-04-21 16:28:04 2020
 3 2020-04-21 16:28:04 24
2020-04-21 16:28:04 Last login: Tue Apr 21 16:27:49 2020 from 10.10.30.233

2020-04-21 16:28:04 su hadoop

2020-04-21 16:28:04 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:28:04 [hadoop@hadoop-01 root]$ 
2020-04-21 16:28:04 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:04 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-24

2020-04-21 16:28:06 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:06 Last login: Tue Apr 21 16:28:06 2020 from 10.10.30.233

2020-04-21 16:28:06 su hadoop

2020-04-21 16:28:06 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:28:06 [hadoop@hadoop-01 root]$ 
2020-04-21 16:28:06 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:06 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/24/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-24

2020-04-21 16:28:13 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:14 Last login: Tue Apr 21 16:28:08 2020 from 10.10.30.233

2020-04-21 16:28:14 su hadoop

2020-04-21 16:28:14 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:28:14 [hadoop@hadoop-01 root]$ 
2020-04-21 16:28:14 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:14 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:14 Last login: Tue Apr 21 16:28:16 2020 from 10.10.30.233

2020-04-21 16:28:14 su hadoop

2020-04-21 16:28:14 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:28:14 [hadoop@hadoop-01 root]$ 
2020-04-21 16:28:14 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:14 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-24 -d day=24 -d year=2020 -f hive/load Logs.hive

2020-04-21 16:28:15 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-21 16:28:15 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:28:17 Hive Session ID = 88e862d2-75ef-49a1-bc82-8e8fbfb0ed3b

2020-04-21 16:28:17 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:28:24 Hive Session ID = 1ca1473e-9b6a-4f6e-b9b6-31b68b4767f2

2020-04-21 16:28:24 OK
Time taken: 0.788 seconds

2020-04-21 16:28:26 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=24)

2020-04-21 16:28:29 OK
Time taken: 4.743 seconds

2020-04-21 16:28:30 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:30 备份hive待处理日志数据已完成。
2020-04-21 16:28:30 2020
 3 2020-04-21 16:28:30 25
2020-04-21 16:28:30 Last login: Tue Apr 21 16:28:16 2020 from 10.10.30.233

2020-04-21 16:28:30 su hadoop

2020-04-21 16:28:30 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:28:30 [hadoop@hadoop-01 root]$ 
2020-04-21 16:28:30 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:30 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-25

2020-04-21 16:28:32 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:32 Last login: Tue Apr 21 16:28:32 2020 from 10.10.30.233

2020-04-21 16:28:32 su hadoop

2020-04-21 16:28:32 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:28:32 [hadoop@hadoop-01 root]$ 
2020-04-21 16:28:32 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:32 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/25/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-25

2020-04-21 16:28:41 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:41 Last login: Tue Apr 21 16:28:34 2020 from 10.10.30.233

2020-04-21 16:28:41 su hadoop

2020-04-21 16:28:41 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:28:41 [hadoop@hadoop-01 root]$ 
2020-04-21 16:28:41 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:41 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:42 Last login: Tue Apr 21 16:28:43 2020 from 10.10.30.233

2020-04-21 16:28:42 su hadoop

2020-04-21 16:28:42 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:28:42 [hadoop@hadoop-01 root]$ 
2020-04-21 16:28:42 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:28:42 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-25 -d day=25 -d year=2020 -f hive/load Logs.hive

2020-04-21 16:28:42 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-21 16:28:43 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:28:45 Hive Session ID = 75fd106d-8d74-4f1a-a25a-5942320c5250

Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:28:51 Hive Session ID = a1b32be7-b55c-4932-9053-916b22691198

2020-04-21 16:28:52 OK
Time taken: 0.802 seconds

2020-04-21 16:28:53 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=25)

2020-04-21 16:29:00 OK
Time taken: 7.921 seconds

2020-04-21 16:29:01 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:29:01 备份hive待处理日志数据已完成。
2020-04-21 16:29:01 开始处理对yn_logs进行分类处理
2020-04-21 16:29:01 Last login: Tue Apr 21 16:28:44 2020 from 10.10.30.233

2020-04-21 16:29:01 su hadoop

2020-04-21 16:29:01 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:29:01 [hadoop@hadoop-01 root]$ 
2020-04-21 16:29:01 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:29:01 cat eventsType.hive
cat: eventsType.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:29:01 Last login: Tue Apr 21 16:29:03 2020 from 10.10.30.233

2020-04-21 16:29:01 su hadoop

2020-04-21 16:29:01 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:29:01 [hadoop@hadoop-01 root]$ 
2020-04-21 16:29:01 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:29:01 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/eventsType.hive

2020-04-21 16:29:02 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-21 16:29:02 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:29:04 Hive Session ID = b7cd3b12-111a-47ad-aa28-8cd6389b170a

2020-04-21 16:29:04 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:29:11 Hive Session ID = a5204404-c614-47e8-963d-0a33c33f1c25

2020-04-21 16:29:12 OK
Time taken: 0.841 seconds

2020-04-21 16:29:15 Query ID = hadoop_20200421162914_9a244a3d-5d4e-4e65-b317-10cc56b7dcbb
Total jobs = 3

2020-04-21 16:29:21 2020-04-21 16:29:23	Dump the side-table for tag: 1 with group count: 9 into file: file:/tmp/hive/b7cd3b12-111a-47ad-aa28-8cd6389b170a/hive_2020-04-21_16-29-14_363_6828143334351816821-1/-local-10003/HashTable-Stage-9/MapJoin-mapfile01--.hashtable

2020-04-21 16:29:22 Execution completed successfully
MapredLocal task succeeded

2020-04-21 16:29:23 Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-21 16:29:25 Starting Job = job_1587346943308_0278, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0278/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0278

2020-04-21 16:29:31 Hadoop job information for Stage-9: number of mappers: 4; number of reducers: 0

2020-04-21 16:29:31 2020-04-21 16:29:33,907 Stage-9 map = 0%,  reduce = 0%

2020-04-21 16:29:49 2020-04-21 16:29:51,574 Stage-9 map = 33%,  reduce = 0%, Cumulative CPU 68.53 sec

2020-04-21 16:29:54 2020-04-21 16:29:56,729 Stage-9 map = 37%,  reduce = 0%, Cumulative CPU 74.83 sec

2020-04-21 16:29:55 2020-04-21 16:29:57,764 Stage-9 map = 53%,  reduce = 0%, Cumulative CPU 93.97 sec

2020-04-21 16:30:01 2020-04-21 16:30:02,919 Stage-9 map = 58%,  reduce = 0%, Cumulative CPU 100.34 sec

2020-04-21 16:30:02 2020-04-21 16:30:03,969 Stage-9 map = 66%,  reduce = 0%, Cumulative CPU 113.71 sec

2020-04-21 16:30:07 2020-04-21 16:30:09,127 Stage-9 map = 76%,  reduce = 0%, Cumulative CPU 126.21 sec

2020-04-21 16:30:08 2020-04-21 16:30:10,153 Stage-9 map = 80%,  reduce = 0%, Cumulative CPU 132.55 sec

2020-04-21 16:30:13 2020-04-21 16:30:15,283 Stage-9 map = 89%,  reduce = 0%, Cumulative CPU 145.3 sec

2020-04-21 16:30:14 2020-04-21 16:30:16,312 Stage-9 map = 93%,  reduce = 0%, Cumulative CPU 151.51 sec

2020-04-21 16:30:17 2020-04-21 16:30:19,401 Stage-9 map = 96%,  reduce = 0%, Cumulative CPU 155.29 sec

2020-04-21 16:30:18 2020-04-21 16:30:20,430 Stage-9 map = 100%,  reduce = 0%, Cumulative CPU 160.2 sec

2020-04-21 16:30:19 MapReduce Total cumulative CPU time: 2 minutes 40 seconds 200 msec
Ended Job = job_1587346943308_0278

2020-04-21 16:30:21 Stage-4 is filtered out by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is selected by condition resolver.

2020-04-21 16:30:23 Launching Job 3 out of 3

2020-04-21 16:30:23 Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-21 16:30:24 Starting Job = job_1587346943308_0279, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0279/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0279

2020-04-21 16:30:30 Hadoop job information for Stage-5: number of mappers: 8; number of reducers: 0

2020-04-21 16:30:30 2020-04-21 16:30:32,652 Stage-5 map = 0%,  reduce = 0%

2020-04-21 16:30:40 2020-04-21 16:30:41,973 Stage-5 map = 13%,  reduce = 0%, Cumulative CPU 4.08 sec

2020-04-21 16:30:41 2020-04-21 16:30:43,002 Stage-5 map = 63%,  reduce = 0%, Cumulative CPU 30.83 sec

2020-04-21 16:30:44 2020-04-21 16:30:46,093 Stage-5 map = 75%,  reduce = 0%, Cumulative CPU 34.07 sec

2020-04-21 16:30:45 2020-04-21 16:30:47,126 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 39.29 sec

2020-04-21 16:30:46 MapReduce Total cumulative CPU time: 39 seconds 290 msec

2020-04-21 16:30:46 Ended Job = job_1587346943308_0279

2020-04-21 16:30:47 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-21_16-29-14_363_6828143334351816821-1/-ext-10000/events_type=operateResumePoint/y=2020/m=03/d=23

2020-04-21 16:30:47 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-21_16-29-14_363_6828143334351816821-1/-ext-10000/events_type=operationDetails/y=2020/m=03/d=23

2020-04-21 16:30:47 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-21_16-29-14_363_6828143334351816821-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=23

2020-04-21 16:30:47 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-21_16-29-14_363_6828143334351816821-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=24

2020-04-21 16:30:49 Loading data to table yn_hadoop.events_type_log partition (events_type=null, y=null, m=null, d=null)

2020-04-21 16:30:49 


2020-04-21 16:30:51 	 Time taken to load dynamic partitions: 1.617 seconds
	 Time taken for adding to write entity : 0.003 seconds

2020-04-21 16:30:53 MapReduce Jobs Launched: 
Stage-Stage-9: Map: 4   Cumulative CPU: 160.2 sec   HDFS Read: 893623459 HDFS Write: 338538889 SUCCESS
Stage-Stage-5: Map: 8   Cumulative CPU: 39.29 sec   HDFS Read: 138892202 HDFS Write: 138924953 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 19 seconds 490 msec
OK

2020-04-21 16:30:53 Time taken: 101.139 seconds

2020-04-21 16:30:54 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:30:54 yn_logs进行分类处理已完成
2020-04-21 16:30:54 开始处理对日统计数据
2020-04-21 16:30:54 -- query hive不支持
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \
--create-hive-table \
--hive-table user_info \
--hive-overwrite
2020-04-21 16:30:54 Last login: Tue Apr 21 16:29:03 2020 from 10.10.30.233

2020-04-21 16:30:54 su hadoop

2020-04-21 16:30:54 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:30:54 [hadoop@hadoop-01 root]$ 
2020-04-21 16:30:54 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:30:54 -- query hive不支持
bash: --: 未找到命令

2020-04-21 16:30:54 [hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> --create-hive-table \
> --hive-table user_info \
> --hive-overwrite

2020-04-21 16:30:54 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-21 16:30:54 2020-04-21 16:30:56,836 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-21 16:30:56,862 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-21 16:30:54 2020-04-21 16:30:56,909 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-21 16:30:56,917 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-21 16:30:56,919 INFO tool.CodeGenTool: Beginning code generation

2020-04-21 16:30:55 2020-04-21 16:30:57,141 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-21 16:30:57,145 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-21 16:30:55 2020-04-21 16:30:57,161 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-21 16:30:56 注: /tmp/sqoop-hadoop/compile/2873f3498246d3c4c886f2012bee164a/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-21 16:30:58,145 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/2873f3498246d3c4c886f2012bee164a/user_info.jar

2020-04-21 16:30:56 2020-04-21 16:30:58,833 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-21 16:30:58,850 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-21 16:30:58,851 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-21 16:30:58,859 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-21 16:30:58,862 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-21 16:30:56 2020-04-21 16:30:58,890 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-21 16:30:57 2020-04-21 16:30:59,348 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0280

2020-04-21 16:30:59 2020-04-21 16:31:01,035 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-21 16:30:59 2020-04-21 16:31:01,482 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-21 16:30:59 2020-04-21 16:31:01,691 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0280
2020-04-21 16:31:01,692 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-21 16:30:59 2020-04-21 16:31:01,889 INFO conf.Configuration: resource-types.xml not found
2020-04-21 16:31:01,890 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-21 16:31:00 2020-04-21 16:31:01,955 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0280

2020-04-21 16:31:00 2020-04-21 16:31:01,990 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0280/
2020-04-21 16:31:01,991 INFO mapreduce.Job: Running job: job_1587346943308_0280

2020-04-21 16:31:06 2020-04-21 16:31:08,078 INFO mapreduce.Job: Job job_1587346943308_0280 running in uber mode : false
2020-04-21 16:31:08,080 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-21 16:31:10 2020-04-21 16:31:12,138 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-21 16:31:11 2020-04-21 16:31:13,155 INFO mapreduce.Job: Job job_1587346943308_0280 completed successfully

2020-04-21 16:31:11 2020-04-21 16:31:13,265 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2619
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2619
		Total vcore-milliseconds taken by all map tasks=2619
		Total megabyte-milliseconds taken by all map tasks=2681856
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=51
		CPU time spen
2020-04-21 16:31:11 t (ms)=1190
		Physical memory (bytes) snapshot=243073024
		Virtual memory (bytes) snapshot=2447654912
		Total committed heap usage (bytes)=191365120
		Peak Map Physical memory (bytes)=243073024
		Peak Map Virtual memory (bytes)=2447654912
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572
2020-04-21 16:31:13,273 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 14.3736 seconds (42.0266 KB/sec)
2020-04-21 16:31:13,277 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-21 16:31:13,277 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info
2020-04-21 16:31:13,296 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-21 16:31:13,298 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-21 16:31:11 2020-04-21 16:31:13,323 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-21 16:31:13,330 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-21 16:31:11 2020-04-21 16:31:13,498 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-21 16:31:12 2020-04-21 16:31:14,085 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-21 16:31:14,113 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-21 16:31:12 2020-04-21 16:31:14,529 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-21 16:31:14,530 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-21 16:31:14,530 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-21 16:31:14,530 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-21 16:31:14,535 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:31:14 2020-04-21 16:31:16,538 INFO hive.HiveImport: Hive Session ID = 56fa4fc4-ccd0-4747-b782-51013ce69059

2020-04-21 16:31:14 2020-04-21 16:31:16,587 INFO hive.HiveImport: 
2020-04-21 16:31:16,587 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:31:21 2020-04-21 16:31:23,750 INFO hive.HiveImport: Hive Session ID = 35829977-8ec4-4df3-b491-c44b800a85dc

2020-04-21 16:31:23 2020-04-21 16:31:25,482 INFO hive.HiveImport: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table hive.yn_hadoop.user_info already exists)

2020-04-21 16:31:24 2020-04-21 16:31:25,994 ERROR tool.ImportTool: Import failed: java.io.IOException: Hive exited with status 1
	at org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:384)
	at org.apache.sqoop.hive.HiveImport.executeScript(HiveImport.java:337)
	at org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:241)
	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:537)
	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)


2020-04-21 16:31:24 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:31:24 Last login: Tue Apr 21 16:30:56 2020 from 10.10.30.233

2020-04-21 16:31:24 su hadoop

2020-04-21 16:31:24 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:31:24 [hadoop@hadoop-01 root]$ 
2020-04-21 16:31:24 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:31:24 cat statistic/RegionbyDay.hive
cat: statistic/RegionbyDay.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:31:24 Last login: Tue Apr 21 16:31:26 2020 from 10.10.30.233

2020-04-21 16:31:24 su hadoop

2020-04-21 16:31:25 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:31:25 [hadoop@hadoop-01 root]$ 
2020-04-21 16:31:25 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:31:25 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-21 16:31:25 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-21 16:31:26 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:31:28 Hive Session ID = 6fc9c99b-b233-40aa-a2c6-22b747429284

2020-04-21 16:31:28 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:31:34 Hive Session ID = f9313d59-43a1-406c-9d6f-ca0fa49beeb6

2020-04-21 16:31:35 OK
Time taken: 0.798 seconds

2020-04-21 16:31:37 Query ID = hadoop_20200421163137_828755a6-0070-4ca3-a621-c5863e57d948
Total jobs = 3
Launching Job 1 out of 3

2020-04-21 16:31:37 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:31:39 Starting Job = job_1587346943308_0281, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0281/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0281

2020-04-21 16:31:44 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-21 16:31:44 2020-04-21 16:31:46,882 Stage-1 map = 0%,  reduce = 0%

2020-04-21 16:31:55 2020-04-21 16:31:57,294 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 6.92 sec

2020-04-21 16:31:56 2020-04-21 16:31:58,330 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.71 sec

2020-04-21 16:32:00 2020-04-21 16:32:02,485 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 19.54 sec

2020-04-21 16:32:01 MapReduce Total cumulative CPU time: 19 seconds 540 msec

2020-04-21 16:32:01 Ended Job = job_1587346943308_0281

2020-04-21 16:32:01 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:32:02 Starting Job = job_1587346943308_0282, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0282/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0282

2020-04-21 16:32:11 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-21 16:32:11 2020-04-21 16:32:13,846 Stage-2 map = 0%,  reduce = 0%

2020-04-21 16:32:18 2020-04-21 16:32:20,048 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.78 sec

2020-04-21 16:32:26 2020-04-21 16:32:28,325 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.05 sec

2020-04-21 16:32:27 MapReduce Total cumulative CPU time: 7 seconds 50 msec
Ended Job = job_1587346943308_0282

2020-04-21 16:32:27 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-21 16:32:27 


2020-04-21 16:32:27 	 Time taken to load dynamic partitions: 0.351 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:32:28 Starting Job = job_1587346943308_0283, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0283/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0283

2020-04-21 16:32:37 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-21 16:32:37 2020-04-21 16:32:39,644 Stage-4 map = 0%,  reduce = 0%

2020-04-21 16:32:43 2020-04-21 16:32:45,869 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec

2020-04-21 16:32:50 2020-04-21 16:32:52,075 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.94 sec

2020-04-21 16:32:51 MapReduce Total cumulative CPU time: 3 seconds 940 msec
Ended Job = job_1587346943308_0283

2020-04-21 16:32:51 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 19.54 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.05 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.94 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 30 seconds 530 msec
OK

2020-04-21 16:32:51 Time taken: 76.364 seconds

2020-04-21 16:32:52 Query ID = hadoop_20200421163253_69684a65-fe51-468d-b5c0-4ee1545fb94c
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:32:52 Starting Job = job_1587346943308_0284, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0284/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0284

2020-04-21 16:33:02 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-21 16:33:02 2020-04-21 16:33:04,904 Stage-1 map = 0%,  reduce = 0%

2020-04-21 16:33:11 2020-04-21 16:33:13,248 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 6.06 sec

2020-04-21 16:33:12 2020-04-21 16:33:14,281 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.63 sec

2020-04-21 16:33:18 2020-04-21 16:33:20,493 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.03 sec

2020-04-21 16:33:20 MapReduce Total cumulative CPU time: 17 seconds 30 msec
Ended Job = job_1587346943308_0284

2020-04-21 16:33:20 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:33:21 Starting Job = job_1587346943308_0285, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0285/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0285

2020-04-21 16:33:30 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2020-04-21 16:33:32,350 Stage-2 map = 0%,  reduce = 0%

2020-04-21 16:33:36 2020-04-21 16:33:38,557 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.62 sec

2020-04-21 16:33:44 2020-04-21 16:33:46,803 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.7 sec

2020-04-21 16:33:45 MapReduce Total cumulative CPU time: 7 seconds 700 msec
Ended Job = job_1587346943308_0285

2020-04-21 16:33:45 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-21 16:33:46 


2020-04-21 16:33:46 	 Time taken to load dynamic partitions: 0.281 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:33:46 Starting Job = job_1587346943308_0286, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0286/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0286

2020-04-21 16:33:56 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-21 16:33:58,700 Stage-4 map = 0%,  reduce = 0%

2020-04-21 16:34:02 2020-04-21 16:34:04,892 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.51 sec

2020-04-21 16:34:09 2020-04-21 16:34:11,084 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.26 sec

2020-04-21 16:34:10 MapReduce Total cumulative CPU time: 4 seconds 260 msec
Ended Job = job_1587346943308_0286

2020-04-21 16:34:10 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 17.03 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.7 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.26 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 28 seconds 990 msec
OK
Time taken: 78.601 seconds

2020-04-21 16:34:12 Query ID = hadoop_20200421163412_8681cb7e-5ac4-4545-94d3-a4d2e1caf07a
Total jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:34:13 Starting Job = job_1587346943308_0287, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0287/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0287

2020-04-21 16:34:21 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-04-21 16:34:23,497 Stage-1 map = 0%,  reduce = 0%

2020-04-21 16:34:27 2020-04-21 16:34:29,784 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.0 sec

2020-04-21 16:34:35 2020-04-21 16:34:36,993 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.66 sec

2020-04-21 16:34:36 MapReduce Total cumulative CPU time: 7 seconds 660 msec

2020-04-21 16:34:36 Ended Job = job_1587346943308_0287

2020-04-21 16:34:36 Launching Job 2 out of 4

2020-04-21 16:34:36 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:34:36 Starting Job = job_1587346943308_0288, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0288/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0288

2020-04-21 16:34:46 Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1

2020-04-21 16:34:46 2020-04-21 16:34:48,736 Stage-5 map = 0%,  reduce = 0%

2020-04-21 16:34:53 2020-04-21 16:34:54,939 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 4.04 sec

2020-04-21 16:34:59 2020-04-21 16:35:01,119 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 7.39 sec

2020-04-21 16:35:00 MapReduce Total cumulative CPU time: 7 seconds 390 msec

2020-04-21 16:35:00 Ended Job = job_1587346943308_0288

2020-04-21 16:35:00 Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:35:00 Starting Job = job_1587346943308_0289, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0289/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0289

2020-04-21 16:35:10 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1

2020-04-21 16:35:10 2020-04-21 16:35:12,884 Stage-2 map = 0%,  reduce = 0%

2020-04-21 16:35:17 2020-04-21 16:35:19,078 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.53 sec

2020-04-21 16:35:24 2020-04-21 16:35:26,312 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.49 sec

2020-04-21 16:35:25 MapReduce Total cumulative CPU time: 6 seconds 490 msec
Ended Job = job_1587346943308_0289

2020-04-21 16:35:25 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-21 16:35:25 


2020-04-21 16:35:25 	 Time taken to load dynamic partitions: 0.262 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:35:26 Starting Job = job_1587346943308_0290, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0290/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0290

2020-04-21 16:35:36 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-21 16:35:36 2020-04-21 16:35:38,342 Stage-4 map = 0%,  reduce = 0%

2020-04-21 16:35:42 2020-04-21 16:35:44,532 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.42 sec

2020-04-21 16:35:47 2020-04-21 16:35:49,693 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.94 sec

2020-04-21 16:35:48 MapReduce Total cumulative CPU time: 3 seconds 940 msec
Ended Job = job_1587346943308_0290

2020-04-21 16:35:49 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.66 sec   HDFS Read: 877473 HDFS Write: 182 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 7.39 sec   HDFS Read: 482570 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.49 sec   HDFS Read: 19803 HDFS Write: 706 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.94 sec   HDFS Read: 14036 HDFS Write: 559 SUCCESS
Total MapReduce CPU Time Spent: 25 seconds 480 msec
OK
Time taken: 98.571 seconds

2020-04-21 16:35:49 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:36:31 开始备份hive待处理日志数据...
2020-04-21 16:36:32 Last login: Tue Apr 21 16:31:26 2020 from 10.10.30.233

2020-04-21 16:36:32 su hadoop

2020-04-21 16:36:32 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:36:32 [hadoop@hadoop-01 root]$ 
2020-04-21 16:36:32 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:36:32 hadoop fs -rm -R -f hdfs://192.168.15.21:90 00/data/hive/backup/

2020-04-21 16:36:33 2020-04-21 16:36:35,411 INFO fs.TrashPolicyDefault: Moved: 'hdfs://192.168.15.21:9000/data/hive/backup' to trash at: hdfs://192.168.15.21:9000/user/hadoop/.Trash/Current/data/hive/backup1587458195399

2020-04-21 16:36:33 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:36:34 2020
 3 2020-04-21 16:36:34 23
2020-04-21 16:36:34 Last login: Tue Apr 21 16:36:34 2020 from 10.10.30.233

2020-04-21 16:36:34 su hadoop

2020-04-21 16:36:34 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:36:34 [hadoop@hadoop-01 root]$ 
2020-04-21 16:36:34 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:36:34 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-23

2020-04-21 16:36:35 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:36:36 Last login: Tue Apr 21 16:36:36 2020 from 10.10.30.233

2020-04-21 16:36:36 su hadoop

2020-04-21 16:36:36 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:36:36 [hadoop@hadoop-01 root]$ 
2020-04-21 16:36:36 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:36:36 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/23/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-23

2020-04-21 16:36:43 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:36:43 Last login: Tue Apr 21 16:36:38 2020 from 10.10.30.233

2020-04-21 16:36:43 su hadoop

2020-04-21 16:36:43 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:36:43 [hadoop@hadoop-01 root]$ 
2020-04-21 16:36:43 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:36:43 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:36:44 Last login: Tue Apr 21 16:36:45 2020 from 10.10.30.233

2020-04-21 16:36:44 su hadoop

2020-04-21 16:36:44 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:36:44 [hadoop@hadoop-01 root]$ 
2020-04-21 16:36:44 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:36:44 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-23 -d day=23 -d year=2020 -f hive/load Logs.hive

2020-04-21 16:36:44 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-21 16:36:45 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:36:47 Hive Session ID = d1e134ee-a842-4426-8dc0-0fc168859c10

2020-04-21 16:36:47 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:36:53 Hive Session ID = 9c7c3ad6-eaac-4b1f-a176-e99714f367e5

2020-04-21 16:36:54 OK
Time taken: 0.784 seconds

2020-04-21 16:36:55 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=23)

2020-04-21 16:37:00 OK
Time taken: 6.15 seconds

2020-04-21 16:37:01 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:01 备份hive待处理日志数据已完成。
2020-04-21 16:37:01 2020
 3 2020-04-21 16:37:01 24
2020-04-21 16:37:01 Last login: Tue Apr 21 16:36:45 2020 from 10.10.30.233

2020-04-21 16:37:01 su hadoop

2020-04-21 16:37:01 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:37:01 [hadoop@hadoop-01 root]$ 
2020-04-21 16:37:01 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:01 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-24

2020-04-21 16:37:03 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:03 Last login: Tue Apr 21 16:37:03 2020 from 10.10.30.233

2020-04-21 16:37:03 su hadoop

2020-04-21 16:37:03 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:37:03 [hadoop@hadoop-01 root]$ 
2020-04-21 16:37:03 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:03 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/24/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-24

2020-04-21 16:37:10 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:10 Last login: Tue Apr 21 16:37:05 2020 from 10.10.30.233

2020-04-21 16:37:10 su hadoop

2020-04-21 16:37:10 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:37:10 [hadoop@hadoop-01 root]$ 
2020-04-21 16:37:10 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:10 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:11 Last login: Tue Apr 21 16:37:12 2020 from 10.10.30.233

2020-04-21 16:37:11 su hadoop

2020-04-21 16:37:11 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:37:11 [hadoop@hadoop-01 root]$ 
2020-04-21 16:37:11 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:11 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-24 -d day=24 -d year=2020 -f hive/load Logs.hive

2020-04-21 16:37:11 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-21 16:37:12 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:37:14 Hive Session ID = e281a138-5a89-48b3-a5ae-4658f41de940

2020-04-21 16:37:14 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:37:20 Hive Session ID = 84b1fc04-dd58-4a45-a196-e3cc8309e770

2020-04-21 16:37:21 OK
Time taken: 0.78 seconds

2020-04-21 16:37:22 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=24)

2020-04-21 16:37:27 OK
Time taken: 6.156 seconds

2020-04-21 16:37:28 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:28 备份hive待处理日志数据已完成。
2020-04-21 16:37:28 2020
 3 2020-04-21 16:37:28 25
2020-04-21 16:37:28 Last login: Tue Apr 21 16:37:12 2020 from 10.10.30.233

2020-04-21 16:37:28 su hadoop

2020-04-21 16:37:28 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:37:28 [hadoop@hadoop-01 root]$ 
2020-04-21 16:37:28 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:28 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-25

2020-04-21 16:37:30 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:30 Last login: Tue Apr 21 16:37:30 2020 from 10.10.30.233

2020-04-21 16:37:30 su hadoop

2020-04-21 16:37:30 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:37:30 [hadoop@hadoop-01 root]$ 
2020-04-21 16:37:30 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:30 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/25/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-25

2020-04-21 16:37:38 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:38 Last login: Tue Apr 21 16:37:32 2020 from 10.10.30.233

2020-04-21 16:37:38 su hadoop

2020-04-21 16:37:38 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:37:38 [hadoop@hadoop-01 root]$ 
2020-04-21 16:37:38 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:38 cat loadLogs.hive

2020-04-21 16:37:38 cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:38 Last login: Tue Apr 21 16:37:40 2020 from 10.10.30.233

2020-04-21 16:37:38 su hadoop

2020-04-21 16:37:38 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:37:38 [hadoop@hadoop-01 root]$ 
2020-04-21 16:37:38 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:38 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-25 -d day=25 -d year=2020 -f hive/load Logs.hive

2020-04-21 16:37:39 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-21 16:37:39 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:37:41 Hive Session ID = 0fd0ad90-d02c-454c-b342-0dffe034013c

2020-04-21 16:37:41 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:37:48 Hive Session ID = 066c2719-50c5-46bb-8879-aff8d6aa70d7

2020-04-21 16:37:49 OK

2020-04-21 16:37:49 Time taken: 0.778 seconds

2020-04-21 16:37:50 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=25)

2020-04-21 16:37:55 OK
Time taken: 6.588 seconds

2020-04-21 16:37:56 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:56 备份hive待处理日志数据已完成。
2020-04-21 16:37:56 开始处理对yn_logs进行分类处理
2020-04-21 16:37:56 Last login: Tue Apr 21 16:37:40 2020 from 10.10.30.233

2020-04-21 16:37:56 su hadoop

2020-04-21 16:37:56 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:37:56 [hadoop@hadoop-01 root]$ 
2020-04-21 16:37:56 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:56 cat eventsType.hive
cat: eventsType.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:56 Last login: Tue Apr 21 16:37:58 2020 from 10.10.30.233

2020-04-21 16:37:56 su hadoop

2020-04-21 16:37:56 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:37:56 [hadoop@hadoop-01 root]$ 
2020-04-21 16:37:56 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:37:56 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/eventsType.hive

2020-04-21 16:37:57 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-21 16:37:57 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:38:00 Hive Session ID = 7f7f04b9-865a-4dd9-a4c5-c5ec95f97b63

Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:38:08 Hive Session ID = 3ed8dab3-0f8f-4b84-a2f7-7c8ef5102b97

2020-04-21 16:38:08 OK
Time taken: 0.77 seconds

2020-04-21 16:38:12 Query ID = hadoop_20200421163810_580a05b7-5998-4185-97ae-6daf8bc552d3
Total jobs = 3

2020-04-21 16:38:12 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:38:17 2020-04-21 16:38:19	Starting to launch local task to process map join;	maximum memory = 239075328

2020-04-21 16:38:18 2020-04-21 16:38:19	Dump the side-table for tag: 1 with group count: 9 into file: file:/tmp/hive/7f7f04b9-865a-4dd9-a4c5-c5ec95f97b63/hive_2020-04-21_16-38-10_919_3868883429213625220-1/-local-10003/HashTable-Stage-9/MapJoin-mapfile01--.hashtable
2020-04-21 16:38:19	Uploaded 1 File to: file:/tmp/hive/7f7f04b9-865a-4dd9-a4c5-c5ec95f97b63/hive_2020-04-21_16-38-10_919_3868883429213625220-1/-local-10003/HashTable-Stage-9/MapJoin-mapfile01--.hashtable (488 bytes)
2020-04-21 16:38:19	End of local task; Time Taken: 0.478 sec.

2020-04-21 16:38:18 Execution completed successfully
MapredLocal task succeeded

2020-04-21 16:38:20 Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-21 16:38:22 Starting Job = job_1587346943308_0291, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0291/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0291

2020-04-21 16:38:28 Hadoop job information for Stage-9: number of mappers: 4; number of reducers: 0

2020-04-21 16:38:29 2020-04-21 16:38:30,993 Stage-9 map = 0%,  reduce = 0%

2020-04-21 16:38:46 2020-04-21 16:38:48,667 Stage-9 map = 33%,  reduce = 0%, Cumulative CPU 67.26 sec

2020-04-21 16:38:50 2020-04-21 16:38:52,800 Stage-9 map = 42%,  reduce = 0%, Cumulative CPU 71.74 sec

2020-04-21 16:38:51 2020-04-21 16:38:53,839 Stage-9 map = 46%,  reduce = 0%, Cumulative CPU 78.2 sec

2020-04-21 16:38:52 2020-04-21 16:38:54,863 Stage-9 map = 55%,  reduce = 0%, Cumulative CPU 91.15 sec

2020-04-21 16:38:58 2020-04-21 16:39:00,013 Stage-9 map = 59%,  reduce = 0%, Cumulative CPU 97.52 sec

2020-04-21 16:38:59 2020-04-21 16:39:01,043 Stage-9 map = 67%,  reduce = 0%, Cumulative CPU 110.44 sec

2020-04-21 16:39:04 2020-04-21 16:39:06,219 Stage-9 map = 71%,  reduce = 0%, Cumulative CPU 116.89 sec

2020-04-21 16:39:05 2020-04-21 16:39:07,245 Stage-9 map = 81%,  reduce = 0%, Cumulative CPU 129.55 sec

2020-04-21 16:39:10 2020-04-21 16:39:12,372 Stage-9 map = 88%,  reduce = 0%, Cumulative CPU 142.27 sec

2020-04-21 16:39:11 2020-04-21 16:39:13,401 Stage-9 map = 94%,  reduce = 0%, Cumulative CPU 149.17 sec

2020-04-21 16:39:14 2020-04-21 16:39:16,493 Stage-9 map = 100%,  reduce = 0%, Cumulative CPU 156.81 sec

2020-04-21 16:39:15 MapReduce Total cumulative CPU time: 2 minutes 36 seconds 810 msec

2020-04-21 16:39:15 Ended Job = job_1587346943308_0291

2020-04-21 16:39:16 Stage-4 is filtered out by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is selected by condition resolver.

2020-04-21 16:39:18 Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-21 16:39:19 Starting Job = job_1587346943308_0292, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0292/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0292

2020-04-21 16:39:26 Hadoop job information for Stage-5: number of mappers: 8; number of reducers: 0
2020-04-21 16:39:28,221 Stage-5 map = 0%,  reduce = 0%

2020-04-21 16:39:35 2020-04-21 16:39:37,517 Stage-5 map = 25%,  reduce = 0%, Cumulative CPU 10.78 sec

2020-04-21 16:39:36 2020-04-21 16:39:38,544 Stage-5 map = 50%,  reduce = 0%, Cumulative CPU 23.77 sec

2020-04-21 16:39:37 2020-04-21 16:39:39,589 Stage-5 map = 63%,  reduce = 0%, Cumulative CPU 30.48 sec

2020-04-21 16:39:39 2020-04-21 16:39:41,661 Stage-5 map = 75%,  reduce = 0%, Cumulative CPU 33.75 sec

2020-04-21 16:39:40 2020-04-21 16:39:42,684 Stage-5 map = 88%,  reduce = 0%, Cumulative CPU 36.68 sec

2020-04-21 16:39:41 2020-04-21 16:39:43,714 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 38.85 sec

2020-04-21 16:39:42 MapReduce Total cumulative CPU time: 38 seconds 850 msec
Ended Job = job_1587346943308_0292

2020-04-21 16:39:44 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-21_16-38-10_919_3868883429213625220-1/-ext-10000/events_type=operateResumePoint/y=2020/m=03/d=23
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-21_16-38-10_919_3868883429213625220-1/-ext-10000/events_type=operationDetails/y=2020/m=03/d=23
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-21_16-38-10_919_3868883429213625220-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=23

2020-04-21 16:39:44 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-21_16-38-10_919_3868883429213625220-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=24

2020-04-21 16:39:46 Loading data to table yn_hadoop.events_type_log partition (events_type=null, y=null, m=null, d=null)

2020-04-21 16:39:46 


2020-04-21 16:39:47 	 Time taken to load dynamic partitions: 1.489 seconds
	 Time taken for adding to write entity : 0.003 seconds

2020-04-21 16:39:50 MapReduce Jobs Launched: 
Stage-Stage-9: Map: 4   Cumulative CPU: 156.81 sec   HDFS Read: 893623459 HDFS Write: 338538895 SUCCESS
Stage-Stage-5: Map: 8   Cumulative CPU: 38.85 sec   HDFS Read: 138892202 HDFS Write: 138924953 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 15 seconds 660 msec
OK

2020-04-21 16:39:50 Time taken: 101.163 seconds

2020-04-21 16:39:50 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:39:50 yn_logs进行分类处理已完成
2020-04-21 16:39:50 开始处理对日统计数据
2020-04-21 16:39:50 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \

--hive-table user_info \
--hive-overwrite
2020-04-21 16:39:50 Last login: Tue Apr 21 16:37:58 2020 from 10.10.30.233

2020-04-21 16:39:50 su hadoop

2020-04-21 16:39:50 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:39:50 [hadoop@hadoop-01 root]$ 
2020-04-21 16:39:50 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:39:50 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> 

2020-04-21 16:39:50 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-21 16:39:51 2020-04-21 16:39:53,385 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-21 16:39:53,412 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-21 16:39:51 2020-04-21 16:39:53,458 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-21 16:39:53,467 INFO manager.SqlManager: Using default fetchSize of 1000

2020-04-21 16:39:51 2020-04-21 16:39:53,469 INFO tool.CodeGenTool: Beginning code generation

2020-04-21 16:39:51 2020-04-21 16:39:53,685 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-21 16:39:53,689 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-21 16:39:51 2020-04-21 16:39:53,704 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-21 16:39:52 注: /tmp/sqoop-hadoop/compile/5d60e7d2df1c7f7ff9d2ed9c2cb9e882/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-21 16:39:54,703 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/5d60e7d2df1c7f7ff9d2ed9c2cb9e882/user_info.jar

2020-04-21 16:39:53 2020-04-21 16:39:55,405 INFO tool.ImportTool: Destination directory user_info deleted.
2020-04-21 16:39:55,411 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-21 16:39:55,412 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-21 16:39:55,420 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-21 16:39:55,423 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-21 16:39:55,437 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-21 16:39:53 2020-04-21 16:39:55,907 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0293

2020-04-21 16:39:56 2020-04-21 16:39:58,467 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-21 16:39:56 2020-04-21 16:39:58,507 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-21 16:39:56 2020-04-21 16:39:58,762 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0293
2020-04-21 16:39:58,764 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-21 16:39:56 2020-04-21 16:39:58,930 INFO conf.Configuration: resource-types.xml not found
2020-04-21 16:39:58,931 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-21 16:39:57 2020-04-21 16:39:58,977 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0293

2020-04-21 16:39:57 2020-04-21 16:39:59,003 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0293/
2020-04-21 16:39:59,003 INFO mapreduce.Job: Running job: job_1587346943308_0293

2020-04-21 16:40:03 2020-04-21 16:40:05,095 INFO mapreduce.Job: Job job_1587346943308_0293 running in uber mode : false
2020-04-21 16:40:05,097 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-21 16:40:08 2020-04-21 16:40:10,162 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-21 16:40:10,172 INFO mapreduce.Job: Job job_1587346943308_0293 completed successfully

2020-04-21 16:40:08 2020-04-21 16:40:10,285 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2651
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2651
		Total vcore-milliseconds taken by all map tasks=2651
		Total megabyte-milliseconds taken by all map tasks=2714624
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=51
		CPU time spen
2020-04-21 16:40:08 t (ms)=1230
		Physical memory (bytes) snapshot=243343360
		Virtual memory (bytes) snapshot=2446278656
		Total committed heap usage (bytes)=191889408
		Peak Map Physical memory (bytes)=243343360
		Peak Map Virtual memory (bytes)=2446278656
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572
2020-04-21 16:40:10,293 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 14.8436 seconds (40.6958 KB/sec)
2020-04-21 16:40:10,297 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-21 16:40:10,297 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info
2020-04-21 16:40:10,308 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-21 16:40:10,310 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-21 16:40:08 2020-04-21 16:40:10,329 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-21 16:40:10,336 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-21 16:40:08 2020-04-21 16:40:10,505 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-21 16:40:09 2020-04-21 16:40:11,059 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-21 16:40:11,086 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-21 16:40:09 2020-04-21 16:40:11,526 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-21 16:40:11,526 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-21 16:40:11,526 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-21 16:40:11,526 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-21 16:40:11,531 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:40:11 2020-04-21 16:40:13,535 INFO hive.HiveImport: Hive Session ID = e7500394-dab4-4e3c-97a5-e3eb515d3e00

2020-04-21 16:40:11 2020-04-21 16:40:13,583 INFO hive.HiveImport: 
2020-04-21 16:40:13,583 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:40:18 2020-04-21 16:40:20,716 INFO hive.HiveImport: Hive Session ID = d8f764ff-6a4e-431b-98f9-a6932396650c

2020-04-21 16:40:20 2020-04-21 16:40:22,161 INFO hive.HiveImport: OK
2020-04-21 16:40:22,171 INFO hive.HiveImport: Time taken: 1.387 seconds

2020-04-21 16:40:20 2020-04-21 16:40:22,442 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-21 16:40:21 2020-04-21 16:40:22,921 INFO hive.HiveImport: OK
2020-04-21 16:40:22,931 INFO hive.HiveImport: Time taken: 0.739 seconds

2020-04-21 16:40:21 2020-04-21 16:40:23,430 INFO hive.HiveImport: Hive import complete.

2020-04-21 16:40:21 2020-04-21 16:40:23,443 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.

2020-04-21 16:40:21 [hadoop@hadoop-01 sx_hadoop_script]$ --hive-table user_info \
> --hive-overwrite
bash: --hive-table: 未找到命令
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:40:22 Last login: Tue Apr 21 16:39:52 2020 from 10.10.30.233

2020-04-21 16:40:22 su hadoop

2020-04-21 16:40:22 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:40:22 [hadoop@hadoop-01 root]$ 
2020-04-21 16:40:22 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:40:22 cat statistic/RegionbyDay.hive
cat: statistic/RegionbyDay.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:40:22 Last login: Tue Apr 21 16:40:24 2020 from 10.10.30.233

2020-04-21 16:40:22 su hadoop

2020-04-21 16:40:22 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:40:22 [hadoop@hadoop-01 root]$ 
2020-04-21 16:40:22 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:40:22 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-21 16:40:23 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-21 16:40:23 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-21 16:40:23 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-21 16:40:25 Hive Session ID = cc547cf2-132e-4a40-b5da-983ef747835b

2020-04-21 16:40:25 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-21 16:40:32 Hive Session ID = c210c006-5729-4cbb-9a44-1525c06f07d7

2020-04-21 16:40:33 OK
Time taken: 0.825 seconds

2020-04-21 16:40:35 Query ID = hadoop_20200421164035_9c1edbdb-ce4b-4a0e-a96b-138912738120
Total jobs = 3
Launching Job 1 out of 3

2020-04-21 16:40:35 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:40:37 Starting Job = job_1587346943308_0294, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0294/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0294

2020-04-21 16:40:43 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-21 16:40:43 2020-04-21 16:40:45,294 Stage-1 map = 0%,  reduce = 0%

2020-04-21 16:40:51 2020-04-21 16:40:53,622 Stage-1 map = 50%,  reduce = 0%

2020-04-21 16:40:54 2020-04-21 16:40:56,778 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.84 sec

2020-04-21 16:41:00 2020-04-21 16:41:01,971 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 19.86 sec

2020-04-21 16:41:01 MapReduce Total cumulative CPU time: 19 seconds 860 msec

2020-04-21 16:41:01 Ended Job = job_1587346943308_0294

2020-04-21 16:41:01 Launching Job 2 out of 3

2020-04-21 16:41:01 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:41:01 Starting Job = job_1587346943308_0295, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0295/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0295

2020-04-21 16:41:11 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-21 16:41:11 2020-04-21 16:41:13,869 Stage-2 map = 0%,  reduce = 0%

2020-04-21 16:41:18 2020-04-21 16:41:20,119 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.7 sec

2020-04-21 16:41:25 2020-04-21 16:41:27,359 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.36 sec

2020-04-21 16:41:26 MapReduce Total cumulative CPU time: 7 seconds 360 msec
Ended Job = job_1587346943308_0295

2020-04-21 16:41:26 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-21 16:41:26 


2020-04-21 16:41:26 	 Time taken to load dynamic partitions: 0.372 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:41:27 Starting Job = job_1587346943308_0296, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0296/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0296

2020-04-21 16:41:37 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-21 16:41:37 2020-04-21 16:41:39,607 Stage-4 map = 0%,  reduce = 0%

2020-04-21 16:41:42 2020-04-21 16:41:44,865 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.54 sec

2020-04-21 16:41:49 2020-04-21 16:41:51,059 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.43 sec

2020-04-21 16:41:50 MapReduce Total cumulative CPU time: 4 seconds 430 msec
Ended Job = job_1587346943308_0296

2020-04-21 16:41:50 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 19.86 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.36 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.43 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 650 msec
OK

2020-04-21 16:41:50 Time taken: 77.685 seconds

2020-04-21 16:41:51 Query ID = hadoop_20200421164152_b95f8de5-9214-4958-8167-67875d35b414
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:41:52 Starting Job = job_1587346943308_0297, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0297/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0297

2020-04-21 16:42:00 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-21 16:42:00 2020-04-21 16:42:02,490 Stage-1 map = 0%,  reduce = 0%

2020-04-21 16:42:08 2020-04-21 16:42:10,752 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 5.25 sec

2020-04-21 16:42:10 2020-04-21 16:42:12,820 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.64 sec

2020-04-21 16:42:16 2020-04-21 16:42:18,047 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.86 sec

2020-04-21 16:42:18 MapReduce Total cumulative CPU time: 16 seconds 860 msec

2020-04-21 16:42:18 Ended Job = job_1587346943308_0297

2020-04-21 16:42:18 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:42:18 Starting Job = job_1587346943308_0298, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0298/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0298

2020-04-21 16:42:28 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-21 16:42:28 2020-04-21 16:42:30,859 Stage-2 map = 0%,  reduce = 0%

2020-04-21 16:42:35 2020-04-21 16:42:37,069 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.68 sec

2020-04-21 16:42:42 2020-04-21 16:42:44,296 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.91 sec

2020-04-21 16:42:43 MapReduce Total cumulative CPU time: 7 seconds 910 msec

2020-04-21 16:42:43 Ended Job = job_1587346943308_0298

2020-04-21 16:42:43 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-21 16:42:43 


2020-04-21 16:42:43 	 Time taken to load dynamic partitions: 0.227 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:42:44 Starting Job = job_1587346943308_0299, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0299/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0299

2020-04-21 16:42:54 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-21 16:42:56,334 Stage-4 map = 0%,  reduce = 0%

2020-04-21 16:43:00 2020-04-21 16:43:02,539 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.48 sec

2020-04-21 16:43:06 2020-04-21 16:43:08,729 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.14 sec

2020-04-21 16:43:08 MapReduce Total cumulative CPU time: 4 seconds 140 msec
Ended Job = job_1587346943308_0299

2020-04-21 16:43:09 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 16.86 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.91 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.14 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 28 seconds 910 msec
OK
Time taken: 78.176 seconds

2020-04-21 16:43:11 Query ID = hadoop_20200421164311_100b9310-91b6-483e-bfab-e72d12831954
Total jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:43:12 Starting Job = job_1587346943308_0300, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0300/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0300

2020-04-21 16:43:19 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-21 16:43:19 2020-04-21 16:43:21,095 Stage-1 map = 0%,  reduce = 0%

2020-04-21 16:43:26 2020-04-21 16:43:28,365 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.99 sec

2020-04-21 16:43:32 2020-04-21 16:43:34,562 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.31 sec

2020-04-21 16:43:33 MapReduce Total cumulative CPU time: 10 seconds 310 msec
Ended Job = job_1587346943308_0300

2020-04-21 16:43:33 Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:43:34 Starting Job = job_1587346943308_0301, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0301/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0301

2020-04-21 16:43:44 Hadoop job information for Stage-5: number of mappers: 2; number of reducers: 1

2020-04-21 16:43:44 2020-04-21 16:43:46,280 Stage-5 map = 0%,  reduce = 0%

2020-04-21 16:43:51 2020-04-21 16:43:53,505 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 7.14 sec

2020-04-21 16:43:58 2020-04-21 16:44:00,731 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 10.79 sec

2020-04-21 16:43:59 MapReduce Total cumulative CPU time: 10 seconds 790 msec
Ended Job = job_1587346943308_0301
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:44:00 Starting Job = job_1587346943308_0302, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0302/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0302

2020-04-21 16:44:10 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1

2020-04-21 16:44:10 2020-04-21 16:44:12,905 Stage-2 map = 0%,  reduce = 0%

2020-04-21 16:44:17 2020-04-21 16:44:19,195 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.59 sec

2020-04-21 16:44:24 2020-04-21 16:44:26,423 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.31 sec

2020-04-21 16:44:25 MapReduce Total cumulative CPU time: 6 seconds 310 msec
Ended Job = job_1587346943308_0302

2020-04-21 16:44:25 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-21 16:44:25 


2020-04-21 16:44:25 	 Time taken to load dynamic partitions: 0.242 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-21 16:44:26 Starting Job = job_1587346943308_0303, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0303/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0303

2020-04-21 16:44:36 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-21 16:44:38,481 Stage-4 map = 0%,  reduce = 0%

2020-04-21 16:44:42 2020-04-21 16:44:44,694 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.4 sec

2020-04-21 16:44:48 2020-04-21 16:44:50,886 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.82 sec

2020-04-21 16:44:50 MapReduce Total cumulative CPU time: 3 seconds 820 msec
Ended Job = job_1587346943308_0303

2020-04-21 16:44:50 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 10.31 sec   HDFS Read: 883908 HDFS Write: 182 SUCCESS
Stage-Stage-5: Map: 2  Reduce: 1   Cumulative CPU: 10.79 sec   HDFS Read: 489429 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.31 sec   HDFS Read: 19803 HDFS Write: 706 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.82 sec   HDFS Read: 14036 HDFS Write: 559 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 230 msec
OK

2020-04-21 16:44:50 Time taken: 101.032 seconds

2020-04-21 16:44:50 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:44:50 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--driver com.mysql.jdbc.Driver \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-21 16:44:50 Last login: Tue Apr 21 16:40:24 2020 from 10.10.30.233

2020-04-21 16:44:50 su hadoop

2020-04-21 16:44:50 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:44:50 [hadoop@hadoop-01 root]$ 
2020-04-21 16:44:50 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:44:50 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --driver com.mysql.jdbc.Driver \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-21 16:44:51 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-21 16:44:51 2020-04-21 16:44:53,496 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-21 16:44:53,521 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-21 16:44:51 2020-04-21 16:44:53,566 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-21 16:44:51 2020-04-21 16:44:53,575 INFO manager.SqlManager: Using default fetchSize of 1000

2020-04-21 16:44:51 2020-04-21 16:44:53,800 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-21 16:44:51 2020-04-21 16:44:53,802 INFO tool.CodeGenTool: Beginning code generation

2020-04-21 16:44:51 2020-04-21 16:44:53,825 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-21 16:44:51 2020-04-21 16:44:53,843 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-21 16:44:52 注: /tmp/sqoop-hadoop/compile/8b69b17fb6ee7fa210739d69e32b7f42/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-21 16:44:54,825 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/8b69b17fb6ee7fa210739d69e32b7f42/user_info.jar

2020-04-21 16:44:52 2020-04-21 16:44:54,833 ERROR tool.ExportTool: Error during export: 
Mixed update/insert is not supported against the target database yet
	at org.apache.sqoop.manager.ConnManager.upsertTable(ConnManager.java:684)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-21 16:44:52 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:44:52 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--driver com.mysql.jdbc.Driver \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-21 16:44:53 Last login: Tue Apr 21 16:44:52 2020 from 10.10.30.233

2020-04-21 16:44:53 su hadoop

2020-04-21 16:44:53 [root@hadoop-01 ~]# su hadoop

2020-04-21 16:44:53 [hadoop@hadoop-01 root]$ 
2020-04-21 16:44:53 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-21 16:44:53 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --driver com.mysql.jdbc.Driver \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-21 16:44:53 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-21 16:44:53 2020-04-21 16:44:55,740 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-21 16:44:55,765 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-21 16:44:53 2020-04-21 16:44:55,813 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-21 16:44:53 2020-04-21 16:44:55,822 INFO manager.SqlManager: Using default fetchSize of 1000

2020-04-21 16:44:54 2020-04-21 16:44:56,015 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-21 16:44:56,017 INFO tool.CodeGenTool: Beginning code generation
2020-04-21 16:44:56,040 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-21 16:44:54 2020-04-21 16:44:56,058 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-21 16:44:55 注: /tmp/sqoop-hadoop/compile/7280434b225c82d10839bf2577a123b7/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-21 16:44:57,089 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/7280434b225c82d10839bf2577a123b7/user_info.jar
2020-04-21 16:44:57,096 ERROR tool.ExportTool: Error during export: 
Mixed update/insert is not supported against the target database yet
	at org.apache.sqoop.manager.ConnManager.upsertTable(ConnManager.java:684)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)
[hadoop@hadoop-01 sx_hadoo
2020-04-21 16:44:55 p_script]$ 
2020-04-21 16:44:55 日统计数据已完成
