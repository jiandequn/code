2020-04-22 09:19:47 开始备份hive待处理日志数据...
2020-04-22 09:19:47 Last login: Wed Apr 22 09:10:32 2020 from 10.10.30.233

2020-04-22 09:19:47 su hadoop

2020-04-22 09:19:47 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:19:47 [hadoop@hadoop-01 root]$ 
2020-04-22 09:19:47 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:19:47 hadoop fs -rm -R -f hdfs://192.168.15.21:90 00/data/hive/backup/

2020-04-22 09:19:48 2020-04-22 09:19:51,945 INFO fs.TrashPolicyDefault: Moved: 'hdfs://192.168.15.21:9000/data/hive/backup' to trash at: hdfs://192.168.15.21:9000/user/hadoop/.Trash/Current/data/hive/backup

2020-04-22 09:19:49 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:19:49 2020
 3 2020-04-22 09:19:49 23
2020-04-22 09:19:49 Last login: Wed Apr 22 09:19:50 2020 from 10.10.30.233

2020-04-22 09:19:49 su hadoop

2020-04-22 09:19:49 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:19:49 [hadoop@hadoop-01 root]$ 
2020-04-22 09:19:49 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:19:49 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-23

2020-04-22 09:19:51 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:19:51 Last login: Wed Apr 22 09:19:52 2020 from 10.10.30.233

2020-04-22 09:19:51 su hadoop

2020-04-22 09:19:51 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:19:51 [hadoop@hadoop-01 root]$ 
2020-04-22 09:19:51 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:19:51 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/23/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-23

2020-04-22 09:19:58 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:20:32 开始备份hive待处理日志数据...
2020-04-22 09:20:33 Last login: Wed Apr 22 09:19:54 2020 from 10.10.30.233

2020-04-22 09:20:33 su hadoop

2020-04-22 09:20:33 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:20:33 [hadoop@hadoop-01 root]$ 
2020-04-22 09:20:33 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:20:33 hadoop fs -rm -R -f hdfs://192.168.15.21:90 00/data/hive/backup/

2020-04-22 09:20:34 2020-04-22 09:20:37,516 INFO fs.TrashPolicyDefault: Moved: 'hdfs://192.168.15.21:9000/data/hive/backup' to trash at: hdfs://192.168.15.21:9000/user/hadoop/.Trash/Current/data/hive/backup1587518437506

2020-04-22 09:20:34 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:20:34 2020
 3 2020-04-22 09:20:34 23
2020-04-22 09:20:35 Last login: Wed Apr 22 09:20:36 2020 from 10.10.30.233

2020-04-22 09:20:35 su hadoop

2020-04-22 09:20:35 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:20:35 [hadoop@hadoop-01 root]$ 
2020-04-22 09:20:35 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:20:35 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-23

2020-04-22 09:20:36 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:20:37 Last login: Wed Apr 22 09:20:38 2020 from 10.10.30.233

2020-04-22 09:20:37 su hadoop

2020-04-22 09:20:37 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:20:37 [hadoop@hadoop-01 root]$ 
2020-04-22 09:20:37 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:20:37 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/23/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-23

2020-04-22 09:20:42 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:20:42 Last login: Wed Apr 22 09:20:40 2020 from 10.10.30.233

2020-04-22 09:20:42 su hadoop
[root@hadoop-01 ~]# su hadoop

2020-04-22 09:20:42 [hadoop@hadoop-01 root]$ 
2020-04-22 09:20:42 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:20:42 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:20:43 Last login: Wed Apr 22 09:20:45 2020 from 10.10.30.233

2020-04-22 09:20:43 su hadoop

2020-04-22 09:20:43 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:20:43 [hadoop@hadoop-01 root]$ 
2020-04-22 09:20:43 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:20:43 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-23 -d day=23 -d year=2020 -f hive/load Logs.hive

2020-04-22 09:20:43 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:20:44 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:20:46 Hive Session ID = 3023e48a-892a-4a2a-9188-e399ecc4d923

2020-04-22 09:20:46 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:20:52 Hive Session ID = 439a82dc-7c6a-44bf-9f33-45abe17a5f0b

2020-04-22 09:20:53 OK
Time taken: 0.808 seconds

2020-04-22 09:20:54 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=23)

2020-04-22 09:20:59 OK
Time taken: 6.046 seconds

2020-04-22 09:21:00 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:00 备份hive待处理日志数据已完成。
2020-04-22 09:21:00 2020
 3 2020-04-22 09:21:00 24
2020-04-22 09:21:00 Last login: Wed Apr 22 09:20:46 2020 from 10.10.30.233

2020-04-22 09:21:00 su hadoop

2020-04-22 09:21:00 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:21:00 [hadoop@hadoop-01 root]$ 
2020-04-22 09:21:00 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:00 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-24

2020-04-22 09:21:02 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:02 Last login: Wed Apr 22 09:21:03 2020 from 10.10.30.233

2020-04-22 09:21:02 su hadoop

2020-04-22 09:21:02 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:21:02 [hadoop@hadoop-01 root]$ 
2020-04-22 09:21:02 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:02 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/24/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-24

2020-04-22 09:21:07 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:08 Last login: Wed Apr 22 09:21:05 2020 from 10.10.30.233

2020-04-22 09:21:08 su hadoop

2020-04-22 09:21:08 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:21:08 [hadoop@hadoop-01 root]$ 
2020-04-22 09:21:08 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:08 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:08 Last login: Wed Apr 22 09:21:11 2020 from 10.10.30.233

2020-04-22 09:21:08 su hadoop

2020-04-22 09:21:08 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:21:08 [hadoop@hadoop-01 root]$ 
2020-04-22 09:21:08 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:08 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-24 -d day=24 -d year=2020 -f hive/load Logs.hive

2020-04-22 09:21:09 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:21:09 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:21:11 Hive Session ID = 9ba01a94-4121-4ef9-b503-b22f78f5996f

2020-04-22 09:21:11 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:21:18 Hive Session ID = 6ea887e9-81ea-4111-8299-74ffb47f3f35

2020-04-22 09:21:19 OK
Time taken: 0.766 seconds

2020-04-22 09:21:20 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=24)

2020-04-22 09:21:25 OK
Time taken: 6.376 seconds

2020-04-22 09:21:25 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:25 备份hive待处理日志数据已完成。
2020-04-22 09:21:25 2020
 3 2020-04-22 09:21:25 25
2020-04-22 09:21:26 Last login: Wed Apr 22 09:21:11 2020 from 10.10.30.233

2020-04-22 09:21:26 su hadoop

2020-04-22 09:21:26 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:21:26 [hadoop@hadoop-01 root]$ 
2020-04-22 09:21:26 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:26 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-25

2020-04-22 09:21:28 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:28 Last login: Wed Apr 22 09:21:29 2020 from 10.10.30.233

2020-04-22 09:21:28 su hadoop

2020-04-22 09:21:28 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:21:28 [hadoop@hadoop-01 root]$ 
2020-04-22 09:21:28 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:28 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/25/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-25

2020-04-22 09:21:39 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:39 Last login: Wed Apr 22 09:21:31 2020 from 10.10.30.233

2020-04-22 09:21:39 su hadoop

2020-04-22 09:21:39 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:21:39 [hadoop@hadoop-01 root]$ 
2020-04-22 09:21:39 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:39 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:39 Last login: Wed Apr 22 09:21:42 2020 from 10.10.30.233

2020-04-22 09:21:39 su hadoop

2020-04-22 09:21:39 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:21:39 [hadoop@hadoop-01 root]$ 
2020-04-22 09:21:39 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:39 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-25 -d day=25 -d year=2020 -f hive/load Logs.hive

2020-04-22 09:21:40 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:21:40 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:21:43 Hive Session ID = 686c0e70-3728-49cc-91fa-a37712fae607

2020-04-22 09:21:43 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:21:49 Hive Session ID = 1bf40289-3903-40a4-b864-63416d3309cd

2020-04-22 09:21:50 OK
Time taken: 0.819 seconds

2020-04-22 09:21:51 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=25)

2020-04-22 09:21:57 OK
Time taken: 7.117 seconds

2020-04-22 09:21:58 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:58 备份hive待处理日志数据已完成。
2020-04-22 09:21:58 开始处理对yn_logs进行分类处理
2020-04-22 09:21:58 Last login: Wed Apr 22 09:21:42 2020 from 10.10.30.233

2020-04-22 09:21:58 su hadoop

2020-04-22 09:21:58 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:21:58 [hadoop@hadoop-01 root]$ 
2020-04-22 09:21:58 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:58 cat eventsType.hive
cat: eventsType.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:58 Last login: Wed Apr 22 09:22:01 2020 from 10.10.30.233

2020-04-22 09:21:58 su hadoop

2020-04-22 09:21:59 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:21:59 [hadoop@hadoop-01 root]$ 
2020-04-22 09:21:59 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:21:59 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/eventsType.hive

2020-04-22 09:21:59 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:22:00 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:22:02 Hive Session ID = afd53ecd-a6d9-4e44-bd27-ec827d09f09f

2020-04-22 09:22:02 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:22:08 Hive Session ID = 35349e50-a396-49b2-8be9-b7ef0a100e64

2020-04-22 09:22:09 OK
Time taken: 0.795 seconds

2020-04-22 09:22:12 Query ID = hadoop_20200422092212_f7250449-8a57-40e0-8842-252282f99e49
Total jobs = 3

2020-04-22 09:22:13 SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 09:22:18 2020-04-22 09:22:21	Starting to launch local task to process map join;	maximum memory = 239075328

2020-04-22 09:22:19 Execution completed successfully
MapredLocal task succeeded

2020-04-22 09:22:20 Launching Job 1 out of 3

2020-04-22 09:22:20 Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-22 09:22:22 Starting Job = job_1587346943308_0304, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0304/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0304

2020-04-22 09:22:27 Hadoop job information for Stage-9: number of mappers: 4; number of reducers: 0

2020-04-22 09:22:27 2020-04-22 09:22:30,740 Stage-9 map = 0%,  reduce = 0%

2020-04-22 09:22:45 2020-04-22 09:22:48,366 Stage-9 map = 6%,  reduce = 0%, Cumulative CPU 17.23 sec

2020-04-22 09:22:46 2020-04-22 09:22:49,394 Stage-9 map = 12%,  reduce = 0%, Cumulative CPU 35.18 sec

2020-04-22 09:22:47 2020-04-22 09:22:50,424 Stage-9 map = 32%,  reduce = 0%, Cumulative CPU 67.52 sec

2020-04-22 09:22:51 2020-04-22 09:22:54,539 Stage-9 map = 35%,  reduce = 0%, Cumulative CPU 74.16 sec

2020-04-22 09:22:52 2020-04-22 09:22:55,568 Stage-9 map = 50%,  reduce = 0%, Cumulative CPU 85.7 sec

2020-04-22 09:22:53 2020-04-22 09:22:56,598 Stage-9 map = 55%,  reduce = 0%, Cumulative CPU 92.16 sec

2020-04-22 09:22:57 2020-04-22 09:23:00,716 Stage-9 map = 59%,  reduce = 0%, Cumulative CPU 98.62 sec

2020-04-22 09:22:58 2020-04-22 09:23:01,765 Stage-9 map = 63%,  reduce = 0%, Cumulative CPU 105.31 sec

2020-04-22 09:22:59 2020-04-22 09:23:02,789 Stage-9 map = 69%,  reduce = 0%, Cumulative CPU 111.51 sec

2020-04-22 09:23:03 2020-04-22 09:23:06,905 Stage-9 map = 73%,  reduce = 0%, Cumulative CPU 117.77 sec

2020-04-22 09:23:04 2020-04-22 09:23:07,932 Stage-9 map = 81%,  reduce = 0%, Cumulative CPU 130.48 sec

2020-04-22 09:23:10 2020-04-22 09:23:13,075 Stage-9 map = 90%,  reduce = 0%, Cumulative CPU 142.19 sec

2020-04-22 09:23:11 2020-04-22 09:23:14,104 Stage-9 map = 94%,  reduce = 0%, Cumulative CPU 148.41 sec

2020-04-22 09:23:14 2020-04-22 09:23:17,182 Stage-9 map = 97%,  reduce = 0%, Cumulative CPU 152.85 sec

2020-04-22 09:23:15 2020-04-22 09:23:18,223 Stage-9 map = 100%,  reduce = 0%, Cumulative CPU 156.48 sec

2020-04-22 09:23:16 MapReduce Total cumulative CPU time: 2 minutes 36 seconds 480 msec

2020-04-22 09:23:16 Ended Job = job_1587346943308_0304

2020-04-22 09:23:16 Stage-4 is filtered out by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is selected by condition resolver.

2020-04-22 09:23:18 Launching Job 3 out of 3

2020-04-22 09:23:18 Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-22 09:23:19 Starting Job = job_1587346943308_0305, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0305/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0305

2020-04-22 09:23:27 Hadoop job information for Stage-5: number of mappers: 9; number of reducers: 0
2020-04-22 09:23:30,495 Stage-5 map = 0%,  reduce = 0%

2020-04-22 09:23:35 2020-04-22 09:23:38,770 Stage-5 map = 11%,  reduce = 0%, Cumulative CPU 5.69 sec

2020-04-22 09:23:37 2020-04-22 09:23:40,840 Stage-5 map = 44%,  reduce = 0%, Cumulative CPU 25.71 sec

2020-04-22 09:23:38 2020-04-22 09:23:41,877 Stage-5 map = 56%,  reduce = 0%, Cumulative CPU 33.71 sec

2020-04-22 09:23:40 2020-04-22 09:23:43,942 Stage-5 map = 67%,  reduce = 0%, Cumulative CPU 37.13 sec

2020-04-22 09:23:42 2020-04-22 09:23:46,007 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 44.72 sec

2020-04-22 09:23:45 MapReduce Total cumulative CPU time: 44 seconds 720 msec
Ended Job = job_1587346943308_0305

2020-04-22 09:23:46 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-22_09-22-12_596_2713298239925400515-1/-ext-10000/events_type=operateResumePoint/y=2020/m=03/d=23

2020-04-22 09:23:46 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-22_09-22-12_596_2713298239925400515-1/-ext-10000/events_type=operationDetails/y=2020/m=03/d=23
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-22_09-22-12_596_2713298239925400515-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=23

2020-04-22 09:23:48 Loading data to table yn_hadoop.events_type_log partition (events_type=null, y=null, m=null, d=null)

2020-04-22 09:23:48 


2020-04-22 09:23:50 	 Time taken to load dynamic partitions: 1.61 seconds
	 Time taken for adding to write entity : 0.003 seconds

2020-04-22 09:23:52 MapReduce Jobs Launched: 
Stage-Stage-9: Map: 4   Cumulative CPU: 156.48 sec   HDFS Read: 893623459 HDFS Write: 338539508 SUCCESS

2020-04-22 09:23:52 Stage-Stage-5: Map: 9   Cumulative CPU: 44.72 sec   HDFS Read: 193904147 HDFS Write: 193928806 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 21 seconds 200 msec
OK

2020-04-22 09:23:52 Time taken: 103.143 seconds

2020-04-22 09:23:53 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:23:53 yn_logs进行分类处理已完成
2020-04-22 09:23:53 开始处理对日统计数据
2020-04-22 09:23:53 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \

--hive-table user_info \
--hive-overwrite
2020-04-22 09:23:53 Last login: Wed Apr 22 09:22:02 2020 from 10.10.30.233

2020-04-22 09:23:53 su hadoop
[root@hadoop-01 ~]# su hadoop

2020-04-22 09:23:53 [hadoop@hadoop-01 root]$ 
2020-04-22 09:23:53 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:23:53 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> 

2020-04-22 09:23:53 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 09:23:54 2020-04-22 09:23:57,069 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 09:23:54 2020-04-22 09:23:57,095 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 09:23:54 2020-04-22 09:23:57,141 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-22 09:23:54 2020-04-22 09:23:57,149 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 09:23:57,151 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 09:23:54 2020-04-22 09:23:57,365 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 09:23:57,372 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:23:54 2020-04-22 09:23:57,400 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 09:23:55 注: /tmp/sqoop-hadoop/compile/8fe4048db0030ae6d688889ace995ac5/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 09:23:58,335 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/8fe4048db0030ae6d688889ace995ac5/user_info.jar

2020-04-22 09:23:56 2020-04-22 09:23:59,075 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.

2020-04-22 09:23:56 2020-04-22 09:23:59,081 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 09:23:59,082 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 09:23:59,089 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 09:23:59,092 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:23:56 2020-04-22 09:23:59,105 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 09:23:56 2020-04-22 09:23:59,640 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0306

2020-04-22 09:23:57 2020-04-22 09:24:00,493 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 09:23:57 2020-04-22 09:24:00,527 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 09:23:57 2020-04-22 09:24:00,684 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0306
2020-04-22 09:24:00,687 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 09:23:57 2020-04-22 09:24:00,911 INFO conf.Configuration: resource-types.xml not found
2020-04-22 09:24:00,911 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 09:23:57 2020-04-22 09:24:00,954 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0306

2020-04-22 09:23:57 2020-04-22 09:24:00,977 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0306/
2020-04-22 09:24:00,978 INFO mapreduce.Job: Running job: job_1587346943308_0306

2020-04-22 09:24:04 2020-04-22 09:24:07,064 INFO mapreduce.Job: Job job_1587346943308_0306 running in uber mode : false
2020-04-22 09:24:07,066 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 09:24:08 2020-04-22 09:24:11,143 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-22 09:24:09 2020-04-22 09:24:12,161 INFO mapreduce.Job: Job job_1587346943308_0306 completed successfully

2020-04-22 09:24:09 2020-04-22 09:24:12,277 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2594
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2594
		Total vcore-milliseconds taken by all map tasks=2594
		Total megabyte-milliseconds taken by all map tasks=2656256
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=59
		CPU time spen
2020-04-22 09:24:09 t (ms)=1270
		Physical memory (bytes) snapshot=241397760
		Virtual memory (bytes) snapshot=2447200256
		Total committed heap usage (bytes)=189267968
		Peak Map Physical memory (bytes)=241397760
		Peak Map Virtual memory (bytes)=2447200256
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572

2020-04-22 09:24:09 2020-04-22 09:24:12,284 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 13.1681 seconds (45.8741 KB/sec)
2020-04-22 09:24:12,287 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-22 09:24:12,287 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info

2020-04-22 09:24:09 2020-04-22 09:24:12,302 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 09:24:12,304 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:24:09 2020-04-22 09:24:12,324 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 09:24:12,332 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 09:24:09 2020-04-22 09:24:12,495 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 09:24:10 2020-04-22 09:24:13,074 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-22 09:24:13,096 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 09:24:10 2020-04-22 09:24:13,532 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 09:24:13,532 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 09:24:13,532 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 09:24:13,532 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 09:24:13,539 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:24:12 2020-04-22 09:24:15,550 INFO hive.HiveImport: Hive Session ID = 31ec9366-a463-4027-95aa-1abda343de15
2020-04-22 09:24:15,601 INFO hive.HiveImport: 
2020-04-22 09:24:15,601 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:24:19 2020-04-22 09:24:22,813 INFO hive.HiveImport: Hive Session ID = cb666571-8924-4935-a1d6-0228e3761156

2020-04-22 09:24:21 2020-04-22 09:24:24,236 INFO hive.HiveImport: OK
2020-04-22 09:24:24,246 INFO hive.HiveImport: Time taken: 1.354 seconds

2020-04-22 09:24:21 2020-04-22 09:24:24,518 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 09:24:22 2020-04-22 09:24:25,054 INFO hive.HiveImport: OK
2020-04-22 09:24:25,061 INFO hive.HiveImport: Time taken: 0.795 seconds

2020-04-22 09:24:22 2020-04-22 09:24:25,567 INFO hive.HiveImport: Hive import complete.
2020-04-22 09:24:25,580 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.

2020-04-22 09:24:22 [hadoop@hadoop-01 sx_hadoop_script]$ --hive-table user_info \
> --hive-overwrite
bash: --hive-table: 未找到命令
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:24:23 Last login: Wed Apr 22 09:23:56 2020 from 10.10.30.233

2020-04-22 09:24:23 su hadoop

2020-04-22 09:24:23 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:24:23 [hadoop@hadoop-01 root]$ 
2020-04-22 09:24:23 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:24:23 cat statistic/RegionbyDay.hive
cat: statistic/RegionbyDay.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:24:23 Last login: Wed Apr 22 09:24:26 2020 from 10.10.30.233

2020-04-22 09:24:23 su hadoop

2020-04-22 09:24:23 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:24:23 [hadoop@hadoop-01 root]$ 
2020-04-22 09:24:23 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:24:23 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 09:24:24 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:24:24 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:24:26 Hive Session ID = 992d904c-a248-44a1-b80e-635e17546cf6

2020-04-22 09:24:26 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:24:33 Hive Session ID = eb8cbb30-8315-461f-b17e-a3b4c57bc584

2020-04-22 09:24:33 OK
Time taken: 0.818 seconds

2020-04-22 09:24:35 Query ID = hadoop_20200422092437_22ff26e9-c67d-420d-a0ab-67e1744b56d4
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 09:24:35 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:24:37 Starting Job = job_1587346943308_0307, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0307/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0307

2020-04-22 09:24:43 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 09:24:43 2020-04-22 09:24:46,353 Stage-1 map = 0%,  reduce = 0%

2020-04-22 09:24:52 2020-04-22 09:24:55,712 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.31 sec

2020-04-22 09:24:53 2020-04-22 09:24:56,760 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.52 sec

2020-04-22 09:24:59 2020-04-22 09:25:02,993 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 19.54 sec

2020-04-22 09:25:01 MapReduce Total cumulative CPU time: 19 seconds 540 msec
Ended Job = job_1587346943308_0307

2020-04-22 09:25:01 Launching Job 2 out of 3

2020-04-22 09:25:01 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:25:02 Starting Job = job_1587346943308_0308, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0308/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0308

2020-04-22 09:25:11 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 09:25:11 2020-04-22 09:25:14,278 Stage-2 map = 0%,  reduce = 0%

2020-04-22 09:25:17 2020-04-22 09:25:20,502 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.15 sec

2020-04-22 09:25:24 2020-04-22 09:25:27,789 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.34 sec

2020-04-22 09:25:25 MapReduce Total cumulative CPU time: 7 seconds 340 msec
Ended Job = job_1587346943308_0308

2020-04-22 09:25:25 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 09:25:25 


2020-04-22 09:25:26 	 Time taken to load dynamic partitions: 0.33 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:25:27 Starting Job = job_1587346943308_0309, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0309/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0309

2020-04-22 09:25:36 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 09:25:36 2020-04-22 09:25:39,381 Stage-4 map = 0%,  reduce = 0%

2020-04-22 09:25:42 2020-04-22 09:25:45,567 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.37 sec

2020-04-22 09:25:47 2020-04-22 09:25:50,740 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.89 sec

2020-04-22 09:25:48 MapReduce Total cumulative CPU time: 3 seconds 890 msec
Ended Job = job_1587346943308_0309

2020-04-22 09:25:49 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 19.54 sec   HDFS Read: 193048749 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.34 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.89 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 30 seconds 770 msec
OK

2020-04-22 09:25:49 Time taken: 75.451 seconds

2020-04-22 09:25:49 Query ID = hadoop_20200422092552_673a06c4-cac9-4477-9a79-787a38233201
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:25:50 Starting Job = job_1587346943308_0310, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0310/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0310

2020-04-22 09:25:59 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1

2020-04-22 09:25:59 2020-04-22 09:26:02,517 Stage-1 map = 0%,  reduce = 0%

2020-04-22 09:26:08 2020-04-22 09:26:11,825 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.74 sec

2020-04-22 09:26:16 2020-04-22 09:26:19,060 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.6 sec

2020-04-22 09:26:17 MapReduce Total cumulative CPU time: 11 seconds 600 msec
Ended Job = job_1587346943308_0310
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:26:17 Starting Job = job_1587346943308_0311, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0311/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0311

2020-04-22 09:26:27 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 09:26:27 2020-04-22 09:26:30,822 Stage-2 map = 0%,  reduce = 0%

2020-04-22 09:26:33 2020-04-22 09:26:36,088 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.58 sec

2020-04-22 09:26:41 2020-04-22 09:26:44,331 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.83 sec

2020-04-22 09:26:42 MapReduce Total cumulative CPU time: 7 seconds 830 msec
Ended Job = job_1587346943308_0311
Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 09:26:42 


2020-04-22 09:26:42 	 Time taken to load dynamic partitions: 0.222 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:26:43 Starting Job = job_1587346943308_0312, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0312/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0312

2020-04-22 09:26:53 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 09:26:56,326 Stage-4 map = 0%,  reduce = 0%

2020-04-22 09:26:59 2020-04-22 09:27:02,536 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec

2020-04-22 09:27:05 2020-04-22 09:27:08,748 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.94 sec

2020-04-22 09:27:06 MapReduce Total cumulative CPU time: 3 seconds 940 msec
Ended Job = job_1587346943308_0312

2020-04-22 09:27:07 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 11.6 sec   HDFS Read: 84983052 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.83 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.94 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 23 seconds 370 msec
OK
Time taken: 77.582 seconds

2020-04-22 09:27:08 Query ID = hadoop_20200422092710_e1dcedce-20e6-4bf1-9733-7450226b1e70
Total jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:27:09 Starting Job = job_1587346943308_0313, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0313/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0313

2020-04-22 09:27:17 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 09:27:17 2020-04-22 09:27:20,671 Stage-1 map = 0%,  reduce = 0%

2020-04-22 09:27:24 2020-04-22 09:27:27,943 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 3.77 sec

2020-04-22 09:27:25 2020-04-22 09:27:28,980 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.38 sec

2020-04-22 09:27:32 2020-04-22 09:27:35,175 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.83 sec

2020-04-22 09:27:33 MapReduce Total cumulative CPU time: 10 seconds 830 msec
Ended Job = job_1587346943308_0313

2020-04-22 09:27:33 Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:27:34 Starting Job = job_1587346943308_0314, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0314/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0314

2020-04-22 09:27:44 Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2020-04-22 09:27:47,335 Stage-5 map = 0%,  reduce = 0%

2020-04-22 09:27:50 2020-04-22 09:27:53,529 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 3.82 sec

2020-04-22 09:27:57 2020-04-22 09:28:00,749 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 6.65 sec

2020-04-22 09:27:58 MapReduce Total cumulative CPU time: 6 seconds 650 msec
Ended Job = job_1587346943308_0314
Launching Job 3 out of 4

2020-04-22 09:27:58 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:27:59 Starting Job = job_1587346943308_0315, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0315/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0315

2020-04-22 09:28:09 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1

2020-04-22 09:28:09 2020-04-22 09:28:12,944 Stage-2 map = 0%,  reduce = 0%

2020-04-22 09:28:16 2020-04-22 09:28:19,222 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.3 sec

2020-04-22 09:28:23 2020-04-22 09:28:26,445 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.07 sec

2020-04-22 09:28:24 MapReduce Total cumulative CPU time: 6 seconds 70 msec

2020-04-22 09:28:24 Ended Job = job_1587346943308_0315

2020-04-22 09:28:24 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-22 09:28:24 


2020-04-22 09:28:24 	 Time taken to load dynamic partitions: 0.23 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:28:25 Starting Job = job_1587346943308_0316, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0316/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0316

2020-04-22 09:28:35 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 09:28:35 2020-04-22 09:28:38,460 Stage-4 map = 0%,  reduce = 0%

2020-04-22 09:28:40 2020-04-22 09:28:43,636 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.45 sec

2020-04-22 09:28:46 2020-04-22 09:28:49,800 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.92 sec

2020-04-22 09:28:47 MapReduce Total cumulative CPU time: 3 seconds 920 msec
Ended Job = job_1587346943308_0316

2020-04-22 09:28:48 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 10.83 sec   HDFS Read: 883908 HDFS Write: 182 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 6.65 sec   HDFS Read: 482570 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.07 sec   HDFS Read: 19803 HDFS Write: 706 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.92 sec   HDFS Read: 14036 HDFS Write: 559 SUCCESS
Total MapReduce CPU Time Spent: 27 seconds 470 msec
OK
Time taken: 100.971 seconds

2020-04-22 09:28:48 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:28:48 Last login: Wed Apr 22 09:24:26 2020 from 10.10.30.233

2020-04-22 09:28:48 su hadoop

2020-04-22 09:28:48 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:28:48 [hadoop@hadoop-01 root]$ 
2020-04-22 09:28:48 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:28:48 cat statistic/add_user_day.hive
cat: statistic/add_user_day.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:28:49 Last login: Wed Apr 22 09:28:51 2020 from 10.10.30.233

2020-04-22 09:28:49 su hadoop
[root@hadoop-01 ~]# su hadoop

2020-04-22 09:28:49 [hadoop@hadoop-01 root]$ 
2020-04-22 09:28:49 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:28:49 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 09:28:49 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:28:50 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:28:52 Hive Session ID = 20d3ba2d-5ebc-4e8d-954e-dc49416580c2

Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:28:58 Hive Session ID = 797564a8-bd46-4e97-9df7-9375ec0f5c16

2020-04-22 09:28:59 FAILED: SemanticException [Error 10001]: Line 6:11 Table not found 'user_info'

2020-04-22 09:29:00 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:29:00 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--driver com.mysql.jdbc.Driver \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 09:29:00 Last login: Wed Apr 22 09:28:52 2020 from 10.10.30.233

2020-04-22 09:29:00 su hadoop

2020-04-22 09:29:00 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:29:00 [hadoop@hadoop-01 root]$ 
2020-04-22 09:29:00 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:29:00 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --driver com.mysql.jdbc.Driver \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 09:29:00 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 09:29:01 2020-04-22 09:29:04,524 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 09:29:04,551 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 09:29:01 2020-04-22 09:29:04,598 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-22 09:29:01 2020-04-22 09:29:04,606 INFO manager.SqlManager: Using default fetchSize of 1000

2020-04-22 09:29:01 2020-04-22 09:29:04,804 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 09:29:04,806 INFO tool.CodeGenTool: Beginning code generation
2020-04-22 09:29:04,830 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:29:01 2020-04-22 09:29:04,847 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 09:29:02 注: /tmp/sqoop-hadoop/compile/871986486d425882bdaff87aeb803973/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 09:29:05,822 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/871986486d425882bdaff87aeb803973/user_info.jar
2020-04-22 09:29:05,830 ERROR tool.ExportTool: Error during export: 
Mixed update/insert is not supported against the target database yet
	at org.apache.sqoop.manager.ConnManager.upsertTable(ConnManager.java:684)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 09:29:02 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:29:02 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--driver com.mysql.jdbc.Driver \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 09:29:03 Last login: Wed Apr 22 09:29:03 2020 from 10.10.30.233

2020-04-22 09:29:03 su hadoop

2020-04-22 09:29:03 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:29:03 [hadoop@hadoop-01 root]$ 
2020-04-22 09:29:03 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:29:03 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --driver com.mysql.jdbc.Driver \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 09:29:03 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 09:29:03 2020-04-22 09:29:06,739 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 09:29:06,764 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 09:29:03 2020-04-22 09:29:06,816 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-22 09:29:03 2020-04-22 09:29:06,824 INFO manager.SqlManager: Using default fetchSize of 1000

2020-04-22 09:29:03 2020-04-22 09:29:07,049 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:29:03 2020-04-22 09:29:07,051 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 09:29:04 2020-04-22 09:29:07,074 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:29:04 2020-04-22 09:29:07,091 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 09:29:05 注: /tmp/sqoop-hadoop/compile/edc29cd4d83d8c07a6005296afbdd911/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 09:29:08,023 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/edc29cd4d83d8c07a6005296afbdd911/user_info.jar
2020-04-22 09:29:08,030 ERROR tool.ExportTool: Error during export: 
Mixed update/insert is not supported against the target database yet
	at org.apache.sqoop.manager.ConnManager.upsertTable(ConnManager.java:684)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)
[hadoop@hadoop-01 sx_hadoo
2020-04-22 09:29:05 p_script]$ 
2020-04-22 09:29:05 日统计数据已完成
2020-04-22 09:36:12 开始备份hive待处理日志数据...
2020-04-22 09:36:12 Last login: Wed Apr 22 09:29:06 2020 from 10.10.30.233

2020-04-22 09:36:12 su hadoop

2020-04-22 09:36:12 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:36:12 [hadoop@hadoop-01 root]$ 
2020-04-22 09:36:12 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:12 hadoop fs -rm -R -f hdfs://192.168.15.21:90 00/data/hive/backup/

2020-04-22 09:36:13 2020-04-22 09:36:16,828 INFO fs.TrashPolicyDefault: Moved: 'hdfs://192.168.15.21:9000/data/hive/backup' to trash at: hdfs://192.168.15.21:9000/user/hadoop/.Trash/Current/data/hive/backup1587519376819

2020-04-22 09:36:14 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:14 2020
 3 2020-04-22 09:36:14 23
2020-04-22 09:36:14 Last login: Wed Apr 22 09:36:15 2020 from 10.10.30.233

2020-04-22 09:36:14 su hadoop

2020-04-22 09:36:14 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:36:14 [hadoop@hadoop-01 root]$ 
2020-04-22 09:36:14 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:14 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-23

2020-04-22 09:36:16 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:16 Last login: Wed Apr 22 09:36:17 2020 from 10.10.30.233

2020-04-22 09:36:16 su hadoop

2020-04-22 09:36:16 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:36:16 [hadoop@hadoop-01 root]$ 
2020-04-22 09:36:16 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:16 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/23/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-23

2020-04-22 09:36:23 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:23 Last login: Wed Apr 22 09:36:21 2020 from 10.10.30.233

2020-04-22 09:36:23 su hadoop

2020-04-22 09:36:23 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:36:23 [hadoop@hadoop-01 root]$ 
2020-04-22 09:36:23 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:23 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:23 Last login: Wed Apr 22 09:36:26 2020 from 10.10.30.233

2020-04-22 09:36:23 su hadoop

2020-04-22 09:36:23 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:36:24 [hadoop@hadoop-01 root]$ 
2020-04-22 09:36:24 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:24 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-23 -d day=23 -d year=2020 -f hive/load Logs.hive

2020-04-22 09:36:24 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:36:25 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 09:36:25 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:36:27 Hive Session ID = 6eadfd12-7c56-4945-874e-e8bfbef2ac8a

2020-04-22 09:36:27 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:36:34 Hive Session ID = 8f8e387b-c056-489d-bc9d-2fcaaea8868a

2020-04-22 09:36:34 OK

2020-04-22 09:36:34 Time taken: 0.834 seconds

2020-04-22 09:36:36 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=23)

2020-04-22 09:36:40 OK
Time taken: 5.58 seconds

2020-04-22 09:36:41 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:41 备份hive待处理日志数据已完成。
2020-04-22 09:36:41 2020
 3 2020-04-22 09:36:41 24
2020-04-22 09:36:41 Last login: Wed Apr 22 09:36:26 2020 from 10.10.30.233

2020-04-22 09:36:41 su hadoop

2020-04-22 09:36:41 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:36:41 [hadoop@hadoop-01 root]$ 
2020-04-22 09:36:41 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:41 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-24

2020-04-22 09:36:42 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:43 Last login: Wed Apr 22 09:36:44 2020 from 10.10.30.233

2020-04-22 09:36:43 su hadoop

2020-04-22 09:36:43 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:36:43 [hadoop@hadoop-01 root]$ 
2020-04-22 09:36:43 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:43 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/24/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-24

2020-04-22 09:36:50 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:51 Last login: Wed Apr 22 09:36:46 2020 from 10.10.30.233

2020-04-22 09:36:51 su hadoop

2020-04-22 09:36:51 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:36:51 [hadoop@hadoop-01 root]$ 
2020-04-22 09:36:51 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:51 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:51 Last login: Wed Apr 22 09:36:54 2020 from 10.10.30.233

2020-04-22 09:36:51 su hadoop

2020-04-22 09:36:51 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:36:51 [hadoop@hadoop-01 root]$ 
2020-04-22 09:36:51 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:36:51 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-24 -d day=24 -d year=2020 -f hive/load Logs.hive

2020-04-22 09:36:52 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:36:52 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:36:54 Hive Session ID = 933edc20-6f97-494c-bebd-269f8d21d09c

2020-04-22 09:36:54 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:37:01 Hive Session ID = 97f6a41e-b2db-4ff5-8bb5-7857f3755513

2020-04-22 09:37:02 OK
Time taken: 0.796 seconds

2020-04-22 09:37:03 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=24)

2020-04-22 09:37:07 OK

2020-04-22 09:37:07 Time taken: 4.965 seconds

2020-04-22 09:37:07 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:07 备份hive待处理日志数据已完成。
2020-04-22 09:37:07 2020
 3 2020-04-22 09:37:07 25
2020-04-22 09:37:07 Last login: Wed Apr 22 09:36:54 2020 from 10.10.30.233

2020-04-22 09:37:07 su hadoop

2020-04-22 09:37:07 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:37:07 [hadoop@hadoop-01 root]$ 
2020-04-22 09:37:07 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:07 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-25

2020-04-22 09:37:09 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:09 Last login: Wed Apr 22 09:37:10 2020 from 10.10.30.233

2020-04-22 09:37:09 su hadoop

2020-04-22 09:37:09 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:37:09 [hadoop@hadoop-01 root]$ 
2020-04-22 09:37:09 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:09 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/25/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-25

2020-04-22 09:37:17 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:17 Last login: Wed Apr 22 09:37:12 2020 from 10.10.30.233

2020-04-22 09:37:17 su hadoop

2020-04-22 09:37:17 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:37:17 [hadoop@hadoop-01 root]$ 
2020-04-22 09:37:17 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:17 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:18 Last login: Wed Apr 22 09:37:20 2020 from 10.10.30.233

2020-04-22 09:37:18 su hadoop

2020-04-22 09:37:18 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:37:18 [hadoop@hadoop-01 root]$ 
2020-04-22 09:37:18 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:18 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-25 -d day=25 -d year=2020 -f hive/load Logs.hive

2020-04-22 09:37:18 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:37:19 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:37:21 Hive Session ID = 56ea92be-16e4-438f-a802-1f0e8091eef6

2020-04-22 09:37:21 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:37:27 Hive Session ID = 7dcf814c-5c5a-4e1c-bf38-4a8b6af628c7

2020-04-22 09:37:28 OK
Time taken: 0.771 seconds

2020-04-22 09:37:29 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=25)

2020-04-22 09:37:34 OK
Time taken: 6.163 seconds

2020-04-22 09:37:35 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:35 备份hive待处理日志数据已完成。
2020-04-22 09:37:35 开始处理对yn_logs进行分类处理
2020-04-22 09:37:35 Last login: Wed Apr 22 09:37:21 2020 from 10.10.30.233

2020-04-22 09:37:35 su hadoop

2020-04-22 09:37:35 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:37:35 [hadoop@hadoop-01 root]$ 
2020-04-22 09:37:35 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:35 cat eventsType.hive

2020-04-22 09:37:35 cat: eventsType.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:35 Last login: Wed Apr 22 09:37:38 2020 from 10.10.30.233

2020-04-22 09:37:35 su hadoop

2020-04-22 09:37:35 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:37:35 [hadoop@hadoop-01 root]$ 
2020-04-22 09:37:35 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:37:35 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/eventsType.hive

2020-04-22 09:37:36 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:37:36 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:37:38 Hive Session ID = c24d6d3a-427c-4cfc-85c1-aef065d0fd0d

2020-04-22 09:37:38 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:37:45 Hive Session ID = 9f7d9f7e-b33c-42a5-ab9b-9a5f4561cb25

2020-04-22 09:37:46 OK

2020-04-22 09:37:46 Time taken: 0.783 seconds

2020-04-22 09:37:49 Query ID = hadoop_20200422093749_7068baa3-4359-4109-92c5-37ecaed72eb1
Total jobs = 3

2020-04-22 09:37:50 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:37:55 2020-04-22 09:37:58	Starting to launch local task to process map join;	maximum memory = 239075328

2020-04-22 09:37:56 Execution completed successfully
MapredLocal task succeeded

2020-04-22 09:37:57 Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-22 09:37:59 Starting Job = job_1587346943308_0318, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0318/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0318

2020-04-22 09:38:06 Hadoop job information for Stage-9: number of mappers: 4; number of reducers: 0

2020-04-22 09:38:06 2020-04-22 09:38:09,211 Stage-9 map = 0%,  reduce = 0%

2020-04-22 09:38:23 2020-04-22 09:38:26,843 Stage-9 map = 6%,  reduce = 0%, Cumulative CPU 17.64 sec

2020-04-22 09:38:24 2020-04-22 09:38:27,877 Stage-9 map = 29%,  reduce = 0%, Cumulative CPU 69.36 sec

2020-04-22 09:38:29 2020-04-22 09:38:33,032 Stage-9 map = 46%,  reduce = 0%, Cumulative CPU 81.81 sec

2020-04-22 09:38:31 2020-04-22 09:38:34,065 Stage-9 map = 54%,  reduce = 0%, Cumulative CPU 94.79 sec

2020-04-22 09:38:36 2020-04-22 09:38:39,203 Stage-9 map = 64%,  reduce = 0%, Cumulative CPU 107.29 sec

2020-04-22 09:38:37 2020-04-22 09:38:40,231 Stage-9 map = 68%,  reduce = 0%, Cumulative CPU 113.81 sec

2020-04-22 09:38:42 2020-04-22 09:38:45,371 Stage-9 map = 78%,  reduce = 0%, Cumulative CPU 126.35 sec

2020-04-22 09:38:43 2020-04-22 09:38:46,401 Stage-9 map = 81%,  reduce = 0%, Cumulative CPU 132.68 sec

2020-04-22 09:38:46 2020-04-22 09:38:49,472 Stage-9 map = 85%,  reduce = 0%, Cumulative CPU 137.19 sec

2020-04-22 09:38:48 2020-04-22 09:38:51,530 Stage-9 map = 93%,  reduce = 0%, Cumulative CPU 149.67 sec

2020-04-22 09:38:53 2020-04-22 09:38:56,675 Stage-9 map = 96%,  reduce = 0%, Cumulative CPU 153.17 sec

2020-04-22 09:38:54 2020-04-22 09:38:57,719 Stage-9 map = 99%,  reduce = 0%, Cumulative CPU 159.2 sec

2020-04-22 09:38:55 2020-04-22 09:38:58,739 Stage-9 map = 100%,  reduce = 0%, Cumulative CPU 159.54 sec

2020-04-22 09:38:56 MapReduce Total cumulative CPU time: 2 minutes 39 seconds 540 msec

2020-04-22 09:38:56 Ended Job = job_1587346943308_0318

2020-04-22 09:38:57 Stage-4 is filtered out by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is selected by condition resolver.

2020-04-22 09:38:59 Launching Job 3 out of 3

2020-04-22 09:38:59 Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-22 09:39:00 Starting Job = job_1587346943308_0319, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0319/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0319

2020-04-22 09:39:07 Hadoop job information for Stage-5: number of mappers: 9; number of reducers: 0
2020-04-22 09:39:10,631 Stage-5 map = 0%,  reduce = 0%

2020-04-22 09:39:16 2020-04-22 09:39:19,930 Stage-5 map = 11%,  reduce = 0%, Cumulative CPU 7.6 sec

2020-04-22 09:39:17 2020-04-22 09:39:20,959 Stage-5 map = 44%,  reduce = 0%, Cumulative CPU 27.15 sec

2020-04-22 09:39:18 2020-04-22 09:39:21,990 Stage-5 map = 56%,  reduce = 0%, Cumulative CPU 34.18 sec

2020-04-22 09:39:22 2020-04-22 09:39:25,107 Stage-5 map = 67%,  reduce = 0%, Cumulative CPU 37.4 sec

2020-04-22 09:39:23 2020-04-22 09:39:26,141 Stage-5 map = 78%,  reduce = 0%, Cumulative CPU 39.81 sec

2020-04-22 09:39:24 2020-04-22 09:39:27,169 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 45.39 sec

2020-04-22 09:39:25 MapReduce Total cumulative CPU time: 45 seconds 390 msec
Ended Job = job_1587346943308_0319

2020-04-22 09:39:25 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-22_09-37-49_695_1784641776624485322-1/-ext-10000/events_type=operateResumePoint/y=2020/m=03/d=23
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-22_09-37-49_695_1784641776624485322-1/-ext-10000/events_type=operationDetails/y=2020/m=03/d=23
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-22_09-37-49_695_1784641776624485322-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=23

2020-04-22 09:39:27 Loading data to table yn_hadoop.events_type_log partition (events_type=null, y=null, m=null, d=null)

2020-04-22 09:39:27 


2020-04-22 09:39:29 	 Time taken to load dynamic partitions: 1.504 seconds
	 Time taken for adding to write entity : 0.003 seconds

2020-04-22 09:39:31 MapReduce Jobs Launched: 
Stage-Stage-9: Map: 4   Cumulative CPU: 159.54 sec   HDFS Read: 893623459 HDFS Write: 338539509 SUCCESS
Stage-Stage-5: Map: 9   Cumulative CPU: 45.39 sec   HDFS Read: 193904147 HDFS Write: 193928806 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 24 seconds 930 msec
OK

2020-04-22 09:39:31 Time taken: 105.079 seconds

2020-04-22 09:39:32 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:39:32 yn_logs进行分类处理已完成
2020-04-22 09:39:32 开始处理对日统计数据
2020-04-22 09:39:32 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \

--hive-table user_info \
--hive-overwrite
2020-04-22 09:39:32 Last login: Wed Apr 22 09:38:43 2020 from 10.10.30.233

2020-04-22 09:39:32 su hadoop

2020-04-22 09:39:32 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:39:32 [hadoop@hadoop-01 root]$ 
2020-04-22 09:39:32 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:39:32 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> 

2020-04-22 09:39:32 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 09:39:33 2020-04-22 09:39:36,116 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 09:39:36,141 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 09:39:33 2020-04-22 09:39:36,192 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-22 09:39:33 2020-04-22 09:39:36,201 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 09:39:36,203 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 09:39:33 2020-04-22 09:39:36,422 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 09:39:36,427 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:39:33 2020-04-22 09:39:36,444 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 09:39:34 注: /tmp/sqoop-hadoop/compile/effea08c33602ea377a4aac3e4baf085/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 09:39:37,398 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/effea08c33602ea377a4aac3e4baf085/user_info.jar

2020-04-22 09:39:35 2020-04-22 09:39:38,114 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-22 09:39:38,119 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 09:39:38,120 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 09:39:38,124 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 09:39:38,125 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 09:39:38,134 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 09:39:35 2020-04-22 09:39:38,625 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0320

2020-04-22 09:39:37 2020-04-22 09:39:40,299 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 09:39:37 2020-04-22 09:39:40,746 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 09:39:37 2020-04-22 09:39:40,945 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0320
2020-04-22 09:39:40,947 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 09:39:38 2020-04-22 09:39:41,133 INFO conf.Configuration: resource-types.xml not found
2020-04-22 09:39:41,133 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 09:39:38 2020-04-22 09:39:41,178 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0320

2020-04-22 09:39:38 2020-04-22 09:39:41,204 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0320/
2020-04-22 09:39:41,204 INFO mapreduce.Job: Running job: job_1587346943308_0320

2020-04-22 09:39:44 2020-04-22 09:39:47,294 INFO mapreduce.Job: Job job_1587346943308_0320 running in uber mode : false
2020-04-22 09:39:47,296 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 09:39:49 2020-04-22 09:39:52,362 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-22 09:39:49 2020-04-22 09:39:52,373 INFO mapreduce.Job: Job job_1587346943308_0320 completed successfully

2020-04-22 09:39:49 2020-04-22 09:39:52,488 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2704
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2704
		Total vcore-milliseconds taken by all map tasks=2704
		Total megabyte-milliseconds taken by all map tasks=2768896
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=52
		CPU time spen
2020-04-22 09:39:49 t (ms)=1310
		Physical memory (bytes) snapshot=242909184
		Virtual memory (bytes) snapshot=2446512128
		Total committed heap usage (bytes)=192937984
		Peak Map Physical memory (bytes)=242909184
		Peak Map Virtual memory (bytes)=2446512128
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572
2020-04-22 09:39:52,496 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 14.3527 seconds (42.0879 KB/sec)

2020-04-22 09:39:49 2020-04-22 09:39:52,499 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-22 09:39:52,499 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info

2020-04-22 09:39:49 2020-04-22 09:39:52,511 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 09:39:52,512 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:39:49 2020-04-22 09:39:52,527 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 09:39:52,533 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 09:39:49 2020-04-22 09:39:52,686 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 09:39:50 2020-04-22 09:39:53,246 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-22 09:39:53,269 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 09:39:50 2020-04-22 09:39:53,709 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 09:39:53,709 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 09:39:53,709 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 09:39:53,709 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 09:39:53,720 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:39:52 2020-04-22 09:39:55,828 INFO hive.HiveImport: Hive Session ID = 395a77d9-9e0b-4163-994a-de9a8d279f47

2020-04-22 09:39:52 2020-04-22 09:39:55,877 INFO hive.HiveImport: 
2020-04-22 09:39:55,877 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:39:59 2020-04-22 09:40:03,020 INFO hive.HiveImport: Hive Session ID = c2cc344a-3ad0-436d-a78d-286692774f78

2020-04-22 09:40:01 2020-04-22 09:40:04,432 INFO hive.HiveImport: OK
2020-04-22 09:40:04,443 INFO hive.HiveImport: Time taken: 1.354 seconds

2020-04-22 09:40:01 2020-04-22 09:40:04,717 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 09:40:02 2020-04-22 09:40:05,237 INFO hive.HiveImport: OK
2020-04-22 09:40:05,249 INFO hive.HiveImport: Time taken: 0.782 seconds

2020-04-22 09:40:02 2020-04-22 09:40:05,760 INFO hive.HiveImport: Hive import complete.
2020-04-22 09:40:05,772 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.

2020-04-22 09:40:03 [hadoop@hadoop-01 sx_hadoop_script]$ --hive-table user_info \
> --hive-overwrite
bash: --hive-table: 未找到命令
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:40:03 Last login: Wed Apr 22 09:39:35 2020 from 10.10.30.233

2020-04-22 09:40:03 su hadoop

2020-04-22 09:40:03 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:40:03 [hadoop@hadoop-01 root]$ 
2020-04-22 09:40:03 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:40:03 cat statistic/RegionbyDay.hive

2020-04-22 09:40:03 cat: statistic/RegionbyDay.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:40:03 Last login: Wed Apr 22 09:40:06 2020 from 10.10.30.233

2020-04-22 09:40:03 su hadoop

2020-04-22 09:40:03 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:40:03 [hadoop@hadoop-01 root]$ 
2020-04-22 09:40:03 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:40:03 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 09:40:04 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:40:04 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:40:06 Hive Session ID = 7b592f61-d047-4d8a-ad97-215c0ed749a6

2020-04-22 09:40:06 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:40:13 Hive Session ID = 866538df-4d7d-46c3-ba84-55c31f27be67

2020-04-22 09:40:14 OK
Time taken: 0.79 seconds

2020-04-22 09:40:16 Query ID = hadoop_20200422094017_ee752b96-1a54-4f23-9eee-172559c1d0e8
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 09:40:16 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:40:18 Starting Job = job_1587346943308_0321, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0321/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0321

2020-04-22 09:40:24 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 09:40:24 2020-04-22 09:40:27,777 Stage-1 map = 0%,  reduce = 0%

2020-04-22 09:40:33 2020-04-22 09:40:36,131 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 4.99 sec

2020-04-22 09:40:37 2020-04-22 09:40:40,284 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.5 sec

2020-04-22 09:40:40 2020-04-22 09:40:43,414 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.03 sec

2020-04-22 09:40:41 MapReduce Total cumulative CPU time: 20 seconds 30 msec
Ended Job = job_1587346943308_0321

2020-04-22 09:40:41 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:40:42 Starting Job = job_1587346943308_0322, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0322/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0322

2020-04-22 09:40:52 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 09:40:52 2020-04-22 09:40:55,280 Stage-2 map = 0%,  reduce = 0%

2020-04-22 09:40:58 2020-04-22 09:41:01,503 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.74 sec

2020-04-22 09:41:05 2020-04-22 09:41:08,732 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.07 sec

2020-04-22 09:41:06 MapReduce Total cumulative CPU time: 7 seconds 70 msec
Ended Job = job_1587346943308_0322

2020-04-22 09:41:06 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 09:41:06 


2020-04-22 09:41:07 	 Time taken to load dynamic partitions: 0.423 seconds
	 Time taken for adding to write entity : 0.002 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:41:07 Starting Job = job_1587346943308_0323, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0323/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0323

2020-04-22 09:41:18 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 09:41:22,025 Stage-4 map = 0%,  reduce = 0%

2020-04-22 09:41:24 2020-04-22 09:41:27,315 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.4 sec

2020-04-22 09:41:29 2020-04-22 09:41:32,470 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.92 sec

2020-04-22 09:41:30 MapReduce Total cumulative CPU time: 3 seconds 920 msec
Ended Job = job_1587346943308_0323

2020-04-22 09:41:31 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 20.03 sec   HDFS Read: 193048749 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.07 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.92 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 20 msec
OK
Time taken: 76.845 seconds

2020-04-22 09:41:31 Query ID = hadoop_20200422094134_6fcd3f9a-43b6-4f2d-8bbe-1518a63baa47
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:41:32 Starting Job = job_1587346943308_0324, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0324/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0324

2020-04-22 09:41:41 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-04-22 09:41:44,388 Stage-1 map = 0%,  reduce = 0%

2020-04-22 09:41:50 2020-04-22 09:41:53,676 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.78 sec

2020-04-22 09:41:57 2020-04-22 09:42:00,899 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.71 sec

2020-04-22 09:41:58 MapReduce Total cumulative CPU time: 11 seconds 710 msec
Ended Job = job_1587346943308_0324
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:41:59 Starting Job = job_1587346943308_0325, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0325/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0325

2020-04-22 09:42:10 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2020-04-22 09:42:13,671 Stage-2 map = 0%,  reduce = 0%

2020-04-22 09:42:15 2020-04-22 09:42:18,972 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.51 sec

2020-04-22 09:42:24 2020-04-22 09:42:27,226 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.71 sec

2020-04-22 09:42:25 MapReduce Total cumulative CPU time: 7 seconds 710 msec
Ended Job = job_1587346943308_0325

2020-04-22 09:42:25 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 09:42:25 


2020-04-22 09:42:25 	 Time taken to load dynamic partitions: 0.296 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:42:26 Starting Job = job_1587346943308_0326, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0326/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0326

2020-04-22 09:42:36 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 09:42:39,797 Stage-4 map = 0%,  reduce = 0%

2020-04-22 09:42:41 2020-04-22 09:42:44,975 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.39 sec

2020-04-22 09:42:49 2020-04-22 09:42:52,190 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.2 sec

2020-04-22 09:42:50 MapReduce Total cumulative CPU time: 4 seconds 200 msec
Ended Job = job_1587346943308_0326

2020-04-22 09:42:50 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 11.71 sec   HDFS Read: 84983052 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.71 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.2 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 23 seconds 620 msec
OK
Time taken: 79.174 seconds

2020-04-22 09:42:52 Query ID = hadoop_20200422094253_5b52a308-7971-465a-bae7-8249f37d12fb
Total jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:42:53 Starting Job = job_1587346943308_0327, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0327/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0327

2020-04-22 09:43:01 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1

2020-04-22 09:43:01 2020-04-22 09:43:04,452 Stage-1 map = 0%,  reduce = 0%

2020-04-22 09:43:07 2020-04-22 09:43:10,692 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.8 sec

2020-04-22 09:43:14 2020-04-22 09:43:17,923 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.5 sec

2020-04-22 09:43:15 MapReduce Total cumulative CPU time: 7 seconds 500 msec

2020-04-22 09:43:15 Ended Job = job_1587346943308_0327

2020-04-22 09:43:15 Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:43:16 Starting Job = job_1587346943308_0328, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0328/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0328

2020-04-22 09:43:26 Hadoop job information for Stage-5: number of mappers: 2; number of reducers: 1
2020-04-22 09:43:29,672 Stage-5 map = 0%,  reduce = 0%

2020-04-22 09:43:33 2020-04-22 09:43:36,895 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 7.03 sec

2020-04-22 09:43:40 2020-04-22 09:43:43,094 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 10.08 sec

2020-04-22 09:43:41 MapReduce Total cumulative CPU time: 10 seconds 80 msec
Ended Job = job_1587346943308_0328
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:43:41 Starting Job = job_1587346943308_0329, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0329/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0329

2020-04-22 09:43:51 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1

2020-04-22 09:43:51 2020-04-22 09:43:54,859 Stage-2 map = 0%,  reduce = 0%

2020-04-22 09:43:58 2020-04-22 09:44:01,078 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.31 sec

2020-04-22 09:44:04 2020-04-22 09:44:07,262 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.6 sec

2020-04-22 09:44:05 MapReduce Total cumulative CPU time: 6 seconds 600 msec
Ended Job = job_1587346943308_0329

2020-04-22 09:44:05 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-22 09:44:05 


2020-04-22 09:44:05 	 Time taken to load dynamic partitions: 0.222 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:44:06 Starting Job = job_1587346943308_0330, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0330/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0330

2020-04-22 09:44:16 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 09:44:16 2020-04-22 09:44:19,278 Stage-4 map = 0%,  reduce = 0%

2020-04-22 09:44:22 2020-04-22 09:44:25,483 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.5 sec

2020-04-22 09:44:26 2020-04-22 09:44:29,618 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.01 sec

2020-04-22 09:44:28 MapReduce Total cumulative CPU time: 4 seconds 10 msec
Ended Job = job_1587346943308_0330

2020-04-22 09:44:28 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.5 sec   HDFS Read: 877473 HDFS Write: 182 SUCCESS
Stage-Stage-5: Map: 2  Reduce: 1   Cumulative CPU: 10.08 sec   HDFS Read: 489429 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.6 sec   HDFS Read: 19803 HDFS Write: 706 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.01 sec   HDFS Read: 14036 HDFS Write: 559 SUCCESS
Total MapReduce CPU Time Spent: 28 seconds 190 msec
OK
Time taken: 98.446 seconds

2020-04-22 09:44:29 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:44:29 Last login: Wed Apr 22 09:40:06 2020 from 10.10.30.233

2020-04-22 09:44:29 su hadoop

2020-04-22 09:44:29 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:44:29 [hadoop@hadoop-01 root]$ 
2020-04-22 09:44:29 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:44:29 cat statistic/add_user_day.hive

2020-04-22 09:44:29 cat: statistic/add_user_day.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:44:29 Last login: Wed Apr 22 09:44:32 2020 from 10.10.30.233

2020-04-22 09:44:29 su hadoop

2020-04-22 09:44:29 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:44:29 [hadoop@hadoop-01 root]$ 
2020-04-22 09:44:29 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:44:29 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 09:44:30 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:44:30 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 09:44:30 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:44:32 Hive Session ID = abfefb64-9467-4d39-9a60-cb625caacc88

2020-04-22 09:44:33 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:44:39 Hive Session ID = 6f816073-7271-4c5c-892e-4ea1764a988b

2020-04-22 09:44:40 FAILED: SemanticException [Error 10001]: Line 6:11 Table not found 'user_info'

2020-04-22 09:44:41 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:44:41 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 09:44:41 Last login: Wed Apr 22 09:44:32 2020 from 10.10.30.233

2020-04-22 09:44:41 su hadoop

2020-04-22 09:44:41 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:44:41 [hadoop@hadoop-01 root]$ 
2020-04-22 09:44:41 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:44:41 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 09:44:41 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 09:44:42 2020-04-22 09:44:45,465 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 09:44:45,489 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 09:44:42 2020-04-22 09:44:45,567 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 09:44:45,570 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 09:44:42 2020-04-22 09:44:45,768 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 09:44:45,783 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 09:44:42 2020-04-22 09:44:45,788 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 09:44:43 注: /tmp/sqoop-hadoop/compile/ba7f7ea3d0002878d79ef054b8b52116/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 09:44:46,738 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/ba7f7ea3d0002878d79ef054b8b52116/user_info.jar
2020-04-22 09:44:46,744 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 09:44:46,744 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 09:44:46,744 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 09:44:46,744 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 09:44:46,744 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-22 09:44:43 2020-04-22 09:44:46,750 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 09:44:46,751 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 09:44:43 2020-04-22 09:44:46,851 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 09:44:44 2020-04-22 09:44:47,453 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=Y/m=m/d=d does not exist
2020-04-22 09:44:47,472 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 09:44:47,473 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 09:44:47,473 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 09:44:44 2020-04-22 09:44:47,907 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0331

2020-04-22 09:44:46 2020-04-22 09:44:49,629 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0331

2020-04-22 09:44:46 2020-04-22 09:44:49,688 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=Y/m=m/d=d
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessControlle
2020-04-22 09:44:46 r.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	
2020-04-22 09:44:46 at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 09:44:46 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:44:47 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 09:44:47 Last login: Wed Apr 22 09:44:44 2020 from 10.10.30.233

2020-04-22 09:44:47 su hadoop

2020-04-22 09:44:47 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:44:47 [hadoop@hadoop-01 root]$ 
2020-04-22 09:44:47 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:44:47 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 09:44:47 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 09:44:47 2020-04-22 09:44:50,812 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 09:44:50,836 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 09:44:47 2020-04-22 09:44:50,913 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 09:44:50,916 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 09:44:48 2020-04-22 09:44:51,123 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 09:44:51,137 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 09:44:51,142 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 09:44:49 注: /tmp/sqoop-hadoop/compile/ee7909ddbb2faf198adb5accb8b3aa12/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。

2020-04-22 09:44:49 2020-04-22 09:44:52,126 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/ee7909ddbb2faf198adb5accb8b3aa12/user_info.jar
2020-04-22 09:44:52,133 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 09:44:52,133 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 09:44:52,133 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 09:44:52,133 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 09:44:52,133 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 09:44:52,138 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 09:44:52,138 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 09:44:49 2020-04-22 09:44:52,236 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 09:44:49 2020-04-22 09:44:52,864 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=Y/m=m/d=d does not exist
2020-04-22 09:44:52,883 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 09:44:52,885 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 09:44:52,885 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 09:44:50 2020-04-22 09:44:53,317 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0332

2020-04-22 09:44:51 2020-04-22 09:44:55,018 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0332
2020-04-22 09:44:55,022 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=Y/m=m/d=d
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at 
2020-04-22 09:44:51 org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apa
2020-04-22 09:44:51 che.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 09:44:52 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:44:52 日统计数据已完成
2020-04-22 09:50:25 开始备份hive待处理日志数据...
2020-04-22 09:50:25 Last login: Wed Apr 22 09:49:38 2020 from 10.10.30.233

2020-04-22 09:50:25 su hadoop

2020-04-22 09:50:25 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:50:25 [hadoop@hadoop-01 root]$ 
2020-04-22 09:50:25 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:25 hadoop fs -rm -R -f hdfs://192.168.15.21:90 00/data/hive/backup/

2020-04-22 09:50:26 2020-04-22 09:50:29,874 INFO fs.TrashPolicyDefault: Moved: 'hdfs://192.168.15.21:9000/data/hive/backup' to trash at: hdfs://192.168.15.21:9000/user/hadoop/.Trash/Current/data/hive/backup1587520229863

2020-04-22 09:50:27 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:27 2020
 3 2020-04-22 09:50:27 23
2020-04-22 09:50:27 Last login: Wed Apr 22 09:50:28 2020 from 10.10.30.233

2020-04-22 09:50:27 su hadoop

2020-04-22 09:50:27 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:50:27 [hadoop@hadoop-01 root]$ 
2020-04-22 09:50:27 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:27 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-23

2020-04-22 09:50:29 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:29 Last login: Wed Apr 22 09:50:30 2020 from 10.10.30.233

2020-04-22 09:50:29 su hadoop

2020-04-22 09:50:29 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:50:29 [hadoop@hadoop-01 root]$ 
2020-04-22 09:50:29 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:29 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/23/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-23

2020-04-22 09:50:36 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:36 Last login: Wed Apr 22 09:50:32 2020 from 10.10.30.233

2020-04-22 09:50:36 su hadoop

2020-04-22 09:50:36 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:50:36 [hadoop@hadoop-01 root]$ 
2020-04-22 09:50:36 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:36 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:36 Last login: Wed Apr 22 09:50:39 2020 from 10.10.30.233

2020-04-22 09:50:36 su hadoop

2020-04-22 09:50:36 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:50:36 [hadoop@hadoop-01 root]$ 
2020-04-22 09:50:36 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:36 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-23 -d day=23 -d year=2020 -f hive/load Logs.hive

2020-04-22 09:50:37 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:50:38 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:50:39 Hive Session ID = f18f1139-1821-4aeb-bd86-5c96c56b2cbd

2020-04-22 09:50:40 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:50:46 Hive Session ID = 75b5614e-771c-4136-82d9-bad6b075e6d6

2020-04-22 09:50:47 OK
Time taken: 0.813 seconds

2020-04-22 09:50:48 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=23)

2020-04-22 09:50:52 OK
Time taken: 5.325 seconds

2020-04-22 09:50:53 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:53 备份hive待处理日志数据已完成。
2020-04-22 09:50:53 2020
 3 2020-04-22 09:50:53 24
2020-04-22 09:50:53 Last login: Wed Apr 22 09:50:39 2020 from 10.10.30.233

2020-04-22 09:50:53 su hadoop

2020-04-22 09:50:53 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:50:53 [hadoop@hadoop-01 root]$ 
2020-04-22 09:50:53 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:53 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-24

2020-04-22 09:50:55 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:55 Last login: Wed Apr 22 09:50:56 2020 from 10.10.30.233

2020-04-22 09:50:55 su hadoop

2020-04-22 09:50:55 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:50:55 [hadoop@hadoop-01 root]$ 
2020-04-22 09:50:55 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:50:55 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/24/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-24

2020-04-22 09:51:02 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:02 Last login: Wed Apr 22 09:50:58 2020 from 10.10.30.233

2020-04-22 09:51:02 su hadoop

2020-04-22 09:51:02 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:51:02 [hadoop@hadoop-01 root]$ 
2020-04-22 09:51:02 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:02 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:03 Last login: Wed Apr 22 09:51:05 2020 from 10.10.30.233

2020-04-22 09:51:03 su hadoop

2020-04-22 09:51:03 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:51:03 [hadoop@hadoop-01 root]$ 
2020-04-22 09:51:03 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:03 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-24 -d day=24 -d year=2020 -f hive/load Logs.hive

2020-04-22 09:51:03 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:51:04 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:51:06 Hive Session ID = 86be5b28-110b-415b-9f0f-d7c623649f03

2020-04-22 09:51:06 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:51:13 Hive Session ID = 559ddf48-3821-4e34-8f7d-508d04ac7d14

2020-04-22 09:51:14 OK

2020-04-22 09:51:14 Time taken: 0.797 seconds

2020-04-22 09:51:15 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=24)

2020-04-22 09:51:19 OK
Time taken: 5.551 seconds

2020-04-22 09:51:20 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:20 备份hive待处理日志数据已完成。
2020-04-22 09:51:20 2020
 3 2020-04-22 09:51:20 25
2020-04-22 09:51:20 Last login: Wed Apr 22 09:51:06 2020 from 10.10.30.233

2020-04-22 09:51:20 su hadoop

2020-04-22 09:51:20 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:51:20 [hadoop@hadoop-01 root]$ 
2020-04-22 09:51:20 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:20 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-25

2020-04-22 09:51:22 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:22 Last login: Wed Apr 22 09:51:23 2020 from 10.10.30.233

2020-04-22 09:51:22 su hadoop

2020-04-22 09:51:22 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:51:22 [hadoop@hadoop-01 root]$ 
2020-04-22 09:51:22 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:22 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/25/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-25

2020-04-22 09:51:30 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:30 Last login: Wed Apr 22 09:51:25 2020 from 10.10.30.233

2020-04-22 09:51:30 su hadoop

2020-04-22 09:51:30 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:51:30 [hadoop@hadoop-01 root]$ 
2020-04-22 09:51:30 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:30 cat loadLogs.hive
cat: loadLogs.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:30 Last login: Wed Apr 22 09:51:33 2020 from 10.10.30.233

2020-04-22 09:51:30 su hadoop

2020-04-22 09:51:30 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:51:30 [hadoop@hadoop-01 root]$ 
2020-04-22 09:51:30 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:30 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-25 -d day=25 -d year=2020 -f hive/load Logs.hive

2020-04-22 09:51:31 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:51:32 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:51:33 Hive Session ID = 87282c1e-8a10-40fc-9ca0-5ee99d8a3e45

2020-04-22 09:51:34 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:51:40 Hive Session ID = 84469b21-ba5a-462a-9ca6-8e598d22edc4

2020-04-22 09:51:41 OK
Time taken: 0.774 seconds

2020-04-22 09:51:42 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=25)

2020-04-22 09:51:47 OK
Time taken: 6.343 seconds

2020-04-22 09:51:48 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:48 备份hive待处理日志数据已完成。
2020-04-22 09:51:48 开始处理对yn_logs进行分类处理
2020-04-22 09:51:48 Last login: Wed Apr 22 09:51:33 2020 from 10.10.30.233

2020-04-22 09:51:48 su hadoop

2020-04-22 09:51:48 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:51:48 [hadoop@hadoop-01 root]$ 
2020-04-22 09:51:48 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:48 cat eventsType.hive

2020-04-22 09:51:48 cat: eventsType.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:48 Last login: Wed Apr 22 09:51:51 2020 from 10.10.30.233

2020-04-22 09:51:48 su hadoop

2020-04-22 09:51:48 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:51:48 [hadoop@hadoop-01 root]$ 
2020-04-22 09:51:48 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:51:48 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/eventsType.hive

2020-04-22 09:51:49 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:51:49 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:51:51 Hive Session ID = 8d06e36a-54ae-4991-9d66-9e521fabceed

2020-04-22 09:51:52 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:52:00 Hive Session ID = cec07905-9174-40a0-acec-e2ebcdf2f7b3

2020-04-22 09:52:01 OK
Time taken: 0.775 seconds

2020-04-22 09:52:04 Query ID = hadoop_20200422095204_a69207f1-0b48-4a19-9414-5fa701a911d0
Total jobs = 3

2020-04-22 09:52:04 SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 09:52:10 2020-04-22 09:52:13	Dump the side-table for tag: 1 with group count: 9 into file: file:/tmp/hive/8d06e36a-54ae-4991-9d66-9e521fabceed/hive_2020-04-22_09-52-04_204_4337033637523651475-1/-local-10003/HashTable-Stage-9/MapJoin-mapfile01--.hashtable

2020-04-22 09:52:10 2020-04-22 09:52:13	Uploaded 1 File to: file:/tmp/hive/8d06e36a-54ae-4991-9d66-9e521fabceed/hive_2020-04-22_09-52-04_204_4337033637523651475-1/-local-10003/HashTable-Stage-9/MapJoin-mapfile01--.hashtable (488 bytes)
2020-04-22 09:52:13	End of local task; Time Taken: 0.475 sec.

2020-04-22 09:52:11 Execution completed successfully
MapredLocal task succeeded

2020-04-22 09:52:12 Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-22 09:52:13 Starting Job = job_1587346943308_0333, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0333/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0333

2020-04-22 09:52:20 Hadoop job information for Stage-9: number of mappers: 4; number of reducers: 0

2020-04-22 09:52:20 2020-04-22 09:52:23,232 Stage-9 map = 0%,  reduce = 0%

2020-04-22 09:52:37 2020-04-22 09:52:40,855 Stage-9 map = 26%,  reduce = 0%, Cumulative CPU 51.73 sec

2020-04-22 09:52:38 2020-04-22 09:52:41,890 Stage-9 map = 32%,  reduce = 0%, Cumulative CPU 69.01 sec

2020-04-22 09:52:43 2020-04-22 09:52:47,030 Stage-9 map = 55%,  reduce = 0%, Cumulative CPU 94.36 sec

2020-04-22 09:52:50 2020-04-22 09:52:53,231 Stage-9 map = 68%,  reduce = 0%, Cumulative CPU 113.56 sec

2020-04-22 09:52:55 2020-04-22 09:52:58,383 Stage-9 map = 72%,  reduce = 0%, Cumulative CPU 119.84 sec

2020-04-22 09:52:56 2020-04-22 09:52:59,409 Stage-9 map = 83%,  reduce = 0%, Cumulative CPU 132.62 sec

2020-04-22 09:53:01 2020-04-22 09:53:04,551 Stage-9 map = 89%,  reduce = 0%, Cumulative CPU 143.36 sec

2020-04-22 09:53:02 2020-04-22 09:53:05,577 Stage-9 map = 93%,  reduce = 0%, Cumulative CPU 148.22 sec

2020-04-22 09:53:05 2020-04-22 09:53:08,666 Stage-9 map = 97%,  reduce = 0%, Cumulative CPU 153.07 sec

2020-04-22 09:53:07 2020-04-22 09:53:10,740 Stage-9 map = 100%,  reduce = 0%, Cumulative CPU 157.59 sec

2020-04-22 09:53:08 MapReduce Total cumulative CPU time: 2 minutes 37 seconds 590 msec

2020-04-22 09:53:08 Ended Job = job_1587346943308_0333

2020-04-22 09:53:10 Stage-4 is filtered out by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is selected by condition resolver.

2020-04-22 09:53:12 Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-22 09:53:12 Starting Job = job_1587346943308_0334, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0334/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0334

2020-04-22 09:53:20 Hadoop job information for Stage-5: number of mappers: 8; number of reducers: 0

2020-04-22 09:53:20 2020-04-22 09:53:23,137 Stage-5 map = 0%,  reduce = 0%

2020-04-22 09:53:29 2020-04-22 09:53:32,454 Stage-5 map = 25%,  reduce = 0%, Cumulative CPU 10.7 sec

2020-04-22 09:53:30 2020-04-22 09:53:33,492 Stage-5 map = 50%,  reduce = 0%, Cumulative CPU 23.05 sec

2020-04-22 09:53:31 2020-04-22 09:53:34,525 Stage-5 map = 63%,  reduce = 0%, Cumulative CPU 30.36 sec

2020-04-22 09:53:33 2020-04-22 09:53:36,585 Stage-5 map = 75%,  reduce = 0%, Cumulative CPU 33.27 sec

2020-04-22 09:53:34 2020-04-22 09:53:37,616 Stage-5 map = 88%,  reduce = 0%, Cumulative CPU 36.76 sec

2020-04-22 09:53:35 2020-04-22 09:53:38,642 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 39.14 sec

2020-04-22 09:53:36 MapReduce Total cumulative CPU time: 39 seconds 140 msec
Ended Job = job_1587346943308_0334

2020-04-22 09:53:38 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-22_09-52-04_204_4337033637523651475-1/-ext-10000/events_type=operateResumePoint/y=2020/m=03/d=23
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-22_09-52-04_204_4337033637523651475-1/-ext-10000/events_type=operationDetails/y=2020/m=03/d=23
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-22_09-52-04_204_4337033637523651475-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=23
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-22_09-52-04_204_4337033637523651475-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=24

2020-04-22 09:53:40 Loading data to table yn_hadoop.events_type_log partition (events_type=null, y=null, m=null, d=null)

2020-04-22 09:53:40 


2020-04-22 09:53:41 	 Time taken to load dynamic partitions: 1.559 seconds
	 Time taken for adding to write entity : 0.003 seconds

2020-04-22 09:53:44 MapReduce Jobs Launched: 
Stage-Stage-9: Map: 4   Cumulative CPU: 157.59 sec   HDFS Read: 893623459 HDFS Write: 338538897 SUCCESS
Stage-Stage-5: Map: 8   Cumulative CPU: 39.14 sec   HDFS Read: 138892202 HDFS Write: 138924953 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 16 seconds 730 msec
OK

2020-04-22 09:53:44 Time taken: 103.208 seconds

2020-04-22 09:53:44 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:53:44 yn_logs进行分类处理已完成
2020-04-22 09:53:44 开始处理对日统计数据
2020-04-22 09:53:44 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \

--hive-table user_info \
--hive-overwrite
2020-04-22 09:53:44 Last login: Wed Apr 22 09:51:51 2020 from 10.10.30.233

2020-04-22 09:53:44 su hadoop

2020-04-22 09:53:45 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:53:45 [hadoop@hadoop-01 root]$ 
2020-04-22 09:53:45 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:53:45 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> 

2020-04-22 09:53:45 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 09:53:45 2020-04-22 09:53:48,693 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 09:53:48,718 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 09:53:45 2020-04-22 09:53:48,765 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-22 09:53:45 2020-04-22 09:53:48,774 INFO manager.SqlManager: Using default fetchSize of 1000

2020-04-22 09:53:45 2020-04-22 09:53:48,776 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 09:53:45 2020-04-22 09:53:48,998 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 09:53:49,002 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:53:45 2020-04-22 09:53:49,018 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 09:53:46 注: /tmp/sqoop-hadoop/compile/bdaeaa3e229c6f1af9bee23292e28fb0/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 09:53:49,995 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/bdaeaa3e229c6f1af9bee23292e28fb0/user_info.jar

2020-04-22 09:53:47 2020-04-22 09:53:50,704 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.

2020-04-22 09:53:47 2020-04-22 09:53:50,711 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 09:53:50,712 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 09:53:50,720 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 09:53:50,722 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:53:47 2020-04-22 09:53:50,737 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 09:53:48 2020-04-22 09:53:51,227 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0335

2020-04-22 09:53:49 2020-04-22 09:53:52,883 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 09:53:49 2020-04-22 09:53:52,919 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 09:53:50 2020-04-22 09:53:53,498 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0335
2020-04-22 09:53:53,500 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 09:53:50 2020-04-22 09:53:53,692 INFO conf.Configuration: resource-types.xml not found
2020-04-22 09:53:53,692 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 09:53:50 2020-04-22 09:53:53,738 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0335

2020-04-22 09:53:50 2020-04-22 09:53:53,762 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0335/
2020-04-22 09:53:53,763 INFO mapreduce.Job: Running job: job_1587346943308_0335

2020-04-22 09:53:55 2020-04-22 09:53:58,839 INFO mapreduce.Job: Job job_1587346943308_0335 running in uber mode : false
2020-04-22 09:53:58,841 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 09:54:00 2020-04-22 09:54:04,001 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 09:54:04,013 INFO mapreduce.Job: Job job_1587346943308_0335 completed successfully

2020-04-22 09:54:01 2020-04-22 09:54:04,122 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2738
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2738
		Total vcore-milliseconds taken by all map tasks=2738
		Total megabyte-milliseconds taken by all map tasks=2803712
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=51
		CPU time spen
2020-04-22 09:54:01 t (ms)=1280
		Physical memory (bytes) snapshot=242790400
		Virtual memory (bytes) snapshot=2446950400
		Total committed heap usage (bytes)=191365120
		Peak Map Physical memory (bytes)=242790400
		Peak Map Virtual memory (bytes)=2446950400
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572
2020-04-22 09:54:04,131 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 13.3818 seconds (45.1415 KB/sec)
2020-04-22 09:54:04,135 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-22 09:54:04,135 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info

2020-04-22 09:54:01 2020-04-22 09:54:04,146 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 09:54:04,147 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 09:54:01 2020-04-22 09:54:04,162 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 09:54:04,167 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 09:54:01 2020-04-22 09:54:04,332 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 09:54:01 2020-04-22 09:54:04,879 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-22 09:54:04,902 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 09:54:02 2020-04-22 09:54:05,334 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 09:54:05,335 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 09:54:05,335 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 09:54:05,335 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 09:54:02 2020-04-22 09:54:05,344 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:54:04 2020-04-22 09:54:07,365 INFO hive.HiveImport: Hive Session ID = f1848ab4-757e-4cc4-a08b-d277644d5508

2020-04-22 09:54:04 2020-04-22 09:54:07,415 INFO hive.HiveImport: 
2020-04-22 09:54:07,416 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:54:11 2020-04-22 09:54:14,619 INFO hive.HiveImport: Hive Session ID = f81152c3-0f50-4412-82d4-09d0eeb775f0

2020-04-22 09:54:13 2020-04-22 09:54:16,222 INFO hive.HiveImport: OK
2020-04-22 09:54:16,232 INFO hive.HiveImport: Time taken: 1.529 seconds

2020-04-22 09:54:13 2020-04-22 09:54:16,508 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 09:54:13 2020-04-22 09:54:17,034 INFO hive.HiveImport: OK

2020-04-22 09:54:13 2020-04-22 09:54:17,045 INFO hive.HiveImport: Time taken: 0.791 seconds

2020-04-22 09:54:14 2020-04-22 09:54:17,546 INFO hive.HiveImport: Hive import complete.
2020-04-22 09:54:17,557 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.

2020-04-22 09:54:14 [hadoop@hadoop-01 sx_hadoop_script]$ --hive-table user_info \
> --hive-overwrite
bash: --hive-table: 未找到命令
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:54:15 Last login: Wed Apr 22 09:53:48 2020 from 10.10.30.233

2020-04-22 09:54:15 su hadoop

2020-04-22 09:54:15 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:54:15 [hadoop@hadoop-01 root]$ 
2020-04-22 09:54:15 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:54:15 cat statistic/RegionbyDay.hive

2020-04-22 09:54:15 cat: statistic/RegionbyDay.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:54:15 Last login: Wed Apr 22 09:54:18 2020 from 10.10.30.233

2020-04-22 09:54:15 su hadoop

2020-04-22 09:54:15 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:54:15 [hadoop@hadoop-01 root]$ 
2020-04-22 09:54:15 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:54:15 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 09:54:16 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:54:16 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 09:54:16 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:54:18 Hive Session ID = b07a43c8-6762-4410-95fd-b718c341b2ff

2020-04-22 09:54:18 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:54:24 Hive Session ID = 31148db3-4afe-4f48-b372-635b2db82a96

2020-04-22 09:54:25 OK

2020-04-22 09:54:25 Time taken: 0.778 seconds

2020-04-22 09:54:27 Query ID = hadoop_20200422095428_9082bd4d-3c0c-41f5-b641-813f72d20d75
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 09:54:27 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:54:29 Starting Job = job_1587346943308_0336, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0336/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0336

2020-04-22 09:54:35 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 09:54:35 2020-04-22 09:54:38,888 Stage-1 map = 0%,  reduce = 0%

2020-04-22 09:54:46 2020-04-22 09:54:49,297 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.15 sec

2020-04-22 09:54:47 2020-04-22 09:54:50,331 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.98 sec

2020-04-22 09:54:51 2020-04-22 09:54:54,485 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 19.68 sec

2020-04-22 09:54:52 MapReduce Total cumulative CPU time: 19 seconds 680 msec

2020-04-22 09:54:52 Ended Job = job_1587346943308_0336

2020-04-22 09:54:52 Launching Job 2 out of 3

2020-04-22 09:54:52 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:54:53 Starting Job = job_1587346943308_0337, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0337/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0337

2020-04-22 09:55:03 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 09:55:03 2020-04-22 09:55:06,420 Stage-2 map = 0%,  reduce = 0%

2020-04-22 09:55:08 2020-04-22 09:55:11,712 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.92 sec

2020-04-22 09:55:16 2020-04-22 09:55:19,981 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.74 sec

2020-04-22 09:55:17 MapReduce Total cumulative CPU time: 7 seconds 740 msec

2020-04-22 09:55:17 Ended Job = job_1587346943308_0337

2020-04-22 09:55:18 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 09:55:18 


2020-04-22 09:55:18 	 Time taken to load dynamic partitions: 0.519 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:55:19 Starting Job = job_1587346943308_0338, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0338/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0338

2020-04-22 09:55:28 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 09:55:28 2020-04-22 09:55:31,837 Stage-4 map = 0%,  reduce = 0%

2020-04-22 09:55:34 2020-04-22 09:55:38,049 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.56 sec

2020-04-22 09:55:40 2020-04-22 09:55:43,206 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.1 sec

2020-04-22 09:55:41 MapReduce Total cumulative CPU time: 4 seconds 100 msec

2020-04-22 09:55:41 Ended Job = job_1587346943308_0338

2020-04-22 09:55:41 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 19.68 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.74 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.1 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 520 msec
OK

2020-04-22 09:55:41 Time taken: 76.051 seconds

2020-04-22 09:55:42 Query ID = hadoop_20200422095544_66b9040a-d065-4aeb-b73d-0c3314932344
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:55:42 Starting Job = job_1587346943308_0339, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0339/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0339

2020-04-22 09:55:51 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 09:55:51 2020-04-22 09:55:54,925 Stage-1 map = 0%,  reduce = 0%

2020-04-22 09:56:00 2020-04-22 09:56:03,200 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 5.03 sec

2020-04-22 09:56:02 2020-04-22 09:56:05,266 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.47 sec

2020-04-22 09:56:06 2020-04-22 09:56:09,387 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.85 sec

2020-04-22 09:56:07 MapReduce Total cumulative CPU time: 16 seconds 850 msec

2020-04-22 09:56:07 Ended Job = job_1587346943308_0339

2020-04-22 09:56:07 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:56:08 Starting Job = job_1587346943308_0340, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0340/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0340

2020-04-22 09:56:17 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 09:56:18 2020-04-22 09:56:21,063 Stage-2 map = 0%,  reduce = 0%

2020-04-22 09:56:24 2020-04-22 09:56:27,259 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.44 sec

2020-04-22 09:56:31 2020-04-22 09:56:34,481 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.25 sec

2020-04-22 09:56:32 MapReduce Total cumulative CPU time: 7 seconds 250 msec

2020-04-22 09:56:32 Ended Job = job_1587346943308_0340

2020-04-22 09:56:32 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 09:56:32 


2020-04-22 09:56:32 	 Time taken to load dynamic partitions: 0.179 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:56:33 Starting Job = job_1587346943308_0341, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0341/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0341

2020-04-22 09:56:43 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 09:56:46,424 Stage-4 map = 0%,  reduce = 0%

2020-04-22 09:56:48 2020-04-22 09:56:51,582 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.45 sec

2020-04-22 09:56:55 2020-04-22 09:56:58,808 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.92 sec

2020-04-22 09:56:56 MapReduce Total cumulative CPU time: 3 seconds 920 msec
Ended Job = job_1587346943308_0341

2020-04-22 09:56:57 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 16.85 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.25 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.92 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 28 seconds 20 msec
OK
Time taken: 75.265 seconds

2020-04-22 09:56:59 Query ID = hadoop_20200422095700_50ae6c88-3d1b-4f62-96e3-68cd466aa036
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:57:00 Starting Job = job_1587346943308_0342, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0342/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0342

2020-04-22 09:57:07 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-04-22 09:57:10,244 Stage-1 map = 0%,  reduce = 0%

2020-04-22 09:57:14 2020-04-22 09:57:17,477 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.86 sec

2020-04-22 09:57:20 2020-04-22 09:57:23,654 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.1 sec

2020-04-22 09:57:21 MapReduce Total cumulative CPU time: 7 seconds 100 msec
Ended Job = job_1587346943308_0342

2020-04-22 09:57:21 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:57:22 Starting Job = job_1587346943308_0343, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0343/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0343

2020-04-22 09:57:32 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 09:57:35,857 Stage-4 map = 0%,  reduce = 0%

2020-04-22 09:57:40 2020-04-22 09:57:43,080 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 4.02 sec

2020-04-22 09:57:45 2020-04-22 09:57:48,224 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 7.52 sec

2020-04-22 09:57:46 MapReduce Total cumulative CPU time: 7 seconds 520 msec
Ended Job = job_1587346943308_0343

2020-04-22 09:57:46 Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 09:57:47 Starting Job = job_1587346943308_0344, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0344/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0344

2020-04-22 09:57:57 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1

2020-04-22 09:57:57 2020-04-22 09:58:00,397 Stage-2 map = 0%,  reduce = 0%

2020-04-22 09:58:03 2020-04-22 09:58:06,590 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.39 sec

2020-04-22 09:58:09 2020-04-22 09:58:12,783 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.03 sec

2020-04-22 09:58:10 MapReduce Total cumulative CPU time: 7 seconds 30 msec
Ended Job = job_1587346943308_0344

2020-04-22 09:58:10 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-22 09:58:10 


2020-04-22 09:58:11 	 Time taken to load dynamic partitions: 0.183 seconds
	 Time taken for adding to write entity : 0.0 seconds

2020-04-22 09:58:11 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.1 sec   HDFS Read: 877473 HDFS Write: 182 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 7.52 sec   HDFS Read: 482570 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 7.03 sec   HDFS Read: 21763 HDFS Write: 822 SUCCESS
Total MapReduce CPU Time Spent: 21 seconds 650 msec
OK

2020-04-22 09:58:11 Time taken: 74.107 seconds

2020-04-22 09:58:11 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:58:11 Last login: Wed Apr 22 09:54:18 2020 from 10.10.30.233

2020-04-22 09:58:11 su hadoop

2020-04-22 09:58:11 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:58:12 [hadoop@hadoop-01 root]$ 
2020-04-22 09:58:12 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:58:12 cat statistic/add_user_day.hive
cat: statistic/add_user_day.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:58:12 Last login: Wed Apr 22 09:58:15 2020 from 10.10.30.233

2020-04-22 09:58:12 su hadoop

2020-04-22 09:58:12 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:58:12 [hadoop@hadoop-01 root]$ 
2020-04-22 09:58:12 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:58:12 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 09:58:12 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 09:58:13 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 09:58:15 Hive Session ID = 87d9efaa-17a4-42be-9782-8f93e39c4e51

2020-04-22 09:58:15 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 09:58:22 Hive Session ID = 1cedb380-f946-4251-8f9a-ab4873808c81

2020-04-22 09:58:23 FAILED: SemanticException [Error 10001]: Line 6:11 Table not found 'user_info'

2020-04-22 09:58:24 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:58:24 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 09:58:24 Last login: Wed Apr 22 09:58:15 2020 from 10.10.30.233

2020-04-22 09:58:24 su hadoop

2020-04-22 09:58:24 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:58:24 [hadoop@hadoop-01 root]$ 
2020-04-22 09:58:24 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:58:24 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 09:58:24 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 09:58:24 2020-04-22 09:58:27,994 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 09:58:28,022 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 09:58:25 2020-04-22 09:58:28,100 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 09:58:28,103 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 09:58:25 2020-04-22 09:58:28,313 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 09:58:25 2020-04-22 09:58:28,335 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 09:58:25 2020-04-22 09:58:28,342 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 09:58:26 注: /tmp/sqoop-hadoop/compile/1ee94f9909277d1ee3c1797c3c79beeb/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 09:58:29,356 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/1ee94f9909277d1ee3c1797c3c79beeb/user_info.jar
2020-04-22 09:58:29,362 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 09:58:29,362 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 09:58:29,362 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 09:58:29,362 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 09:58:29,362 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 09:58:29,368 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 09:58:29,368 INFO Configuration.deprecation: mapred.j
2020-04-22 09:58:26 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 09:58:26 2020-04-22 09:58:29,498 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 09:58:27 2020-04-22 09:58:30,216 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=Y/m=m/d=d does not exist
2020-04-22 09:58:30,235 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 09:58:30,236 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 09:58:30,237 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 09:58:27 2020-04-22 09:58:30,681 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0345

2020-04-22 09:58:29 2020-04-22 09:58:32,380 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0345
2020-04-22 09:58:32,384 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=Y/m=m/d=d
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at 
2020-04-22 09:58:29 org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apa
2020-04-22 09:58:29 che.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 09:58:29 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:58:29 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 09:58:29 Last login: Wed Apr 22 09:58:27 2020 from 10.10.30.233

2020-04-22 09:58:29 su hadoop

2020-04-22 09:58:29 [root@hadoop-01 ~]# su hadoop

2020-04-22 09:58:30 [hadoop@hadoop-01 root]$ 
2020-04-22 09:58:30 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:58:30 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 09:58:30 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 09:58:30 2020-04-22 09:58:33,632 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 09:58:30 2020-04-22 09:58:33,659 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 09:58:30 2020-04-22 09:58:33,738 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 09:58:33,741 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 09:58:30 2020-04-22 09:58:33,964 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 09:58:33,980 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 09:58:33,985 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 09:58:31 注: /tmp/sqoop-hadoop/compile/dcd6d3433f69d432ad23ae916f8956ff/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 09:58:34,952 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/dcd6d3433f69d432ad23ae916f8956ff/user_info.jar
2020-04-22 09:58:34,960 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 09:58:34,960 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 09:58:34,960 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 09:58:34,960 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 09:58:34,960 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-22 09:58:31 2020-04-22 09:58:34,965 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 09:58:34,965 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 09:58:32 2020-04-22 09:58:35,070 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 09:58:32 2020-04-22 09:58:35,711 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=Y/m=m/d=d does not exist

2020-04-22 09:58:32 2020-04-22 09:58:35,730 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

2020-04-22 09:58:32 2020-04-22 09:58:35,732 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 09:58:35,733 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 09:58:33 2020-04-22 09:58:36,187 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0346

2020-04-22 09:58:34 2020-04-22 09:58:37,125 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0346

2020-04-22 09:58:34 2020-04-22 09:58:37,199 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=Y/m=m/d=d
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessControlle
2020-04-22 09:58:34 r.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	
2020-04-22 09:58:34 at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 09:58:34 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 09:58:34 日统计数据已完成
2020-04-22 10:11:51 开始处理对日统计数据
2020-04-22 10:12:09 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \

--hive-table user_info \
--hive-overwrite
2020-04-22 10:12:10 Last login: Wed Apr 22 09:58:33 2020 from 10.10.30.233

2020-04-22 10:12:10 su hadoop

2020-04-22 10:12:10 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:12:10 [hadoop@hadoop-01 root]$ 
2020-04-22 10:12:10 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:12:10 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> 

2020-04-22 10:12:10 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 10:12:10 2020-04-22 10:12:13,785 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 10:12:13,811 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 10:12:10 2020-04-22 10:12:13,859 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-22 10:12:13,868 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 10:12:13,870 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 10:12:11 2020-04-22 10:12:14,120 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 10:12:14,124 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 10:12:14,140 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 10:12:12 注: /tmp/sqoop-hadoop/compile/1180c5d3ce617ec1b2ee907028cb7434/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 10:12:15,112 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/1180c5d3ce617ec1b2ee907028cb7434/user_info.jar

2020-04-22 10:12:12 2020-04-22 10:12:15,816 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-22 10:12:15,822 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 10:12:15,823 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 10:12:15,831 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 10:12:15,834 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 10:12:15,850 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 10:12:13 2020-04-22 10:12:16,340 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0347

2020-04-22 10:12:14 2020-04-22 10:12:18,038 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 10:12:15 2020-04-22 10:12:18,072 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 10:12:15 2020-04-22 10:12:18,259 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0347
2020-04-22 10:12:18,260 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 10:12:15 2020-04-22 10:12:18,417 INFO conf.Configuration: resource-types.xml not found
2020-04-22 10:12:18,417 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 10:12:15 2020-04-22 10:12:18,462 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0347

2020-04-22 10:12:15 2020-04-22 10:12:18,488 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0347/
2020-04-22 10:12:18,488 INFO mapreduce.Job: Running job: job_1587346943308_0347

2020-04-22 10:12:21 2020-04-22 10:12:24,573 INFO mapreduce.Job: Job job_1587346943308_0347 running in uber mode : false
2020-04-22 10:12:24,575 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 10:12:26 2020-04-22 10:12:29,642 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 10:12:29,653 INFO mapreduce.Job: Job job_1587346943308_0347 completed successfully

2020-04-22 10:12:26 2020-04-22 10:12:29,768 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2689
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2689
		Total vcore-milliseconds taken by all map tasks=2689
		Total megabyte-milliseconds taken by all map tasks=2753536
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=59
		CPU time spen
2020-04-22 10:12:26 t (ms)=1270
		Physical memory (bytes) snapshot=243322880
		Virtual memory (bytes) snapshot=2447159296
		Total committed heap usage (bytes)=191365120
		Peak Map Physical memory (bytes)=243322880
		Peak Map Virtual memory (bytes)=2447159296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572
2020-04-22 10:12:29,776 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 13.9138 seconds (43.4154 KB/sec)

2020-04-22 10:12:26 2020-04-22 10:12:29,780 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-22 10:12:29,780 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info

2020-04-22 10:12:26 2020-04-22 10:12:29,795 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 10:12:29,796 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 10:12:26 2020-04-22 10:12:29,820 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 10:12:29,829 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 10:12:26 2020-04-22 10:12:30,006 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 10:12:27 2020-04-22 10:12:30,552 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 10:12:27 2020-04-22 10:12:30,572 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 10:12:27 2020-04-22 10:12:31,011 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 10:12:31,011 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 10:12:31,011 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 10:12:31,012 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 10:12:31,017 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 10:12:29 2020-04-22 10:12:33,065 INFO hive.HiveImport: Hive Session ID = a6acadab-9f4d-4e32-a507-98836b84c51f

2020-04-22 10:12:30 2020-04-22 10:12:33,148 INFO hive.HiveImport: 
2020-04-22 10:12:33,148 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 10:12:37 2020-04-22 10:12:40,473 INFO hive.HiveImport: Hive Session ID = 598e0713-e0d9-4e2a-82e5-b04d7159d983

2020-04-22 10:12:38 2020-04-22 10:12:41,917 INFO hive.HiveImport: OK
2020-04-22 10:12:41,928 INFO hive.HiveImport: Time taken: 1.37 seconds

2020-04-22 10:12:39 2020-04-22 10:12:42,213 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 10:12:39 2020-04-22 10:12:42,709 INFO hive.HiveImport: OK
2020-04-22 10:12:42,720 INFO hive.HiveImport: Time taken: 0.77 seconds

2020-04-22 10:12:40 2020-04-22 10:12:43,226 INFO hive.HiveImport: Hive import complete.
2020-04-22 10:12:43,239 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.

2020-04-22 10:12:40 [hadoop@hadoop-01 sx_hadoop_script]$ --hive-table user_info \
> --hive-overwrite
bash: --hive-table: 未找到命令
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:12:40 Last login: Wed Apr 22 10:12:13 2020 from 10.10.30.233

2020-04-22 10:12:40 su hadoop

2020-04-22 10:12:40 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:12:40 [hadoop@hadoop-01 root]$ 
2020-04-22 10:12:40 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:12:40 cat statistic/RegionbyDay.hive
cat: statistic/RegionbyDay.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:12:41 Last login: Wed Apr 22 10:12:43 2020 from 10.10.30.233

2020-04-22 10:12:41 su hadoop

2020-04-22 10:12:41 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:12:41 [hadoop@hadoop-01 root]$ 
2020-04-22 10:12:41 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:12:41 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 10:12:41 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 10:12:42 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 10:12:44 Hive Session ID = 536b457e-33fa-4a35-8f2f-3a568339110a

2020-04-22 10:12:44 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 10:12:51 Hive Session ID = 6494b6cb-7cb0-4033-8651-0045f19f8a28

2020-04-22 10:12:52 OK

2020-04-22 10:12:52 Time taken: 0.84 seconds

2020-04-22 10:12:53 Query ID = hadoop_20200422101255_63beddf9-586a-447d-b32a-39cbb461edcb
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 10:12:53 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:12:55 Starting Job = job_1587346943308_0348, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0348/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0348

2020-04-22 10:13:00 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 10:13:00 2020-04-22 10:13:03,634 Stage-1 map = 0%,  reduce = 0%

2020-04-22 10:13:11 2020-04-22 10:13:14,129 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.13 sec

2020-04-22 10:13:12 2020-04-22 10:13:15,169 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.7 sec

2020-04-22 10:13:17 2020-04-22 10:13:20,355 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.14 sec

2020-04-22 10:13:18 MapReduce Total cumulative CPU time: 20 seconds 140 msec

2020-04-22 10:13:18 Ended Job = job_1587346943308_0348

2020-04-22 10:13:18 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:13:19 Starting Job = job_1587346943308_0349, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0349/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0349

2020-04-22 10:13:29 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 10:13:29 2020-04-22 10:13:32,255 Stage-2 map = 0%,  reduce = 0%

2020-04-22 10:13:35 2020-04-22 10:13:38,460 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.75 sec

2020-04-22 10:13:42 2020-04-22 10:13:45,672 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.03 sec

2020-04-22 10:13:43 MapReduce Total cumulative CPU time: 7 seconds 30 msec

2020-04-22 10:13:43 Ended Job = job_1587346943308_0349

2020-04-22 10:13:43 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 10:13:43 


2020-04-22 10:13:44 	 Time taken to load dynamic partitions: 0.309 seconds
	 Time taken for adding to write entity : 0.002 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:13:44 Starting Job = job_1587346943308_0350, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0350/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0350

2020-04-22 10:13:55 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 10:13:58,066 Stage-4 map = 0%,  reduce = 0%

2020-04-22 10:14:01 2020-04-22 10:14:04,298 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.72 sec

2020-04-22 10:14:08 2020-04-22 10:14:11,535 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.3 sec

2020-04-22 10:14:09 MapReduce Total cumulative CPU time: 4 seconds 300 msec
Ended Job = job_1587346943308_0350

2020-04-22 10:14:10 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 20.14 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.03 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.3 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 470 msec
OK

2020-04-22 10:14:10 Time taken: 78.164 seconds

2020-04-22 10:14:10 Query ID = hadoop_20200422101413_e2fd2c55-671a-4b81-a744-a502dedd2bea
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:14:11 Starting Job = job_1587346943308_0351, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0351/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0351

2020-04-22 10:14:20 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2020-04-22 10:14:23,755 Stage-1 map = 0%,  reduce = 0%

2020-04-22 10:14:29 2020-04-22 10:14:32,128 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 5.23 sec

2020-04-22 10:14:30 2020-04-22 10:14:33,159 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.49 sec

2020-04-22 10:14:36 2020-04-22 10:14:39,341 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.16 sec

2020-04-22 10:14:37 MapReduce Total cumulative CPU time: 16 seconds 160 msec

2020-04-22 10:14:37 Ended Job = job_1587346943308_0351

2020-04-22 10:14:37 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:14:37 Starting Job = job_1587346943308_0352, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0352/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0352

2020-04-22 10:14:48 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 10:14:48 2020-04-22 10:14:51,102 Stage-2 map = 0%,  reduce = 0%

2020-04-22 10:14:54 2020-04-22 10:14:57,304 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec

2020-04-22 10:15:01 2020-04-22 10:15:04,534 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.61 sec

2020-04-22 10:15:02 MapReduce Total cumulative CPU time: 7 seconds 610 msec

2020-04-22 10:15:02 Ended Job = job_1587346943308_0352

2020-04-22 10:15:02 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 10:15:02 


2020-04-22 10:15:02 	 Time taken to load dynamic partitions: 0.279 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:15:03 Starting Job = job_1587346943308_0353, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0353/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0353

2020-04-22 10:15:13 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 10:15:17,045 Stage-4 map = 0%,  reduce = 0%

2020-04-22 10:15:19 2020-04-22 10:15:22,220 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.44 sec

2020-04-22 10:15:24 2020-04-22 10:15:27,356 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.97 sec

2020-04-22 10:15:25 MapReduce Total cumulative CPU time: 3 seconds 970 msec
Ended Job = job_1587346943308_0353

2020-04-22 10:15:25 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 16.16 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.61 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.97 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 27 seconds 740 msec
OK

2020-04-22 10:15:25 Time taken: 75.321 seconds

2020-04-22 10:15:27 Query ID = hadoop_20200422101528_5eedfa6f-ab5f-43a1-ab22-fa9a551a4179
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:15:28 Starting Job = job_1587346943308_0354, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0354/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0354

2020-04-22 10:15:36 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 10:15:36 2020-04-22 10:15:39,602 Stage-1 map = 0%,  reduce = 0%

2020-04-22 10:15:43 2020-04-22 10:15:46,869 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.77 sec

2020-04-22 10:15:49 2020-04-22 10:15:53,059 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.55 sec

2020-04-22 10:15:51 MapReduce Total cumulative CPU time: 9 seconds 550 msec
Ended Job = job_1587346943308_0354
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:15:51 Starting Job = job_1587346943308_0355, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0355/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0355

2020-04-22 10:16:01 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 10:16:01 2020-04-22 10:16:04,827 Stage-4 map = 0%,  reduce = 0%

2020-04-22 10:16:08 2020-04-22 10:16:11,123 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 4.0 sec

2020-04-22 10:16:13 2020-04-22 10:16:16,299 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 7.65 sec

2020-04-22 10:16:15 MapReduce Total cumulative CPU time: 7 seconds 650 msec
Ended Job = job_1587346943308_0355
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:16:15 Starting Job = job_1587346943308_0356, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0356/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0356

2020-04-22 10:16:26 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1

2020-04-22 10:16:26 2020-04-22 10:16:29,129 Stage-2 map = 0%,  reduce = 0%

2020-04-22 10:16:32 2020-04-22 10:16:35,332 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.9 sec

2020-04-22 10:16:38 2020-04-22 10:16:41,523 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.83 sec

2020-04-22 10:16:39 MapReduce Total cumulative CPU time: 6 seconds 830 msec
Ended Job = job_1587346943308_0356
Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-22 10:16:39 


2020-04-22 10:16:39 	 Time taken to load dynamic partitions: 0.246 seconds
	 Time taken for adding to write entity : 0.001 seconds

2020-04-22 10:16:40 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 9.55 sec   HDFS Read: 883908 HDFS Write: 182 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 7.65 sec   HDFS Read: 482570 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.83 sec   HDFS Read: 21763 HDFS Write: 822 SUCCESS
Total MapReduce CPU Time Spent: 24 seconds 30 msec
OK
Time taken: 74.443 seconds

2020-04-22 10:16:40 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:16:40 Last login: Wed Apr 22 10:16:01 2020 from 10.10.30.233

2020-04-22 10:16:40 su hadoop

2020-04-22 10:16:40 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:16:40 [hadoop@hadoop-01 root]$ 
2020-04-22 10:16:40 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:16:40 cat statistic/add_user_day.hive
cat: statistic/add_user_day.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:16:41 Last login: Wed Apr 22 10:16:43 2020 from 10.10.30.233

2020-04-22 10:16:41 su hadoop

2020-04-22 10:16:41 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:16:41 [hadoop@hadoop-01 root]$ 
2020-04-22 10:16:41 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:16:41 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 10:16:41 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 10:16:42 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 10:16:44 Hive Session ID = 3b06bd01-b93c-4cb6-9b7a-37ef6b5eebdb

2020-04-22 10:16:44 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 10:16:50 Hive Session ID = d3bda1a3-d39e-437f-8812-68fe86071824

2020-04-22 10:16:51 FAILED: SemanticException [Error 10001]: Line 6:11 Table not found 'user_info'

2020-04-22 10:16:52 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:28:18 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 10:28:18 Last login: Wed Apr 22 10:22:59 2020 from 10.10.30.233

2020-04-22 10:28:18 su hadoop

2020-04-22 10:28:18 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:28:18 [hadoop@hadoop-01 root]$ 
2020-04-22 10:28:18 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:28:18 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=Y/m=m/d=d/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 10:28:18 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 10:28:19 2020-04-22 10:28:22,363 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 10:28:22,389 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 10:28:19 2020-04-22 10:28:22,466 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 10:28:22,469 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 10:28:19 2020-04-22 10:28:22,705 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 10:28:22,721 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 10:28:22,726 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 10:28:20 注: /tmp/sqoop-hadoop/compile/068c40880352a4ae614ea0e85c54d1f8/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 10:28:23,695 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/068c40880352a4ae614ea0e85c54d1f8/user_info.jar

2020-04-22 10:28:20 2020-04-22 10:28:23,702 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 10:28:23,702 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 10:28:23,702 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 10:28:23,702 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 10:28:23,702 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-22 10:28:20 2020-04-22 10:28:23,708 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 10:28:23,708 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 10:28:20 2020-04-22 10:28:23,806 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 10:28:21 2020-04-22 10:28:24,406 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=Y/m=m/d=d does not exist
2020-04-22 10:28:24,425 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 10:28:24,427 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 10:28:24,427 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 10:28:21 2020-04-22 10:28:24,895 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0357

2020-04-22 10:28:23 2020-04-22 10:28:26,141 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0357

2020-04-22 10:28:23 2020-04-22 10:28:26,249 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=Y/m=m/d=d
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessControlle
2020-04-22 10:28:23 r.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	
2020-04-22 10:28:23 at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 10:28:23 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:28:48 开始处理对日统计数据
2020-04-22 10:28:50 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \

--hive-table user_info \
--hive-overwrite
2020-04-22 10:28:50 Last login: Wed Apr 22 10:28:21 2020 from 10.10.30.233

2020-04-22 10:28:50 su hadoop

2020-04-22 10:28:50 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:28:50 [hadoop@hadoop-01 root]$ 
2020-04-22 10:28:50 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:28:50 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> 

2020-04-22 10:28:50 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 10:28:51 2020-04-22 10:28:54,501 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 10:28:51 2020-04-22 10:28:54,529 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 10:28:51 2020-04-22 10:28:54,578 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-22 10:28:51 2020-04-22 10:28:54,586 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 10:28:54,588 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 10:28:51 2020-04-22 10:28:54,828 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 10:28:54,833 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 10:28:54,854 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 10:28:52 注: /tmp/sqoop-hadoop/compile/47d48e0f530c4dce89f865c491022837/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 10:28:55,863 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/47d48e0f530c4dce89f865c491022837/user_info.jar

2020-04-22 10:28:53 2020-04-22 10:28:56,549 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.

2020-04-22 10:28:53 2020-04-22 10:28:56,554 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 10:28:56,555 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 10:28:53 2020-04-22 10:28:56,564 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 10:28:56,567 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 10:28:53 2020-04-22 10:28:56,582 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 10:28:54 2020-04-22 10:28:57,102 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0358

2020-04-22 10:28:55 2020-04-22 10:28:58,365 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 10:28:55 2020-04-22 10:28:58,401 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 10:28:55 2020-04-22 10:28:58,588 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0358
2020-04-22 10:28:58,590 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 10:28:55 2020-04-22 10:28:58,758 INFO conf.Configuration: resource-types.xml not found
2020-04-22 10:28:58,758 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 10:28:55 2020-04-22 10:28:58,804 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0358

2020-04-22 10:28:55 2020-04-22 10:28:58,833 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0358/
2020-04-22 10:28:58,833 INFO mapreduce.Job: Running job: job_1587346943308_0358

2020-04-22 10:29:00 2020-04-22 10:29:03,952 INFO mapreduce.Job: Job job_1587346943308_0358 running in uber mode : false
2020-04-22 10:29:03,954 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 10:29:06 2020-04-22 10:29:09,139 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 10:29:09,150 INFO mapreduce.Job: Job job_1587346943308_0358 completed successfully

2020-04-22 10:29:06 2020-04-22 10:29:09,240 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2774
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2774
		Total vcore-milliseconds taken by all map tasks=2774
		Total megabyte-milliseconds taken by all map tasks=2840576
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=61
		CPU time spen
2020-04-22 10:29:06 t (ms)=1270
		Physical memory (bytes) snapshot=244322304
		Virtual memory (bytes) snapshot=2446618624
		Total committed heap usage (bytes)=190840832
		Peak Map Physical memory (bytes)=244322304
		Peak Map Virtual memory (bytes)=2446618624
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572
2020-04-22 10:29:09,246 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 12.6537 seconds (47.7389 KB/sec)
2020-04-22 10:29:09,249 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-22 10:29:09,249 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info
2020-04-22 10:29:09,265 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 10:29:09,267 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 10:29:06 2020-04-22 10:29:09,289 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 10:29:09,297 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 10:29:06 2020-04-22 10:29:09,463 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 10:29:06 2020-04-22 10:29:10,037 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 10:29:06 2020-04-22 10:29:10,061 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 10:29:07 2020-04-22 10:29:10,515 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 10:29:10,515 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 10:29:10,515 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 10:29:10,516 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 10:29:10,520 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 10:29:09 2020-04-22 10:29:12,592 INFO hive.HiveImport: Hive Session ID = df9bfe0e-7d55-4489-9b55-16cf3a21a9a8

2020-04-22 10:29:09 2020-04-22 10:29:12,642 INFO hive.HiveImport: 
2020-04-22 10:29:12,642 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 10:29:16 2020-04-22 10:29:19,654 INFO hive.HiveImport: Hive Session ID = eafc845c-a466-4328-a561-24141eef1600

2020-04-22 10:29:18 2020-04-22 10:29:21,079 INFO hive.HiveImport: OK
2020-04-22 10:29:21,092 INFO hive.HiveImport: Time taken: 1.354 seconds

2020-04-22 10:29:18 2020-04-22 10:29:21,398 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 10:29:18 2020-04-22 10:29:21,907 INFO hive.HiveImport: OK
2020-04-22 10:29:21,925 INFO hive.HiveImport: Time taken: 0.803 seconds

2020-04-22 10:29:19 2020-04-22 10:29:22,425 INFO hive.HiveImport: Hive import complete.
2020-04-22 10:29:22,436 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.

2020-04-22 10:29:19 [hadoop@hadoop-01 sx_hadoop_script]$ --hive-table user_info \
> --hive-overwrite
bash: --hive-table: 未找到命令
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:29:19 Last login: Wed Apr 22 10:28:53 2020 from 10.10.30.233

2020-04-22 10:29:19 su hadoop

2020-04-22 10:29:20 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:29:20 [hadoop@hadoop-01 root]$ 
2020-04-22 10:29:20 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:29:20 cat statistic/RegionbyDay.hive
cat: statistic/RegionbyDay.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:29:20 Last login: Wed Apr 22 10:29:23 2020 from 10.10.30.233

2020-04-22 10:29:20 su hadoop

2020-04-22 10:29:20 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:29:20 [hadoop@hadoop-01 root]$ 
2020-04-22 10:29:20 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:29:20 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 10:29:21 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 10:29:21 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 10:29:23 Hive Session ID = ea6a4f69-aaf5-4c95-842b-25b018658761

2020-04-22 10:29:23 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 10:29:30 Hive Session ID = f0c4ed3b-0cc0-48fb-9b0e-123b476cd0f5

2020-04-22 10:29:30 OK
Time taken: 0.79 seconds

2020-04-22 10:29:32 Query ID = hadoop_20200422102934_119cf26f-567d-48f7-8d18-7bd6b7f3c006
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 10:29:32 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:29:34 Starting Job = job_1587346943308_0359, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0359/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0359

2020-04-22 10:29:40 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 10:29:40 2020-04-22 10:29:43,629 Stage-1 map = 0%,  reduce = 0%

2020-04-22 10:29:49 2020-04-22 10:29:52,982 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.39 sec

2020-04-22 10:29:50 2020-04-22 10:29:54,008 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.26 sec

2020-04-22 10:29:56 2020-04-22 10:29:59,191 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.44 sec

2020-04-22 10:29:57 MapReduce Total cumulative CPU time: 20 seconds 440 msec
Ended Job = job_1587346943308_0359

2020-04-22 10:29:57 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:29:57 Starting Job = job_1587346943308_0360, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0360/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0360

2020-04-22 10:30:09 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 10:30:09 2020-04-22 10:30:12,089 Stage-2 map = 0%,  reduce = 0%

2020-04-22 10:30:15 2020-04-22 10:30:18,314 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.5 sec

2020-04-22 10:30:22 2020-04-22 10:30:25,543 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.29 sec

2020-04-22 10:30:23 MapReduce Total cumulative CPU time: 7 seconds 290 msec
Ended Job = job_1587346943308_0360

2020-04-22 10:30:23 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 10:30:23 


2020-04-22 10:30:24 	 Time taken to load dynamic partitions: 0.371 seconds
	 Time taken for adding to write entity : 0.002 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:30:24 Starting Job = job_1587346943308_0361, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0361/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0361

2020-04-22 10:30:35 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 10:30:38,197 Stage-4 map = 0%,  reduce = 0%

2020-04-22 10:30:40 2020-04-22 10:30:43,372 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.53 sec

2020-04-22 10:30:47 2020-04-22 10:30:50,599 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.16 sec

2020-04-22 10:30:48 MapReduce Total cumulative CPU time: 4 seconds 160 msec

2020-04-22 10:30:48 Ended Job = job_1587346943308_0361

2020-04-22 10:30:49 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 20.44 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.29 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.16 sec   HDFS Read: 16665 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 890 msec
OK

2020-04-22 10:30:49 Time taken: 78.361 seconds

2020-04-22 10:30:49 Query ID = hadoop_20200422103052_388af021-d75d-4f04-bf45-e3e52a822811
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:30:50 Starting Job = job_1587346943308_0362, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0362/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0362

2020-04-22 10:30:59 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 10:30:59 2020-04-22 10:31:02,493 Stage-1 map = 0%,  reduce = 0%

2020-04-22 10:31:07 2020-04-22 10:31:10,825 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 5.26 sec

2020-04-22 10:31:09 2020-04-22 10:31:12,912 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.93 sec

2020-04-22 10:31:14 2020-04-22 10:31:18,068 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.24 sec

2020-04-22 10:31:16 MapReduce Total cumulative CPU time: 17 seconds 240 msec
Ended Job = job_1587346943308_0362
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:31:16 Starting Job = job_1587346943308_0363, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0363/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0363

2020-04-22 10:31:26 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 10:31:26 2020-04-22 10:31:29,843 Stage-2 map = 0%,  reduce = 0%

2020-04-22 10:31:32 2020-04-22 10:31:36,040 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.61 sec

2020-04-22 10:31:41 2020-04-22 10:31:44,288 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.24 sec

2020-04-22 10:31:42 MapReduce Total cumulative CPU time: 7 seconds 240 msec

2020-04-22 10:31:42 Ended Job = job_1587346943308_0363

2020-04-22 10:31:42 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 10:31:42 


2020-04-22 10:31:42 	 Time taken to load dynamic partitions: 0.337 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:31:43 Starting Job = job_1587346943308_0364, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0364/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0364

2020-04-22 10:31:52 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 10:31:52 2020-04-22 10:31:55,470 Stage-4 map = 0%,  reduce = 0%

2020-04-22 10:31:58 2020-04-22 10:32:01,650 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.44 sec

2020-04-22 10:32:04 2020-04-22 10:32:07,835 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.96 sec

2020-04-22 10:32:05 MapReduce Total cumulative CPU time: 3 seconds 960 msec

2020-04-22 10:32:05 Ended Job = job_1587346943308_0364

2020-04-22 10:32:06 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 17.24 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.24 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.96 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 28 seconds 440 msec
OK
Time taken: 76.731 seconds

2020-04-22 10:32:08 Query ID = hadoop_20200422103209_48675d96-f29c-42ae-97c6-09d7b04fd401
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:32:08 Starting Job = job_1587346943308_0365, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0365/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0365

2020-04-22 10:32:16 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1

2020-04-22 10:32:16 2020-04-22 10:32:19,778 Stage-1 map = 0%,  reduce = 0%

2020-04-22 10:32:23 2020-04-22 10:32:27,009 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.86 sec

2020-04-22 10:32:29 2020-04-22 10:32:32,194 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.59 sec

2020-04-22 10:32:31 MapReduce Total cumulative CPU time: 7 seconds 590 msec
Ended Job = job_1587346943308_0365
Launching Job 2 out of 3

2020-04-22 10:32:31 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:32:31 Starting Job = job_1587346943308_0366, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0366/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0366

2020-04-22 10:32:41 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 10:32:41 2020-04-22 10:32:44,977 Stage-4 map = 0%,  reduce = 0%

2020-04-22 10:32:48 2020-04-22 10:32:51,175 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 3.86 sec

2020-04-22 10:32:54 2020-04-22 10:32:57,359 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 6.83 sec

2020-04-22 10:32:55 MapReduce Total cumulative CPU time: 6 seconds 830 msec
Ended Job = job_1587346943308_0366

2020-04-22 10:32:55 Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 10:32:56 Starting Job = job_1587346943308_0367, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0367/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0367

2020-04-22 10:33:06 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1

2020-04-22 10:33:06 2020-04-22 10:33:09,510 Stage-2 map = 0%,  reduce = 0%

2020-04-22 10:33:12 2020-04-22 10:33:15,779 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.26 sec

2020-04-22 10:33:19 2020-04-22 10:33:22,988 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.81 sec

2020-04-22 10:33:20 MapReduce Total cumulative CPU time: 5 seconds 810 msec
Ended Job = job_1587346943308_0367

2020-04-22 10:33:20 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-22 10:33:21 


2020-04-22 10:33:21 	 Time taken to load dynamic partitions: 0.272 seconds
	 Time taken for adding to write entity : 0.0 seconds

2020-04-22 10:33:21 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.59 sec   HDFS Read: 877473 HDFS Write: 182 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 6.83 sec   HDFS Read: 482570 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 5.81 sec   HDFS Read: 21763 HDFS Write: 822 SUCCESS
Total MapReduce CPU Time Spent: 20 seconds 230 msec
OK

2020-04-22 10:33:21 Time taken: 75.35 seconds

2020-04-22 10:33:21 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:33:22 Last login: Wed Apr 22 10:29:23 2020 from 10.10.30.233

2020-04-22 10:33:22 su hadoop

2020-04-22 10:33:22 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:33:22 [hadoop@hadoop-01 root]$ 
2020-04-22 10:33:22 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:33:22 cat statistic/add_user_day.hive
cat: statistic/add_user_day.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:33:22 Last login: Wed Apr 22 10:33:25 2020 from 10.10.30.233

2020-04-22 10:33:22 su hadoop

2020-04-22 10:33:22 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:33:22 [hadoop@hadoop-01 root]$ 
2020-04-22 10:33:22 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:33:22 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 10:33:23 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 10:33:23 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 10:33:25 Hive Session ID = a6361671-8a6f-44ac-8c86-ec6ff0afc267

2020-04-22 10:33:25 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 10:33:32 Hive Session ID = a90184e3-3f27-4d9f-bd52-d05fb5460020

2020-04-22 10:33:33 FAILED: SemanticException [Error 10001]: Line 6:11 Table not found 'user_info'

2020-04-22 10:33:34 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:33:34 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 10:33:34 Last login: Wed Apr 22 10:33:25 2020 from 10.10.30.233

2020-04-22 10:33:34 su hadoop

2020-04-22 10:33:34 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:33:34 [hadoop@hadoop-01 root]$ 
2020-04-22 10:33:34 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:33:34 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 10:33:34 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 10:33:35 2020-04-22 10:33:38,137 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 10:33:35 2020-04-22 10:33:38,163 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 10:33:35 2020-04-22 10:33:38,265 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 10:33:38,270 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 10:33:35 2020-04-22 10:33:38,495 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 10:33:35 2020-04-22 10:33:38,516 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 10:33:35 2020-04-22 10:33:38,523 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 10:33:36 注: /tmp/sqoop-hadoop/compile/3e93bd8b41d713afc001016fcdb09c76/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 10:33:39,524 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/3e93bd8b41d713afc001016fcdb09c76/user_info.jar
2020-04-22 10:33:39,532 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 10:33:39,532 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 10:33:39,532 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 10:33:39,532 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 10:33:39,532 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 10:33:39,537 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 10:33:39,537 INFO Configuration.deprecation: mapred.j
2020-04-22 10:33:36 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 10:33:36 2020-04-22 10:33:39,633 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 10:33:37 2020-04-22 10:33:40,226 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=23 does not exist
2020-04-22 10:33:40,244 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 10:33:40,246 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 10:33:40,246 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 10:33:37 2020-04-22 10:33:40,707 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0368

2020-04-22 10:33:39 2020-04-22 10:33:42,541 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0368
2020-04-22 10:33:42,546 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=23
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
2020-04-22 10:33:39 
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at or
2020-04-22 10:33:39 g.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 10:33:39 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:33:39 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 10:33:40 Last login: Wed Apr 22 10:33:37 2020 from 10.10.30.233

2020-04-22 10:33:40 su hadoop

2020-04-22 10:33:40 [root@hadoop-01 ~]# su hadoop

2020-04-22 10:33:40 [hadoop@hadoop-01 root]$ 
2020-04-22 10:33:40 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:33:40 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 10:33:40 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 10:33:40 2020-04-22 10:33:43,821 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 10:33:43,847 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 10:33:40 2020-04-22 10:33:43,925 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.

2020-04-22 10:33:40 2020-04-22 10:33:43,928 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 10:33:41 2020-04-22 10:33:44,161 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 10:33:44,176 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 10:33:44,181 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 10:33:42 注: /tmp/sqoop-hadoop/compile/58f558478f788119147d0914e1825204/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 10:33:45,145 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/58f558478f788119147d0914e1825204/user_info.jar
2020-04-22 10:33:45,152 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 10:33:45,152 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 10:33:45,152 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 10:33:45,152 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 10:33:45,152 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 10:33:45,158 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 10:33:45,158 INFO Configuration.deprecation: mapred.j
2020-04-22 10:33:42 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 10:33:42 2020-04-22 10:33:45,264 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 10:33:42 2020-04-22 10:33:45,932 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=24 does not exist
2020-04-22 10:33:45,952 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 10:33:45,953 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 10:33:45,953 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 10:33:43 2020-04-22 10:33:46,391 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0369

2020-04-22 10:33:45 2020-04-22 10:33:48,906 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0369
2020-04-22 10:33:48,910 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=24
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
2020-04-22 10:33:45 
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at or
2020-04-22 10:33:45 g.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 10:33:46 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 10:33:46 日统计数据已完成
2020-04-22 11:30:25 开始处理对日统计数据
2020-04-22 11:30:25 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \

--hive-table user_info \
--hive-overwrite
2020-04-22 11:30:25 Last login: Wed Apr 22 10:33:43 2020 from 10.10.30.233

2020-04-22 11:30:25 su hadoop

2020-04-22 11:30:25 [root@hadoop-01 ~]# su hadoop

2020-04-22 11:30:25 [hadoop@hadoop-01 root]$ 
2020-04-22 11:30:25 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:30:25 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> 

2020-04-22 11:30:25 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 11:30:26 2020-04-22 11:30:29,190 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 11:30:29,216 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 11:30:26 2020-04-22 11:30:29,266 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-22 11:30:29,274 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 11:30:29,276 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 11:30:26 2020-04-22 11:30:29,509 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 11:30:26 2020-04-22 11:30:29,514 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 11:30:26 2020-04-22 11:30:29,529 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 11:30:27 注: /tmp/sqoop-hadoop/compile/81c68c6e6b272bd6955ee4999c50f048/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 11:30:30,507 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/81c68c6e6b272bd6955ee4999c50f048/user_info.jar

2020-04-22 11:30:28 2020-04-22 11:30:31,238 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-22 11:30:31,251 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 11:30:31,252 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 11:30:31,256 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 11:30:31,258 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 11:30:28 2020-04-22 11:30:31,282 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 11:30:28 2020-04-22 11:30:31,785 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0370

2020-04-22 11:30:29 2020-04-22 11:30:33,035 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 11:30:30 2020-04-22 11:30:33,071 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 11:30:30 2020-04-22 11:30:33,245 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0370
2020-04-22 11:30:33,247 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 11:30:30 2020-04-22 11:30:33,439 INFO conf.Configuration: resource-types.xml not found
2020-04-22 11:30:33,439 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-04-22 11:30:33,483 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0370

2020-04-22 11:30:30 2020-04-22 11:30:33,507 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0370/
2020-04-22 11:30:33,507 INFO mapreduce.Job: Running job: job_1587346943308_0370

2020-04-22 11:30:36 2020-04-22 11:30:39,594 INFO mapreduce.Job: Job job_1587346943308_0370 running in uber mode : false
2020-04-22 11:30:39,596 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 11:30:41 2020-04-22 11:30:44,667 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 11:30:44,678 INFO mapreduce.Job: Job job_1587346943308_0370 completed successfully

2020-04-22 11:30:41 2020-04-22 11:30:44,769 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2683
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2683
		Total vcore-milliseconds taken by all map tasks=2683
		Total megabyte-milliseconds taken by all map tasks=2747392
	Map-Reduce Framework
		Map input records=0
		Map output records=0
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=61
		CPU time spent (ms)=850
	
2020-04-22 11:30:41 	Physical memory (bytes) snapshot=241340416
		Virtual memory (bytes) snapshot=2446360576
		Total committed heap usage (bytes)=190840832
		Peak Map Physical memory (bytes)=241340416
		Peak Map Virtual memory (bytes)=2446360576
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0

2020-04-22 11:30:41 2020-04-22 11:30:44,775 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 13.4861 seconds (0 bytes/sec)

2020-04-22 11:30:41 2020-04-22 11:30:44,778 INFO mapreduce.ImportJobBase: Retrieved 0 records.
2020-04-22 11:30:44,778 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info

2020-04-22 11:30:41 2020-04-22 11:30:44,792 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 11:30:44,795 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 11:30:41 2020-04-22 11:30:44,819 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 11:30:44,827 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 11:30:41 2020-04-22 11:30:45,024 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 11:30:42 2020-04-22 11:30:45,588 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-22 11:30:45,620 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 11:30:43 2020-04-22 11:30:46,091 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 11:30:46,091 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 11:30:46,091 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 11:30:46,091 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 11:30:43 2020-04-22 11:30:46,103 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 11:30:45 2020-04-22 11:30:48,246 INFO hive.HiveImport: Hive Session ID = 14d4b39a-20c0-46b9-93f4-a559f71b7dc8

2020-04-22 11:30:45 2020-04-22 11:30:48,295 INFO hive.HiveImport: 
2020-04-22 11:30:48,296 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 11:30:52 2020-04-22 11:30:55,530 INFO hive.HiveImport: Hive Session ID = 40af2579-e17b-46e0-b47a-6faeb2618a8f

2020-04-22 11:30:53 2020-04-22 11:30:56,847 INFO hive.HiveImport: OK

2020-04-22 11:30:53 2020-04-22 11:30:56,858 INFO hive.HiveImport: Time taken: 1.252 seconds

2020-04-22 11:30:53 2020-04-22 11:30:57,019 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 11:30:54 2020-04-22 11:30:57,563 INFO hive.HiveImport: OK
2020-04-22 11:30:57,573 INFO hive.HiveImport: Time taken: 0.693 seconds

2020-04-22 11:30:54 2020-04-22 11:30:58,074 INFO hive.HiveImport: Hive import complete.

2020-04-22 11:30:54 2020-04-22 11:30:58,086 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.

2020-04-22 11:30:55 [hadoop@hadoop-01 sx_hadoop_script]$ --hive-table user_info \
> --hive-overwrite
bash: --hive-table: 未找到命令
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:30:55 Last login: Wed Apr 22 11:30:28 2020 from 10.10.30.233

2020-04-22 11:30:55 su hadoop

2020-04-22 11:30:55 [root@hadoop-01 ~]# su hadoop

2020-04-22 11:30:55 [hadoop@hadoop-01 root]$ 
2020-04-22 11:30:55 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:30:55 cat statistic/RegionbyDay.hive
cat: statistic/RegionbyDay.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:30:55 Last login: Wed Apr 22 11:30:58 2020 from 10.10.30.233

2020-04-22 11:30:55 su hadoop

2020-04-22 11:30:55 [root@hadoop-01 ~]# su hadoop

2020-04-22 11:30:55 [hadoop@hadoop-01 root]$ 
2020-04-22 11:30:55 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:30:56 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 11:30:56 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 11:30:57 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 11:30:57 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 11:30:59 Hive Session ID = 325876a9-e75d-4eb0-baee-1c33238c0ccd

2020-04-22 11:30:59 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 11:31:05 Hive Session ID = 680e0496-69ef-4480-b01c-904be972f7fc

2020-04-22 11:31:06 OK
Time taken: 0.816 seconds

2020-04-22 11:31:08 Query ID = hadoop_20200422113109_43526e48-935c-4c37-94b3-d3ecc25263f7
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 11:31:08 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 11:31:11 Starting Job = job_1587346943308_0371, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0371/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0371

2020-04-22 11:31:17 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 11:31:17 2020-04-22 11:31:20,336 Stage-1 map = 0%,  reduce = 0%

2020-04-22 11:31:26 2020-04-22 11:31:29,717 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.05 sec

2020-04-22 11:31:28 2020-04-22 11:31:31,787 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.0 sec

2020-04-22 11:31:32 2020-04-22 11:31:35,940 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 19.88 sec

2020-04-22 11:31:33 MapReduce Total cumulative CPU time: 19 seconds 880 msec

2020-04-22 11:31:33 Ended Job = job_1587346943308_0371

2020-04-22 11:31:34 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 11:31:34 Starting Job = job_1587346943308_0372, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0372/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0372

2020-04-22 11:31:44 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2020-04-22 11:31:47,843 Stage-2 map = 0%,  reduce = 0%

2020-04-22 11:31:50 2020-04-22 11:31:54,061 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.61 sec

2020-04-22 11:31:58 2020-04-22 11:32:01,279 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.79 sec

2020-04-22 11:32:00 MapReduce Total cumulative CPU time: 7 seconds 790 msec

2020-04-22 11:32:00 Ended Job = job_1587346943308_0372

2020-04-22 11:32:00 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 11:32:00 


2020-04-22 11:32:00 	 Time taken to load dynamic partitions: 0.326 seconds
	 Time taken for adding to write entity : 0.002 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 11:32:01 Starting Job = job_1587346943308_0373, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0373/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0373

2020-04-22 11:32:10 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 11:32:10 2020-04-22 11:32:13,495 Stage-4 map = 0%,  reduce = 0%

2020-04-22 11:32:16 2020-04-22 11:32:19,691 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.4 sec

2020-04-22 11:32:22 2020-04-22 11:32:25,878 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.99 sec

2020-04-22 11:32:23 MapReduce Total cumulative CPU time: 3 seconds 990 msec
Ended Job = job_1587346943308_0373

2020-04-22 11:32:24 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 19.88 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.79 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.99 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 660 msec
OK

2020-04-22 11:32:24 Time taken: 77.842 seconds

2020-04-22 11:32:25 Query ID = hadoop_20200422113227_f8941c56-4db0-4443-956e-ce79e7465ed1
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 11:32:25 Starting Job = job_1587346943308_0374, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0374/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0374

2020-04-22 11:32:34 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2020-04-22 11:32:37,854 Stage-1 map = 0%,  reduce = 0%

2020-04-22 11:32:43 2020-04-22 11:32:46,182 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 5.4 sec

2020-04-22 11:32:45 2020-04-22 11:32:48,252 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.81 sec

2020-04-22 11:32:51 2020-04-22 11:32:54,440 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.52 sec

2020-04-22 11:32:52 MapReduce Total cumulative CPU time: 17 seconds 520 msec

2020-04-22 11:32:52 Ended Job = job_1587346943308_0374

2020-04-22 11:32:52 Launching Job 2 out of 3

2020-04-22 11:32:52 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 11:32:52 Starting Job = job_1587346943308_0375, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0375/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0375

2020-04-22 11:33:03 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2020-04-22 11:33:06,203 Stage-2 map = 0%,  reduce = 0%

2020-04-22 11:33:09 2020-04-22 11:33:12,419 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.65 sec

2020-04-22 11:33:16 2020-04-22 11:33:19,628 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.52 sec

2020-04-22 11:33:18 MapReduce Total cumulative CPU time: 7 seconds 520 msec
Ended Job = job_1587346943308_0375

2020-04-22 11:33:18 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 11:33:18 


2020-04-22 11:33:18 	 Time taken to load dynamic partitions: 0.259 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 11:33:19 Starting Job = job_1587346943308_0376, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0376/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0376

2020-04-22 11:33:29 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 11:33:29 2020-04-22 11:33:32,708 Stage-4 map = 0%,  reduce = 0%

2020-04-22 11:33:35 2020-04-22 11:33:38,920 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec

2020-04-22 11:33:42 2020-04-22 11:33:45,112 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.03 sec

2020-04-22 11:33:43 MapReduce Total cumulative CPU time: 4 seconds 30 msec
Ended Job = job_1587346943308_0376

2020-04-22 11:33:43 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 17.52 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.52 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.03 sec   HDFS Read: 19215 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 29 seconds 70 msec
OK
Time taken: 78.707 seconds

2020-04-22 11:33:45 Query ID = hadoop_20200422113346_1fee4ba6-d287-4bef-a2e7-26b02b369ae1
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 11:33:45 Starting Job = job_1587346943308_0377, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0377/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0377

2020-04-22 11:33:53 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1

2020-04-22 11:33:53 2020-04-22 11:33:56,967 Stage-1 map = 0%,  reduce = 0%

2020-04-22 11:34:01 2020-04-22 11:34:04,188 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.04 sec

2020-04-22 11:34:07 2020-04-22 11:34:10,362 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.38 sec

2020-04-22 11:34:08 MapReduce Total cumulative CPU time: 7 seconds 380 msec
Ended Job = job_1587346943308_0377

2020-04-22 11:34:08 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 11:34:08 Starting Job = job_1587346943308_0378, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0378/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0378

2020-04-22 11:34:19 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 11:34:22,141 Stage-4 map = 0%,  reduce = 0%

2020-04-22 11:34:26 2020-04-22 11:34:29,356 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 3.02 sec

2020-04-22 11:34:31 2020-04-22 11:34:34,530 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 6.6 sec

2020-04-22 11:34:32 MapReduce Total cumulative CPU time: 6 seconds 600 msec

2020-04-22 11:34:32 Ended Job = job_1587346943308_0378

2020-04-22 11:34:32 Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 11:34:33 Starting Job = job_1587346943308_0379, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0379/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0379

2020-04-22 11:34:44 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2020-04-22 11:34:47,338 Stage-2 map = 0%,  reduce = 0%

2020-04-22 11:34:50 2020-04-22 11:34:53,633 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.6 sec

2020-04-22 11:34:55 2020-04-22 11:34:58,797 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.5 sec

2020-04-22 11:34:57 MapReduce Total cumulative CPU time: 6 seconds 500 msec

2020-04-22 11:34:57 Ended Job = job_1587346943308_0379

2020-04-22 11:34:57 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-22 11:34:57 


2020-04-22 11:34:58 	 Time taken to load dynamic partitions: 0.226 seconds
	 Time taken for adding to write entity : 0.0 seconds

2020-04-22 11:34:58 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.38 sec   HDFS Read: 877473 HDFS Write: 182 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 6.6 sec   HDFS Read: 482570 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.5 sec   HDFS Read: 21763 HDFS Write: 822 SUCCESS
Total MapReduce CPU Time Spent: 20 seconds 480 msec
OK
Time taken: 74.862 seconds

2020-04-22 11:34:58 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:34:58 Last login: Wed Apr 22 11:30:59 2020 from 10.10.30.233

2020-04-22 11:34:58 su hadoop

2020-04-22 11:34:58 [root@hadoop-01 ~]# su hadoop

2020-04-22 11:34:59 [hadoop@hadoop-01 root]$ 
2020-04-22 11:34:59 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:34:59 cat statistic/add_user_day.hive

2020-04-22 11:34:59 cat: statistic/add_user_day.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:34:59 Last login: Wed Apr 22 11:35:02 2020 from 10.10.30.233

2020-04-22 11:34:59 su hadoop

2020-04-22 11:34:59 [root@hadoop-01 ~]# su hadoop

2020-04-22 11:34:59 [hadoop@hadoop-01 root]$ 
2020-04-22 11:34:59 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:34:59 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 11:34:59 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 11:35:00 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 11:35:02 Hive Session ID = 744c5c08-aead-4a74-8219-2652140deea0

2020-04-22 11:35:02 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 11:35:08 Hive Session ID = 0fd07165-2192-4c47-8c2c-b5e1adf3e927

2020-04-22 11:35:10 FAILED: SemanticException [Error 10001]: Line 6:11 Table not found 'user_info'

2020-04-22 11:35:10 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:35:10 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 11:35:11 Last login: Wed Apr 22 11:35:02 2020 from 10.10.30.233

2020-04-22 11:35:11 su hadoop

2020-04-22 11:35:11 [root@hadoop-01 ~]# su hadoop

2020-04-22 11:35:11 [hadoop@hadoop-01 root]$ 
2020-04-22 11:35:11 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:35:11 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 11:35:11 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 11:35:11 2020-04-22 11:35:14,787 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 11:35:11 2020-04-22 11:35:14,812 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 11:35:11 2020-04-22 11:35:14,889 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 11:35:14,892 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 11:35:12 2020-04-22 11:35:15,089 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 11:35:12 2020-04-22 11:35:15,105 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 11:35:12 2020-04-22 11:35:15,110 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 11:35:12 注: /tmp/sqoop-hadoop/compile/7ed6f35d9f498bb38322bd3d3e5799b1/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 11:35:16,055 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/7ed6f35d9f498bb38322bd3d3e5799b1/user_info.jar
2020-04-22 11:35:16,062 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 11:35:16,062 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 11:35:16,062 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 11:35:16,062 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 11:35:16,062 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 11:35:16,068 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 11:35:16,068 INFO Configuration.deprecation: mapred.j
2020-04-22 11:35:12 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 11:35:13 2020-04-22 11:35:16,198 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 11:35:13 2020-04-22 11:35:16,861 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=23 does not exist

2020-04-22 11:35:13 2020-04-22 11:35:16,880 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 11:35:16,882 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 11:35:16,882 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 11:35:14 2020-04-22 11:35:17,427 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0380

2020-04-22 11:35:16 2020-04-22 11:35:19,124 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0380
2020-04-22 11:35:19,128 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=23
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
2020-04-22 11:35:16 
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at or
2020-04-22 11:35:16 g.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 11:35:16 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:35:16 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 11:35:16 Last login: Wed Apr 22 11:35:14 2020 from 10.10.30.233

2020-04-22 11:35:16 su hadoop

2020-04-22 11:35:16 [root@hadoop-01 ~]# su hadoop

2020-04-22 11:35:16 [hadoop@hadoop-01 root]$ 
2020-04-22 11:35:16 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:35:16 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 11:35:16 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 11:35:17 2020-04-22 11:35:20,452 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 11:35:20,478 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 11:35:17 2020-04-22 11:35:20,563 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 11:35:20,566 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 11:35:17 2020-04-22 11:35:20,767 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 11:35:20,782 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 11:35:17 2020-04-22 11:35:20,787 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 11:35:18 注: /tmp/sqoop-hadoop/compile/02da8ec329d0a30a3d8ef76fe2d8ff1e/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 11:35:21,784 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/02da8ec329d0a30a3d8ef76fe2d8ff1e/user_info.jar
2020-04-22 11:35:21,791 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 11:35:21,791 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 11:35:21,791 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 11:35:21,791 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 11:35:21,791 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 11:35:21,796 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 11:35:21,796 INFO Configuration.deprecation: mapred.j
2020-04-22 11:35:18 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 11:35:18 2020-04-22 11:35:21,895 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 11:35:19 2020-04-22 11:35:22,497 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=24 does not exist

2020-04-22 11:35:19 2020-04-22 11:35:22,516 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

2020-04-22 11:35:19 2020-04-22 11:35:22,518 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 11:35:22,518 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 11:35:19 2020-04-22 11:35:22,956 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0381

2020-04-22 11:35:20 2020-04-22 11:35:23,777 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0381

2020-04-22 11:35:20 2020-04-22 11:35:23,840 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=24
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessCont
2020-04-22 11:35:20 roller.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:24
2020-04-22 11:35:20 3)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 11:35:21 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 11:35:21 日统计数据已完成
2020-04-22 14:05:32 开始处理对日统计数据
2020-04-22 14:05:32 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \

--hive-table user_info \
--hive-overwrite
2020-04-22 14:05:32 Last login: Wed Apr 22 11:35:19 2020 from 10.10.30.233

2020-04-22 14:05:32 su hadoop

2020-04-22 14:05:32 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:05:32 [hadoop@hadoop-01 root]$ 
2020-04-22 14:05:32 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:05:32 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> 

2020-04-22 14:05:32 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:05:32 2020-04-22 14:05:36,543 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 14:05:32 2020-04-22 14:05:36,572 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:05:32 2020-04-22 14:05:36,618 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-22 14:05:32 2020-04-22 14:05:36,626 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 14:05:36,628 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:05:33 2020-04-22 14:05:36,852 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:05:33 2020-04-22 14:05:36,856 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:05:33 2020-04-22 14:05:36,871 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:05:34 注: /tmp/sqoop-hadoop/compile/9ead7aff8b784ae735ac0e5c2b838991/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:05:37,879 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9ead7aff8b784ae735ac0e5c2b838991/user_info.jar

2020-04-22 14:05:34 2020-04-22 14:05:38,574 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-22 14:05:38,581 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 14:05:38,581 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 14:05:38,587 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 14:05:38,589 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:05:38,598 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:05:35 2020-04-22 14:05:39,063 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0382

2020-04-22 14:05:37 2020-04-22 14:05:40,737 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 14:05:37 2020-04-22 14:05:40,775 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 14:05:37 2020-04-22 14:05:40,952 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0382
2020-04-22 14:05:40,953 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 14:05:37 2020-04-22 14:05:41,116 INFO conf.Configuration: resource-types.xml not found

2020-04-22 14:05:37 2020-04-22 14:05:41,116 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 14:05:37 2020-04-22 14:05:41,163 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0382

2020-04-22 14:05:37 2020-04-22 14:05:41,190 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0382/
2020-04-22 14:05:41,191 INFO mapreduce.Job: Running job: job_1587346943308_0382

2020-04-22 14:05:42 2020-04-22 14:05:46,284 INFO mapreduce.Job: Job job_1587346943308_0382 running in uber mode : false
2020-04-22 14:05:46,285 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 14:05:47 2020-04-22 14:05:51,477 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 14:05:51,489 INFO mapreduce.Job: Job job_1587346943308_0382 completed successfully

2020-04-22 14:05:47 2020-04-22 14:05:51,601 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2565
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2565
		Total vcore-milliseconds taken by all map tasks=2565
		Total megabyte-milliseconds taken by all map tasks=2626560
	Map-Reduce Framework
		Map input records=0
		Map output records=0
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=56
		CPU time spent (ms)=820
	
2020-04-22 14:05:47 	Physical memory (bytes) snapshot=242606080
		Virtual memory (bytes) snapshot=2445811712
		Total committed heap usage (bytes)=192413696
		Peak Map Physical memory (bytes)=242606080
		Peak Map Virtual memory (bytes)=2445811712
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 14:05:51,608 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 13.0007 seconds (0 bytes/sec)
2020-04-22 14:05:51,613 INFO mapreduce.ImportJobBase: Retrieved 0 records.
2020-04-22 14:05:51,613 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info
2020-04-22 14:05:51,626 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:05:51,628 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:05:47 2020-04-22 14:05:51,653 INFO hive.HiveImport: Loading uploaded data into Hive

2020-04-22 14:05:47 2020-04-22 14:05:51,662 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 14:05:48 2020-04-22 14:05:51,851 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 14:05:48 2020-04-22 14:05:52,451 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 14:05:48 2020-04-22 14:05:52,493 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 14:05:49 2020-04-22 14:05:53,017 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 14:05:53,018 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 14:05:53,018 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 14:05:53,018 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 14:05:53,023 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:05:51 2020-04-22 14:05:55,055 INFO hive.HiveImport: Hive Session ID = 6d766913-280d-42f0-b248-aeb3df2a5cf1

2020-04-22 14:05:51 2020-04-22 14:05:55,104 INFO hive.HiveImport: 
2020-04-22 14:05:55,104 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:06:46 开始处理对日统计数据
2020-04-22 14:06:46 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \
--hive-table user_info \
--hive-overwrite
2020-04-22 14:06:46 Last login: Wed Apr 22 14:05:35 2020 from 10.10.30.233

2020-04-22 14:06:46 su hadoop

2020-04-22 14:06:46 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:06:46 [hadoop@hadoop-01 root]$ 
2020-04-22 14:06:46 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:06:47 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> --hive-table user_info \
> --hive-overwrite

2020-04-22 14:06:47 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:06:47 2020-04-22 14:06:51,283 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 14:06:51,319 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:06:47 2020-04-22 14:06:51,369 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-22 14:06:51,377 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 14:06:51,379 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:06:47 2020-04-22 14:06:51,618 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:06:51,623 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:06:51,644 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:06:48 注: /tmp/sqoop-hadoop/compile/442779ef9680efa0d7f0c61a9e224699/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:06:52,574 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/442779ef9680efa0d7f0c61a9e224699/user_info.jar

2020-04-22 14:06:49 2020-04-22 14:06:53,293 INFO tool.ImportTool: Destination directory user_info deleted.

2020-04-22 14:06:49 2020-04-22 14:06:53,301 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 14:06:53,302 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 14:06:53,309 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 14:06:53,312 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:06:49 2020-04-22 14:06:53,324 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:06:50 2020-04-22 14:06:53,808 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0383

2020-04-22 14:06:50 2020-04-22 14:06:54,673 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 14:06:50 2020-04-22 14:06:54,710 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 14:06:51 2020-04-22 14:06:54,882 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0383
2020-04-22 14:06:54,883 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 14:06:51 2020-04-22 14:06:55,065 INFO conf.Configuration: resource-types.xml not found
2020-04-22 14:06:55,065 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 14:06:51 2020-04-22 14:06:55,112 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0383

2020-04-22 14:06:51 2020-04-22 14:06:55,159 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0383/
2020-04-22 14:06:55,160 INFO mapreduce.Job: Running job: job_1587346943308_0383

2020-04-22 14:06:57 2020-04-22 14:07:01,247 INFO mapreduce.Job: Job job_1587346943308_0383 running in uber mode : false
2020-04-22 14:07:01,249 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 14:07:01 2020-04-22 14:07:05,314 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 14:07:05,325 INFO mapreduce.Job: Job job_1587346943308_0383 completed successfully

2020-04-22 14:07:01 2020-04-22 14:07:05,438 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2385
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2385
		Total vcore-milliseconds taken by all map tasks=2385
		Total megabyte-milliseconds taken by all map tasks=2442240
	Map-Reduce Framework
		Map input records=0
		Map output records=0
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=52
		CPU time spent (ms)=840
	
2020-04-22 14:07:01 	Physical memory (bytes) snapshot=241750016
		Virtual memory (bytes) snapshot=2446589952
		Total committed heap usage (bytes)=190840832
		Peak Map Physical memory (bytes)=241750016
		Peak Map Virtual memory (bytes)=2446589952
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 14:07:05,448 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 12.1114 seconds (0 bytes/sec)
2020-04-22 14:07:05,452 INFO mapreduce.ImportJobBase: Retrieved 0 records.
2020-04-22 14:07:05,452 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info

2020-04-22 14:07:01 2020-04-22 14:07:05,465 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:07:01 2020-04-22 14:07:05,467 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:07:01 2020-04-22 14:07:05,490 INFO hive.HiveImport: Loading uploaded data into Hive

2020-04-22 14:07:01 2020-04-22 14:07:05,497 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 14:07:01 2020-04-22 14:07:05,653 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 14:07:02 2020-04-22 14:07:06,228 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-22 14:07:06,249 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 14:07:03 2020-04-22 14:07:06,729 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 14:07:06,729 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 14:07:06,729 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 14:07:06,729 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 14:07:06,734 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:07:05 2020-04-22 14:07:08,742 INFO hive.HiveImport: Hive Session ID = 71b05f8a-5291-4703-80bd-89c879febc6e

2020-04-22 14:07:05 2020-04-22 14:07:08,791 INFO hive.HiveImport: 
2020-04-22 14:07:08,791 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:07:12 2020-04-22 14:07:16,064 INFO hive.HiveImport: Hive Session ID = 298f550f-025e-4330-8c53-691dde3ea9b7

2020-04-22 14:07:13 2020-04-22 14:07:17,487 INFO hive.HiveImport: OK
2020-04-22 14:07:17,498 INFO hive.HiveImport: Time taken: 1.336 seconds

2020-04-22 14:07:13 2020-04-22 14:07:17,685 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 14:07:14 2020-04-22 14:07:18,260 INFO hive.HiveImport: OK

2020-04-22 14:07:14 2020-04-22 14:07:18,270 INFO hive.HiveImport: Time taken: 0.749 seconds

2020-04-22 14:07:15 2020-04-22 14:07:18,768 INFO hive.HiveImport: Hive import complete.

2020-04-22 14:07:15 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:07:15 Last login: Wed Apr 22 14:06:50 2020 from 10.10.30.233

2020-04-22 14:07:15 su hadoop

2020-04-22 14:07:15 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:07:15 [hadoop@hadoop-01 root]$ 
2020-04-22 14:07:15 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:07:15 cat statistic/RegionbyDay.hive
cat: statistic/RegionbyDay.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:07:15 Last login: Wed Apr 22 14:07:19 2020 from 10.10.30.233

2020-04-22 14:07:15 su hadoop

2020-04-22 14:07:16 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:07:16 [hadoop@hadoop-01 root]$ 
2020-04-22 14:07:16 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:07:16 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 14:07:16 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 14:07:17 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:07:19 Hive Session ID = 59c7e6b4-43d3-4f51-852d-4ed7e8fd5f5d

Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:07:25 Hive Session ID = 601a717f-10db-448f-95de-5020ef9cf078

2020-04-22 14:07:26 OK
Time taken: 0.807 seconds

2020-04-22 14:07:27 Query ID = hadoop_20200422140730_cc89ef41-6025-4e56-8598-f964b811b269
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 14:07:28 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:07:30 Starting Job = job_1587346943308_0384, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0384/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0384

2020-04-22 14:07:35 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 14:07:35 2020-04-22 14:07:39,008 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:07:44 2020-04-22 14:07:48,427 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 6.41 sec

2020-04-22 14:07:46 2020-04-22 14:07:50,511 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.55 sec

2020-04-22 14:07:51 2020-04-22 14:07:55,692 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.01 sec

2020-04-22 14:07:53 MapReduce Total cumulative CPU time: 20 seconds 10 msec

2020-04-22 14:07:53 Ended Job = job_1587346943308_0384

2020-04-22 14:07:53 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:07:53 Starting Job = job_1587346943308_0385, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0385/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0385

2020-04-22 14:08:04 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 14:08:04 2020-04-22 14:08:08,586 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:08:11 2020-04-22 14:08:14,812 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.76 sec

2020-04-22 14:08:18 2020-04-22 14:08:22,040 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.18 sec

2020-04-22 14:08:19 MapReduce Total cumulative CPU time: 7 seconds 180 msec
Ended Job = job_1587346943308_0385

2020-04-22 14:08:19 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 14:08:19 


2020-04-22 14:08:19 	 Time taken to load dynamic partitions: 0.367 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:08:20 Starting Job = job_1587346943308_0386, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0386/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0386

2020-04-22 14:08:29 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 14:08:29 2020-04-22 14:08:33,322 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:08:35 2020-04-22 14:08:39,520 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.51 sec

2020-04-22 14:08:41 2020-04-22 14:08:45,709 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.14 sec

2020-04-22 14:08:43 MapReduce Total cumulative CPU time: 4 seconds 140 msec
Ended Job = job_1587346943308_0386

2020-04-22 14:08:43 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 20.01 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.18 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.14 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 330 msec
OK

2020-04-22 14:08:43 Time taken: 77.286 seconds

2020-04-22 14:08:44 Query ID = hadoop_20200422140847_d3bd8fa2-8508-42a0-b3bc-224e4f5cf6d3
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:08:45 Starting Job = job_1587346943308_0387, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0387/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0387

2020-04-22 14:08:54 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2020-04-22 14:08:57,888 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:09:02 2020-04-22 14:09:06,159 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 6.72 sec

2020-04-22 14:09:04 2020-04-22 14:09:08,227 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.48 sec

2020-04-22 14:09:08 2020-04-22 14:09:12,348 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 18.38 sec

2020-04-22 14:09:09 MapReduce Total cumulative CPU time: 18 seconds 380 msec
Ended Job = job_1587346943308_0387
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:09:10 Starting Job = job_1587346943308_0388, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0388/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0388

2020-04-22 14:09:19 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 14:09:19 2020-04-22 14:09:23,567 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:09:26 2020-04-22 14:09:29,765 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.51 sec

2020-04-22 14:09:32 2020-04-22 14:09:35,970 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.08 sec

2020-04-22 14:09:34 MapReduce Total cumulative CPU time: 7 seconds 80 msec
Ended Job = job_1587346943308_0388

2020-04-22 14:09:34 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 14:09:34 


2020-04-22 14:09:34 	 Time taken to load dynamic partitions: 0.265 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:09:35 Starting Job = job_1587346943308_0389, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0389/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0389

2020-04-22 14:09:44 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 14:09:44 2020-04-22 14:09:48,091 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:09:50 2020-04-22 14:09:54,287 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.42 sec

2020-04-22 14:09:56 2020-04-22 14:10:00,473 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.06 sec

2020-04-22 14:09:57 MapReduce Total cumulative CPU time: 4 seconds 60 msec
Ended Job = job_1587346943308_0389

2020-04-22 14:09:58 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 18.38 sec   HDFS Read: 84990506 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.08 sec   HDFS Read: 893596 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.06 sec   HDFS Read: 19219 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 29 seconds 520 msec
OK
Time taken: 74.317 seconds

2020-04-22 14:09:59 Query ID = hadoop_20200422141001_e090ece5-0fe9-4b0b-a50f-3ae2d9c552cb
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:10:00 Starting Job = job_1587346943308_0390, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0390/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0390

2020-04-22 14:10:09 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2020-04-22 14:10:13,292 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:10:16 2020-04-22 14:10:20,519 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.67 sec

2020-04-22 14:10:22 2020-04-22 14:10:26,706 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.33 sec

2020-04-22 14:10:24 MapReduce Total cumulative CPU time: 10 seconds 330 msec
Ended Job = job_1587346943308_0390

2020-04-22 14:10:24 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:10:24 Starting Job = job_1587346943308_0391, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0391/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0391

2020-04-22 14:10:34 Hadoop job information for Stage-4: number of mappers: 2; number of reducers: 1

2020-04-22 14:10:34 2020-04-22 14:10:37,903 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:10:42 2020-04-22 14:10:46,141 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 7.2 sec

2020-04-22 14:10:47 2020-04-22 14:10:51,288 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 10.63 sec

2020-04-22 14:10:49 MapReduce Total cumulative CPU time: 10 seconds 630 msec
Ended Job = job_1587346943308_0391

2020-04-22 14:10:49 Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:10:50 Starting Job = job_1587346943308_0392, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0392/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0392

2020-04-22 14:11:00 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2020-04-22 14:11:04,114 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:11:06 2020-04-22 14:11:10,316 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.41 sec

2020-04-22 14:11:12 2020-04-22 14:11:16,514 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.79 sec

2020-04-22 14:11:13 MapReduce Total cumulative CPU time: 6 seconds 790 msec
Ended Job = job_1587346943308_0392

2020-04-22 14:11:13 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-22 14:11:13 


2020-04-22 14:11:14 	 Time taken to load dynamic partitions: 0.208 seconds
	 Time taken for adding to write entity : 0.0 seconds

2020-04-22 14:11:14 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 10.33 sec   HDFS Read: 883908 HDFS Write: 182 SUCCESS
Stage-Stage-4: Map: 2  Reduce: 1   Cumulative CPU: 10.63 sec   HDFS Read: 489429 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.79 sec   HDFS Read: 21763 HDFS Write: 822 SUCCESS
Total MapReduce CPU Time Spent: 27 seconds 750 msec
OK
Time taken: 76.172 seconds

2020-04-22 14:11:14 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:11:15 Last login: Wed Apr 22 14:07:19 2020 from 10.10.30.233

2020-04-22 14:11:15 su hadoop

2020-04-22 14:11:15 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:11:15 [hadoop@hadoop-01 root]$ 
2020-04-22 14:11:15 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:11:15 cat statistic/add_user_day.hive
cat: statistic/add_user_day.hive: 没有那个文件或目录
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:11:15 Last login: Wed Apr 22 14:11:18 2020 from 10.10.30.233

2020-04-22 14:11:15 su hadoop

2020-04-22 14:11:15 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:11:15 [hadoop@hadoop-01 root]$ 
2020-04-22 14:11:15 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:11:15 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 14:11:16 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 14:11:16 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:11:18 Hive Session ID = 085cd43f-c616-4af0-93df-856f8e26bf46

2020-04-22 14:11:18 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:11:25 Hive Session ID = f2f6c9e9-7232-4844-a493-c4975166adf5

2020-04-22 14:11:26 OK
Time taken: 0.847 seconds

2020-04-22 14:11:27 FAILED: SemanticException [Error 10025]: Line 4:44 Expression not in GROUP BY key 'area_code'

2020-04-22 14:11:28 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:11:28 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 14:11:28 Last login: Wed Apr 22 14:11:19 2020 from 10.10.30.233

2020-04-22 14:11:28 su hadoop

2020-04-22 14:11:28 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:11:28 [hadoop@hadoop-01 root]$ 
2020-04-22 14:11:28 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:11:28 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 14:11:28 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:11:29 2020-04-22 14:11:33,042 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 14:11:29 2020-04-22 14:11:33,067 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:11:29 2020-04-22 14:11:33,143 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 14:11:33,146 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:11:29 2020-04-22 14:11:33,346 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 14:11:33,362 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 14:11:29 2020-04-22 14:11:33,368 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:11:30 注: /tmp/sqoop-hadoop/compile/139f466df324bcbdbfa2875e071c80bf/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:11:34,404 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/139f466df324bcbdbfa2875e071c80bf/user_info.jar
2020-04-22 14:11:34,411 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 14:11:34,411 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 14:11:34,411 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 14:11:34,411 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 14:11:34,411 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 14:11:34,417 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 14:11:34,418 INFO Configuration.deprecation: mapred.j
2020-04-22 14:11:30 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 14:11:30 2020-04-22 14:11:34,521 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 14:11:31 2020-04-22 14:11:35,132 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=23 does not exist

2020-04-22 14:11:31 2020-04-22 14:11:35,152 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

2020-04-22 14:11:31 2020-04-22 14:11:35,154 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 14:11:35,155 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:11:31 2020-04-22 14:11:35,593 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0393

2020-04-22 14:11:33 2020-04-22 14:11:37,346 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0393

2020-04-22 14:11:33 2020-04-22 14:11:37,351 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=23
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessCont
2020-04-22 14:11:33 roller.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:24
2020-04-22 14:11:33 3)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 14:11:34 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:11:34 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 14:11:34 Last login: Wed Apr 22 14:11:32 2020 from 10.10.30.233

2020-04-22 14:11:34 su hadoop

2020-04-22 14:11:34 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:11:34 [hadoop@hadoop-01 root]$ 
2020-04-22 14:11:34 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:11:34 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 14:11:34 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:11:34 2020-04-22 14:11:38,632 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 14:11:38,657 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:11:35 2020-04-22 14:11:38,739 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.

2020-04-22 14:11:35 2020-04-22 14:11:38,743 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:11:35 2020-04-22 14:11:38,948 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 14:11:35 2020-04-22 14:11:38,964 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 14:11:35 2020-04-22 14:11:38,969 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:11:36 注: /tmp/sqoop-hadoop/compile/8b27e4da89a43e8afdd8e63b44e4e39e/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:11:39,981 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/8b27e4da89a43e8afdd8e63b44e4e39e/user_info.jar
2020-04-22 14:11:39,989 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 14:11:39,989 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 14:11:39,989 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 14:11:39,989 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 14:11:39,989 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 14:11:39,995 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 14:11:39,995 INFO Configuration.deprecation: mapred.j
2020-04-22 14:11:36 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 14:11:36 2020-04-22 14:11:40,099 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 14:11:36 2020-04-22 14:11:40,715 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=24 does not exist
2020-04-22 14:11:40,734 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

2020-04-22 14:11:37 2020-04-22 14:11:40,736 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 14:11:40,737 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:11:37 2020-04-22 14:11:41,231 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0394

2020-04-22 14:11:39 2020-04-22 14:11:42,961 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0394
2020-04-22 14:11:42,967 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=24
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
2020-04-22 14:11:39 
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at or
2020-04-22 14:11:39 g.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 14:11:39 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:11:39 日统计数据已完成
2020-04-22 14:19:24 开始处理对日统计数据
2020-04-22 14:19:24 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \
--hive-table user_info \
--hive-overwrite
2020-04-22 14:19:24 Last login: Wed Apr 22 14:17:01 2020 from 192.168.15.22

2020-04-22 14:19:24 su hadoop

2020-04-22 14:19:24 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:19:24 [hadoop@hadoop-01 root]$ 
2020-04-22 14:19:24 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:19:24 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> --hive-table user_info \
> --hive-overwrite

2020-04-22 14:19:24 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:19:25 2020-04-22 14:19:28,887 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 14:19:28,913 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:19:25 2020-04-22 14:19:28,960 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-22 14:19:25 2020-04-22 14:19:28,969 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 14:19:28,971 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:19:25 2020-04-22 14:19:29,194 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:19:29,198 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:19:25 2020-04-22 14:19:29,214 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:19:26 注: /tmp/sqoop-hadoop/compile/a896ed9b1bd4d4df127f29881dad42c2/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:19:30,164 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/a896ed9b1bd4d4df127f29881dad42c2/user_info.jar

2020-04-22 14:19:27 2020-04-22 14:19:30,843 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-22 14:19:30,848 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 14:19:30,849 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 14:19:30,853 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 14:19:30,854 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:19:30,863 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:19:27 2020-04-22 14:19:31,316 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0395

2020-04-22 14:19:28 2020-04-22 14:19:32,592 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 14:19:29 2020-04-22 14:19:33,460 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 14:19:29 2020-04-22 14:19:33,682 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0395
2020-04-22 14:19:33,684 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 14:19:30 2020-04-22 14:19:33,883 INFO conf.Configuration: resource-types.xml not found
2020-04-22 14:19:33,883 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-04-22 14:19:33,926 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0395

2020-04-22 14:19:30 2020-04-22 14:19:33,951 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0395/
2020-04-22 14:19:33,952 INFO mapreduce.Job: Running job: job_1587346943308_0395

2020-04-22 14:19:36 2020-04-22 14:19:40,038 INFO mapreduce.Job: Job job_1587346943308_0395 running in uber mode : false
2020-04-22 14:19:40,040 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 14:19:40 2020-04-22 14:19:44,097 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 14:19:44,109 INFO mapreduce.Job: Job job_1587346943308_0395 completed successfully

2020-04-22 14:19:40 2020-04-22 14:19:44,202 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2400
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2400
		Total vcore-milliseconds taken by all map tasks=2400
		Total megabyte-milliseconds taken by all map tasks=2457600
	Map-Reduce Framework
		Map input records=0
		Map output records=0
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=54
		CPU time spent (ms)=790
	
2020-04-22 14:19:40 	Physical memory (bytes) snapshot=245620736
		Virtual memory (bytes) snapshot=2447183872
		Total committed heap usage (bytes)=194510848
		Peak Map Physical memory (bytes)=245620736
		Peak Map Virtual memory (bytes)=2447183872
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 14:19:44,210 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 13.3387 seconds (0 bytes/sec)
2020-04-22 14:19:44,214 INFO mapreduce.ImportJobBase: Retrieved 0 records.
2020-04-22 14:19:44,214 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info
2020-04-22 14:19:44,227 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:19:44,229 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:19:40 2020-04-22 14:19:44,250 INFO hive.HiveImport: Loading uploaded data into Hive

2020-04-22 14:19:40 2020-04-22 14:19:44,258 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 14:19:40 2020-04-22 14:19:44,417 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 14:19:41 2020-04-22 14:19:45,075 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-22 14:19:45,097 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 14:19:41 2020-04-22 14:19:45,528 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 14:19:45,528 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 14:19:45,528 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 14:19:45,528 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 14:19:45,533 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:19:43 2020-04-22 14:19:47,570 INFO hive.HiveImport: Hive Session ID = f6027254-3bd8-4a8e-8f4f-d0bb8a2c2d66

2020-04-22 14:19:43 2020-04-22 14:19:47,621 INFO hive.HiveImport: 
2020-04-22 14:19:47,621 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:19:51 2020-04-22 14:19:54,768 INFO hive.HiveImport: Hive Session ID = b78462d7-35f5-4589-bd2c-cfdef5e60e3d

2020-04-22 14:19:52 2020-04-22 14:19:56,241 INFO hive.HiveImport: OK
2020-04-22 14:19:56,252 INFO hive.HiveImport: Time taken: 1.394 seconds

2020-04-22 14:19:52 2020-04-22 14:19:56,431 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 14:19:53 2020-04-22 14:19:56,972 INFO hive.HiveImport: OK
2020-04-22 14:19:56,982 INFO hive.HiveImport: Time taken: 0.71 seconds

2020-04-22 14:19:53 2020-04-22 14:19:57,482 INFO hive.HiveImport: Hive import complete.

2020-04-22 14:19:54 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:19:54 Last login: Wed Apr 22 14:19:28 2020 from 10.10.30.233

2020-04-22 14:19:54 su hadoop

2020-04-22 14:19:54 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:19:54 [hadoop@hadoop-01 root]$ 
2020-04-22 14:19:54 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:19:54 cat hive/statistic/RegionbyDay.hive

2020-04-22 14:19:54 
-- 按天统计专区信息
-- 进入数据库
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
-- 1、按天去重 日访问用户信息
insert overwrite table visit_user_page_day partition(y,m,d)  select param["parentColumnId"] as parent_column_id,user_type,user_id ,'' area_code,min(create_time) as create_time,y,m,d from events_type_log
where events_type in ('operationDetails','operationPage') and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param["parentColumnId"],user_type,user_id,y,m,d cluster by user_id,user_type;




--3、 按天去除重复播放用户
insert overwrite table play_user_day partition(y,m,d)  select param['parentColumnId'] as parent_column_id,user_type,user_id ,'' area_code,sum(1) as play_count,sum(cast(nvl(param['timePosition'],'0') as bigint)) as duration,y,m,d from even
2020-04-22 14:19:54 ts_type_log
where events_type='operateResumePoint' and param['operateType']='add' and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param['parentColumnId'],user_type,user_id,y,m,d cluster by user_id,user_type;


-- 统计日页面访问用户数，统计日播放用户数，统计日播放次数
-- 统计页面访问用户数
insert  overwrite table  region_count_day partition(t_date)
select a.parent_column_id,a.user_type,sum(a.page_user_count) as page_user_count,sum(play_user_count) as play_user_count,sum(a.duration) as duration,sum(a.play_count) as play_count,a.t as t_date from (
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type, count(1) as page_user_count,0 as play_user_count,0 as duration,0 as play_count from visit_user_page_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
UNION ALL
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type,0 as page_u
2020-04-22 14:19:54 ser_count,count(1) as play_user_count, sum(duration) as duration,sum(play_count) as play_count from play_user_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
) a group by a.t,a.parent_column_id,a.user_type;

-- 统计 播放次数
-- select sum(cast(nvl(data['timePosition'],'0') as bigint)) as duration,count(1) as play_count from events_type_log where y=2020 and m='04' and d='01' and data['operateType']='add';


[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:19:54 Last login: Wed Apr 22 14:19:58 2020 from 10.10.30.233

2020-04-22 14:19:54 su hadoop

2020-04-22 14:19:54 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:19:54 [hadoop@hadoop-01 root]$ 
2020-04-22 14:19:54 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:19:54 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 14:19:55 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 14:19:55 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:19:57 Hive Session ID = 988e3917-5778-419f-809f-a905d12f67b3

2020-04-22 14:19:57 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:20:04 Hive Session ID = da0f26d9-4835-4fb5-9a03-78983ff70832

2020-04-22 14:20:05 OK
Time taken: 0.839 seconds

2020-04-22 14:20:06 Query ID = hadoop_20200422142008_7b71ddda-8a1e-4eff-baca-438d92aee036
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 14:20:06 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:20:09 Starting Job = job_1587346943308_0396, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0396/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0396

2020-04-22 14:20:15 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 14:20:16 2020-04-22 14:20:19,745 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:20:24 2020-04-22 14:20:28,123 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 6.69 sec

2020-04-22 14:20:26 2020-04-22 14:20:30,209 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.25 sec

2020-04-22 14:20:32 2020-04-22 14:20:36,419 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 19.63 sec

2020-04-22 14:20:33 MapReduce Total cumulative CPU time: 19 seconds 630 msec

2020-04-22 14:20:33 Ended Job = job_1587346943308_0396

2020-04-22 14:20:33 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:20:34 Starting Job = job_1587346943308_0397, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0397/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0397

2020-04-22 14:20:44 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 14:20:44 2020-04-22 14:20:48,386 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:20:50 2020-04-22 14:20:54,595 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.68 sec

2020-04-22 14:20:58 2020-04-22 14:21:01,841 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.73 sec

2020-04-22 14:20:59 MapReduce Total cumulative CPU time: 7 seconds 730 msec
Ended Job = job_1587346943308_0397

2020-04-22 14:20:59 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 14:20:59 


2020-04-22 14:20:59 	 Time taken to load dynamic partitions: 0.337 seconds
	 Time taken for adding to write entity : 0.002 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:21:00 Starting Job = job_1587346943308_0398, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0398/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0398

2020-04-22 14:21:10 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 14:21:10 2020-04-22 14:21:14,056 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:21:16 2020-04-22 14:21:20,251 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.51 sec

2020-04-22 14:21:22 2020-04-22 14:21:26,442 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.0 sec

2020-04-22 14:21:23 MapReduce Total cumulative CPU time: 4 seconds 0 msec
Ended Job = job_1587346943308_0398

2020-04-22 14:21:24 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 19.63 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.73 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.0 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 360 msec
OK

2020-04-22 14:21:24 Time taken: 79.295 seconds

2020-04-22 14:21:24 Query ID = hadoop_20200422142128_50ffcba3-d1d9-4700-9f87-061d652a8c6d
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:21:25 Starting Job = job_1587346943308_0399, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0399/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0399

2020-04-22 14:21:34 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 14:21:34 2020-04-22 14:21:38,299 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:21:43 2020-04-22 14:21:47,611 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 6.03 sec

2020-04-22 14:21:45 2020-04-22 14:21:49,677 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 13.69 sec

2020-04-22 14:21:50 2020-04-22 14:21:53,798 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.68 sec

2020-04-22 14:21:52 MapReduce Total cumulative CPU time: 17 seconds 680 msec
Ended Job = job_1587346943308_0399
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:21:52 Starting Job = job_1587346943308_0400, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0400/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0400

2020-04-22 14:22:02 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 14:22:02 2020-04-22 14:22:06,606 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:22:09 2020-04-22 14:22:12,934 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.65 sec

2020-04-22 14:22:16 2020-04-22 14:22:20,143 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.48 sec

2020-04-22 14:22:17 MapReduce Total cumulative CPU time: 7 seconds 480 msec

2020-04-22 14:22:17 Ended Job = job_1587346943308_0400

2020-04-22 14:22:17 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 14:22:17 


2020-04-22 14:22:17 	 Time taken to load dynamic partitions: 0.255 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:22:18 Starting Job = job_1587346943308_0401, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0401/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0401

2020-04-22 14:22:28 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 14:22:28 2020-04-22 14:22:32,170 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:22:34 2020-04-22 14:22:38,396 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.51 sec

2020-04-22 14:22:41 2020-04-22 14:22:45,616 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.23 sec

2020-04-22 14:22:42 MapReduce Total cumulative CPU time: 4 seconds 230 msec
Ended Job = job_1587346943308_0401

2020-04-22 14:22:43 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 17.68 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.48 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.23 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 29 seconds 390 msec
OK

2020-04-22 14:22:43 Time taken: 78.669 seconds

2020-04-22 14:22:45 Query ID = hadoop_20200422142246_b9b445a6-81f7-4afe-8855-0fb70c0da0d1
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:22:45 Starting Job = job_1587346943308_0402, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0402/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0402

2020-04-22 14:22:54 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-04-22 14:22:57,889 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:23:00 2020-04-22 14:23:04,095 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.75 sec

2020-04-22 14:23:07 2020-04-22 14:23:11,309 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.45 sec

2020-04-22 14:23:08 MapReduce Total cumulative CPU time: 7 seconds 450 msec
Ended Job = job_1587346943308_0402

2020-04-22 14:23:08 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:23:09 Starting Job = job_1587346943308_0403, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0403/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0403

2020-04-22 14:23:19 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 14:23:19 2020-04-22 14:23:23,016 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:23:26 2020-04-22 14:23:30,250 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 3.74 sec

2020-04-22 14:23:32 2020-04-22 14:23:36,431 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 7.0 sec

2020-04-22 14:23:33 MapReduce Total cumulative CPU time: 7 seconds 0 msec
Ended Job = job_1587346943308_0403

2020-04-22 14:23:33 Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:23:34 Starting Job = job_1587346943308_0404, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0404/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0404

2020-04-22 14:23:45 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1

2020-04-22 14:23:45 2020-04-22 14:23:49,215 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:23:51 2020-04-22 14:23:55,495 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.96 sec

2020-04-22 14:23:57 2020-04-22 14:24:01,667 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.43 sec

2020-04-22 14:23:58 MapReduce Total cumulative CPU time: 6 seconds 430 msec
Ended Job = job_1587346943308_0404

2020-04-22 14:23:59 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-22 14:23:59 


2020-04-22 14:23:59 	 Time taken to load dynamic partitions: 0.254 seconds
	 Time taken for adding to write entity : 0.001 seconds

2020-04-22 14:23:59 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.45 sec   HDFS Read: 877473 HDFS Write: 182 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 7.0 sec   HDFS Read: 482570 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.43 sec   HDFS Read: 21763 HDFS Write: 822 SUCCESS
Total MapReduce CPU Time Spent: 20 seconds 880 msec
OK
Time taken: 76.274 seconds

2020-04-22 14:23:59 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:24:00 Last login: Wed Apr 22 14:19:58 2020 from 10.10.30.233

2020-04-22 14:24:00 su hadoop

2020-04-22 14:24:00 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:24:00 [hadoop@hadoop-01 root]$ 
2020-04-22 14:24:00 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:24:00 cat hive/statistic/add_user_day.hive
-- 2、天新增用户数据
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
insert overwrite table add_page_user_day partition(y,m,d)
select vud.parent_column_id,vud.user_type,vud.user_id,max(vud.area_code) area_code,vud.create_time,substr(vud.create_time,1,4) y,substr(vud.create_time,6,2) m,substr(vud.create_time,9,2) d   from(
  select parent_column_id,user_type,user_id,area_code,min(create_time) as create_time from visit_user_page_day
  where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by parent_column_id,user_type,user_id
) vud
 left join user_info aud on aud.user_id=vud.user_id and aud.user_type=vud.user_type and  aud.parent_column_id=vud.parent_column_id
 where aud.user_id is null;
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:24:00 Last login: Wed Apr 22 14:24:03 2020 from 10.10.30.233

2020-04-22 14:24:00 su hadoop

2020-04-22 14:24:00 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:24:00 [hadoop@hadoop-01 root]$ 
2020-04-22 14:24:00 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:24:00 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 14:24:01 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 14:24:01 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:24:03 Hive Session ID = 0501e6f6-fe17-466d-825d-42b03f1dc5ab

2020-04-22 14:24:03 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:24:10 Hive Session ID = 57bb0af8-c1b7-4767-857d-f9d983fe139c

2020-04-22 14:24:11 OK
Time taken: 0.851 seconds

2020-04-22 14:24:13 FAILED: SemanticException [Error 10025]: Line 4:44 Expression not in GROUP BY key 'area_code'

2020-04-22 14:24:13 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:24:13 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 14:24:13 Last login: Wed Apr 22 14:24:04 2020 from 10.10.30.233

2020-04-22 14:24:13 su hadoop

2020-04-22 14:24:13 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:24:13 [hadoop@hadoop-01 root]$ 
2020-04-22 14:24:13 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:24:13 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 14:24:13 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:24:14 2020-04-22 14:24:18,194 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 14:24:18,220 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:24:14 2020-04-22 14:24:18,295 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 14:24:18,298 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:24:14 2020-04-22 14:24:18,501 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 14:24:14 2020-04-22 14:24:18,527 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 14:24:14 2020-04-22 14:24:18,534 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:24:16 注: /tmp/sqoop-hadoop/compile/93a1a91c54ce43b7f1d7da3d86821885/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:24:19,808 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/93a1a91c54ce43b7f1d7da3d86821885/user_info.jar

2020-04-22 14:24:16 2020-04-22 14:24:19,819 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 14:24:19,819 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 14:24:19,819 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 14:24:19,819 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 14:24:19,819 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-22 14:24:16 2020-04-22 14:24:19,828 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 14:24:19,828 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 14:24:16 2020-04-22 14:24:19,973 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 14:24:17 2020-04-22 14:24:20,753 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=23 does not exist
2020-04-22 14:24:20,780 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

2020-04-22 14:24:17 2020-04-22 14:24:20,782 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 14:24:20,783 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:24:17 2020-04-22 14:24:21,362 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0405

2020-04-22 14:24:18 2020-04-22 14:24:22,275 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0405

2020-04-22 14:24:18 2020-04-22 14:24:22,383 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=23
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessCont
2020-04-22 14:24:18 roller.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:24
2020-04-22 14:24:18 3)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 14:24:19 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:24:19 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 14:24:19 Last login: Wed Apr 22 14:24:17 2020 from 10.10.30.233

2020-04-22 14:24:19 su hadoop

2020-04-22 14:24:19 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:24:19 [hadoop@hadoop-01 root]$ 
2020-04-22 14:24:19 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:24:19 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 14:24:19 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:24:19 2020-04-22 14:24:23,620 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 14:24:19 2020-04-22 14:24:23,645 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:24:19 2020-04-22 14:24:23,722 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.

2020-04-22 14:24:19 2020-04-22 14:24:23,725 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:24:20 2020-04-22 14:24:23,942 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 14:24:20 2020-04-22 14:24:23,958 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 14:24:20 2020-04-22 14:24:23,963 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:24:21 注: /tmp/sqoop-hadoop/compile/5ea15d8212cb26b7ba6e240e039e1f79/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:24:24,955 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/5ea15d8212cb26b7ba6e240e039e1f79/user_info.jar
2020-04-22 14:24:24,962 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 14:24:24,962 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 14:24:24,962 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 14:24:24,962 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 14:24:24,962 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 14:24:24,967 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 14:24:24,967 INFO Configuration.deprecation: mapred.j
2020-04-22 14:24:21 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 14:24:21 2020-04-22 14:24:25,073 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 14:24:21 2020-04-22 14:24:25,683 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=24 does not exist
2020-04-22 14:24:25,703 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 14:24:25,705 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 14:24:25,706 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:24:22 2020-04-22 14:24:26,140 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0406

2020-04-22 14:24:23 2020-04-22 14:24:27,498 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0406

2020-04-22 14:24:23 2020-04-22 14:24:27,574 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=24
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessCont
2020-04-22 14:24:23 roller.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:24
2020-04-22 14:24:23 3)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 14:24:24 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:24:24 日统计数据已完成
2020-04-22 14:27:11 开始处理对日统计数据
2020-04-22 14:27:11 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \
--hive-table user_info \
--hive-overwrite
2020-04-22 14:27:11 Last login: Wed Apr 22 14:24:23 2020 from 10.10.30.233

2020-04-22 14:27:11 su hadoop

2020-04-22 14:27:11 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:27:12 [hadoop@hadoop-01 root]$ 
2020-04-22 14:27:12 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:27:12 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> --hive-table user_info \
> --hive-overwrite

2020-04-22 14:27:12 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:27:12 2020-04-22 14:27:16,288 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 14:27:12 2020-04-22 14:27:16,313 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:27:12 2020-04-22 14:27:16,366 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-22 14:27:16,375 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 14:27:16,377 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:27:12 2020-04-22 14:27:16,596 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:27:16,600 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:27:12 2020-04-22 14:27:16,615 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:27:13 注: /tmp/sqoop-hadoop/compile/fc9335723d294a7f8b51c097dfb30432/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:27:17,544 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/fc9335723d294a7f8b51c097dfb30432/user_info.jar

2020-04-22 14:27:27 开始处理对日统计数据
2020-04-22 14:27:27 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \
--hive-table user_info \
--hive-overwrite
2020-04-22 14:27:27 Last login: Wed Apr 22 14:27:15 2020 from 10.10.30.233

2020-04-22 14:27:27 su hadoop

2020-04-22 14:27:27 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:27:27 [hadoop@hadoop-01 root]$ 
2020-04-22 14:27:27 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:27:27 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> --hive-table user_info \
> --hive-overwrite

2020-04-22 14:27:27 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:27:28 2020-04-22 14:27:31,932 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 14:27:28 2020-04-22 14:27:31,968 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:27:28 2020-04-22 14:27:32,031 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.

2020-04-22 14:27:28 2020-04-22 14:27:32,041 INFO manager.SqlManager: Using default fetchSize of 1000

2020-04-22 14:27:28 2020-04-22 14:27:32,044 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:27:28 2020-04-22 14:27:32,278 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:27:32,284 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:27:32,303 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:27:29 注: /tmp/sqoop-hadoop/compile/2c0bda8b01aaf6f51a2291394100ae0d/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:27:33,246 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/2c0bda8b01aaf6f51a2291394100ae0d/user_info.jar

2020-04-22 14:27:30 2020-04-22 14:27:33,946 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-22 14:27:33,951 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 14:27:33,951 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 14:27:33,956 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 14:27:33,957 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:27:33,966 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:27:30 2020-04-22 14:27:34,423 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0407

2020-04-22 14:27:31 2020-04-22 14:27:35,717 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 14:27:32 2020-04-22 14:27:35,764 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 14:27:32 2020-04-22 14:27:35,953 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0407
2020-04-22 14:27:35,954 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 14:27:32 2020-04-22 14:27:36,128 INFO conf.Configuration: resource-types.xml not found
2020-04-22 14:27:36,128 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 14:27:32 2020-04-22 14:27:36,377 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0407

2020-04-22 14:27:32 2020-04-22 14:27:36,424 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0407/
2020-04-22 14:27:36,424 INFO mapreduce.Job: Running job: job_1587346943308_0407

2020-04-22 14:27:38 2020-04-22 14:27:42,510 INFO mapreduce.Job: Job job_1587346943308_0407 running in uber mode : false
2020-04-22 14:27:42,512 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 14:27:42 2020-04-22 14:27:46,574 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-22 14:27:42 2020-04-22 14:27:46,586 INFO mapreduce.Job: Job job_1587346943308_0407 completed successfully

2020-04-22 14:27:42 2020-04-22 14:27:46,699 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2313
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2313
		Total vcore-milliseconds taken by all map tasks=2313
		Total megabyte-milliseconds taken by all map tasks=2368512
	Map-Reduce Framework
		Map input records=0
		Map output records=0
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=49
		CPU time spent (ms)=810
	
2020-04-22 14:27:42 	Physical memory (bytes) snapshot=241508352
		Virtual memory (bytes) snapshot=2445737984
		Total committed heap usage (bytes)=190316544
		Peak Map Physical memory (bytes)=241508352
		Peak Map Virtual memory (bytes)=2445737984
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 14:27:46,708 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 12.7323 seconds (0 bytes/sec)
2020-04-22 14:27:46,713 INFO mapreduce.ImportJobBase: Retrieved 0 records.
2020-04-22 14:27:46,713 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info

2020-04-22 14:27:42 2020-04-22 14:27:46,728 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:27:42 2020-04-22 14:27:46,730 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:27:43 2020-04-22 14:27:46,755 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 14:27:46,762 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 14:27:43 2020-04-22 14:27:46,935 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 14:27:43 2020-04-22 14:27:47,494 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 14:27:43 2020-04-22 14:27:47,516 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 14:27:44 2020-04-22 14:27:47,939 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 14:27:47,940 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 14:27:47,940 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 14:27:47,940 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 14:27:47,945 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:27:46 2020-04-22 14:27:49,989 INFO hive.HiveImport: Hive Session ID = dc386fae-56bf-48e4-b719-3d72481f6015

2020-04-22 14:27:46 2020-04-22 14:27:50,037 INFO hive.HiveImport: 
2020-04-22 14:27:50,037 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:27:53 2020-04-22 14:27:57,300 INFO hive.HiveImport: Hive Session ID = 86b3295b-7861-4e6c-8ff2-91de38911705

2020-04-22 14:27:55 2020-04-22 14:27:58,772 INFO hive.HiveImport: OK
2020-04-22 14:27:58,782 INFO hive.HiveImport: Time taken: 1.395 seconds

2020-04-22 14:27:55 2020-04-22 14:27:58,939 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 14:27:55 2020-04-22 14:27:59,522 INFO hive.HiveImport: OK
2020-04-22 14:27:59,532 INFO hive.HiveImport: Time taken: 0.729 seconds

2020-04-22 14:27:56 2020-04-22 14:28:00,035 INFO hive.HiveImport: Hive import complete.

2020-04-22 14:27:56 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:27:56 Last login: Wed Apr 22 14:27:31 2020 from 10.10.30.233

2020-04-22 14:27:56 su hadoop

2020-04-22 14:27:56 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:27:56 [hadoop@hadoop-01 root]$ 
2020-04-22 14:27:56 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:27:56 cat hive/statistic/RegionbyDay.hive

2020-04-22 14:27:56 
-- 按天统计专区信息
-- 进入数据库
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
-- 1、按天去重 日访问用户信息
insert overwrite table visit_user_page_day partition(y,m,d)  select param["parentColumnId"] as parent_column_id,user_type,user_id ,'' area_code,min(create_time) as create_time,y,m,d from events_type_log
where events_type in ('operationDetails','operationPage') and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param["parentColumnId"],user_type,user_id,y,m,d cluster by user_id,user_type;




--3、 按天去除重复播放用户
insert overwrite table play_user_day partition(y,m,d)  select param['parentColumnId'] as parent_column_id,user_type,user_id ,'' area_code,sum(1) as play_count,sum(cast(nvl(param['timePosition'],'0') as bigint)) as duration,y,m,d from even
2020-04-22 14:27:56 ts_type_log
where events_type='operateResumePoint' and param['operateType']='add' and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param['parentColumnId'],user_type,user_id,y,m,d cluster by user_id,user_type;


-- 统计日页面访问用户数，统计日播放用户数，统计日播放次数
-- 统计页面访问用户数
insert  overwrite table  region_count_day partition(t_date)
select a.parent_column_id,a.user_type,sum(a.page_user_count) as page_user_count,sum(play_user_count) as play_user_count,sum(a.duration) as duration,sum(a.play_count) as play_count,a.t as t_date from (
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type, count(1) as page_user_count,0 as play_user_count,0 as duration,0 as play_count from visit_user_page_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
UNION ALL
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type,0 as page_u
2020-04-22 14:27:56 ser_count,count(1) as play_user_count, sum(duration) as duration,sum(play_count) as play_count from play_user_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
) a group by a.t,a.parent_column_id,a.user_type;

-- 统计 播放次数
-- select sum(cast(nvl(data['timePosition'],'0') as bigint)) as duration,count(1) as play_count from events_type_log where y=2020 and m='04' and d='01' and data['operateType']='add';


[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:27:57 Last login: Wed Apr 22 14:28:00 2020 from 10.10.30.233

2020-04-22 14:27:57 su hadoop

2020-04-22 14:27:57 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:27:57 [hadoop@hadoop-01 root]$ 
2020-04-22 14:27:57 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:27:57 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 14:27:57 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 14:27:58 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:28:00 Hive Session ID = f61114a8-1c40-48c4-ab03-c5c5f73b6c32

2020-04-22 14:28:00 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:28:06 Hive Session ID = b2ba963f-f91a-4653-bdf9-e2db1db209da

2020-04-22 14:28:07 OK
Time taken: 0.792 seconds

2020-04-22 14:28:09 Query ID = hadoop_20200422142811_4b321513-3730-4805-8065-a7669139626d
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 14:28:09 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:28:11 Starting Job = job_1587346943308_0408, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0408/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0408

2020-04-22 14:28:17 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 14:28:17 2020-04-22 14:28:21,082 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:28:26 2020-04-22 14:28:30,481 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 6.87 sec

2020-04-22 14:28:28 2020-04-22 14:28:32,543 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.06 sec

2020-04-22 14:28:31 2020-04-22 14:28:35,658 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.51 sec

2020-04-22 14:28:32 MapReduce Total cumulative CPU time: 20 seconds 510 msec

2020-04-22 14:28:33 Ended Job = job_1587346943308_0408

2020-04-22 14:28:33 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:28:33 Starting Job = job_1587346943308_0409, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0409/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0409

2020-04-22 14:28:44 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2020-04-22 14:28:48,552 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:28:51 2020-04-22 14:28:54,765 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.76 sec

2020-04-22 14:28:58 2020-04-22 14:29:01,999 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.04 sec

2020-04-22 14:28:59 MapReduce Total cumulative CPU time: 7 seconds 40 msec
Ended Job = job_1587346943308_0409

2020-04-22 14:28:59 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 14:28:59 


2020-04-22 14:28:59 	 Time taken to load dynamic partitions: 0.35 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:29:00 Starting Job = job_1587346943308_0410, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0410/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0410

2020-04-22 14:29:09 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 14:29:09 2020-04-22 14:29:13,659 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:29:16 2020-04-22 14:29:19,871 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.45 sec

2020-04-22 14:29:21 2020-04-22 14:29:25,034 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.15 sec

2020-04-22 14:29:22 MapReduce Total cumulative CPU time: 4 seconds 150 msec

2020-04-22 14:29:22 Ended Job = job_1587346943308_0410

2020-04-22 14:29:23 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 20.51 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.04 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.15 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 700 msec
OK

2020-04-22 14:29:23 Time taken: 75.399 seconds

2020-04-22 14:29:23 Query ID = hadoop_20200422142926_93a7986d-5a01-4198-81c9-1df4860dd456
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:29:24 Starting Job = job_1587346943308_0411, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0411/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0411

2020-04-22 14:29:32 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2020-04-22 14:29:36,407 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:29:41 2020-04-22 14:29:44,722 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 5.75 sec

2020-04-22 14:29:43 2020-04-22 14:29:46,810 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 13.4 sec

2020-04-22 14:29:47 2020-04-22 14:29:50,959 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.17 sec

2020-04-22 14:29:48 MapReduce Total cumulative CPU time: 17 seconds 170 msec
Ended Job = job_1587346943308_0411

2020-04-22 14:29:48 Launching Job 2 out of 3

2020-04-22 14:29:48 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:29:49 Starting Job = job_1587346943308_0412, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0412/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0412

2020-04-22 14:29:59 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2020-04-22 14:30:03,170 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:30:05 2020-04-22 14:30:09,356 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.6 sec

2020-04-22 14:30:13 2020-04-22 14:30:17,596 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.68 sec

2020-04-22 14:30:14 MapReduce Total cumulative CPU time: 7 seconds 680 msec
Ended Job = job_1587346943308_0412

2020-04-22 14:30:14 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 14:30:15 


2020-04-22 14:30:15 	 Time taken to load dynamic partitions: 0.277 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:30:16 Starting Job = job_1587346943308_0413, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0413/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0413

2020-04-22 14:30:25 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 14:30:25 2020-04-22 14:30:29,127 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:30:31 2020-04-22 14:30:35,331 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec

2020-04-22 14:30:38 2020-04-22 14:30:42,549 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.13 sec

2020-04-22 14:30:39 MapReduce Total cumulative CPU time: 4 seconds 130 msec
Ended Job = job_1587346943308_0413

2020-04-22 14:30:40 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 17.17 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.68 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.13 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 28 seconds 980 msec
OK
Time taken: 76.955 seconds

2020-04-22 14:30:41 Query ID = hadoop_20200422143043_98ed717c-0bdf-40c5-ba46-bb5fd0b02716
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:30:42 Starting Job = job_1587346943308_0414, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0414/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0414

2020-04-22 14:30:50 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2020-04-22 14:30:54,409 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:30:58 2020-04-22 14:31:02,655 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.19 sec

2020-04-22 14:31:05 2020-04-22 14:31:08,852 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.89 sec

2020-04-22 14:31:06 MapReduce Total cumulative CPU time: 10 seconds 890 msec
Ended Job = job_1587346943308_0414
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:31:07 Starting Job = job_1587346943308_0415, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0415/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0415

2020-04-22 14:31:17 Hadoop job information for Stage-4: number of mappers: 2; number of reducers: 1

2020-04-22 14:31:17 2020-04-22 14:31:21,040 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:31:24 2020-04-22 14:31:28,351 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 7.19 sec

2020-04-22 14:31:30 2020-04-22 14:31:34,538 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 10.73 sec

2020-04-22 14:31:31 MapReduce Total cumulative CPU time: 10 seconds 730 msec
Ended Job = job_1587346943308_0415

2020-04-22 14:31:31 Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:31:32 Starting Job = job_1587346943308_0416, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0416/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0416

2020-04-22 14:31:42 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2020-04-22 14:31:46,291 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:31:48 2020-04-22 14:31:52,490 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 1.61 sec

2020-04-22 14:31:49 2020-04-22 14:31:53,525 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.34 sec

2020-04-22 14:31:54 2020-04-22 14:31:58,687 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.69 sec

2020-04-22 14:31:55 MapReduce Total cumulative CPU time: 6 seconds 690 msec
Ended Job = job_1587346943308_0416

2020-04-22 14:31:56 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-22 14:31:56 


2020-04-22 14:31:56 	 Time taken to load dynamic partitions: 0.256 seconds
	 Time taken for adding to write entity : 0.0 seconds

2020-04-22 14:31:56 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 10.89 sec   HDFS Read: 883908 HDFS Write: 182 SUCCESS
Stage-Stage-4: Map: 2  Reduce: 1   Cumulative CPU: 10.73 sec   HDFS Read: 489429 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.69 sec   HDFS Read: 21756 HDFS Write: 822 SUCCESS
Total MapReduce CPU Time Spent: 28 seconds 310 msec
OK
Time taken: 76.371 seconds

2020-04-22 14:31:57 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:31:57 Last login: Wed Apr 22 14:28:00 2020 from 10.10.30.233

2020-04-22 14:31:57 su hadoop

2020-04-22 14:31:57 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:31:57 [hadoop@hadoop-01 root]$ 
2020-04-22 14:31:57 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:31:57 cat hive/statistic/add_user_day.hive
-- 2、天新增用户数据
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
insert overwrite table add_page_user_day partition(y,m,d)
select vud.parent_column_id,vud.user_type,vud.user_id,vud.area_code,vud.create_time,substr(vud.create_time,1,4) y,substr(vud.create_time,6,2) m,substr(vud.create_time,9,2) d   from(
  select parent_column_id,user_type,user_id,area_code,min(create_time) as create_time from visit_user_page_day
  where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by parent_column_id,user_type,user_id.area_code
) vud
 left join user_info aud on aud.user_id=vud.user_id and aud.user_type=vud.user_type and  aud.parent_column_id=vud.parent_column_id
 where aud.user_id is null;
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:31:57 Last login: Wed Apr 22 14:32:01 2020 from 10.10.30.233

2020-04-22 14:31:57 su hadoop

2020-04-22 14:31:57 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:31:57 [hadoop@hadoop-01 root]$ 
2020-04-22 14:31:57 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:31:57 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 14:31:58 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 14:31:58 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:32:00 Hive Session ID = d31ed987-2fb1-4e3e-a71c-b3d2d2913dd4

2020-04-22 14:32:00 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:32:07 Hive Session ID = 00ccec24-3bcf-4e95-929e-acd39a5793e6

2020-04-22 14:32:08 OK
Time taken: 0.791 seconds

2020-04-22 14:32:10 FAILED: SemanticException [Error 10042]: Line 5:120 . Operator is only supported on struct or list of struct types 'area_code'

2020-04-22 14:32:10 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:32:10 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 14:32:10 Last login: Wed Apr 22 14:32:01 2020 from 10.10.30.233

2020-04-22 14:32:10 su hadoop

2020-04-22 14:32:10 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:32:10 [hadoop@hadoop-01 root]$ 
2020-04-22 14:32:10 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:32:10 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 14:32:10 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:32:11 2020-04-22 14:32:15,208 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 14:32:11 2020-04-22 14:32:15,234 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:32:11 2020-04-22 14:32:15,316 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 14:32:15,319 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:32:11 2020-04-22 14:32:15,522 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 14:32:11 2020-04-22 14:32:15,537 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 14:32:11 2020-04-22 14:32:15,542 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:32:12 注: /tmp/sqoop-hadoop/compile/3d6c2062e0b004f6879c89729c8886d0/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:32:16,576 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/3d6c2062e0b004f6879c89729c8886d0/user_info.jar
2020-04-22 14:32:16,584 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 14:32:16,584 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 14:32:16,584 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 14:32:16,584 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 14:32:16,584 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 14:32:16,590 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 14:32:16,590 INFO Configuration.deprecation: mapred.j
2020-04-22 14:32:12 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 14:32:12 2020-04-22 14:32:16,693 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 14:32:13 2020-04-22 14:32:17,350 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=23 does not exist

2020-04-22 14:32:13 2020-04-22 14:32:17,369 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 14:32:13 
2020-04-22 14:32:17,371 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 14:32:17,371 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:32:14 2020-04-22 14:32:17,816 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0417

2020-04-22 14:32:15 2020-04-22 14:32:19,091 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0417
2020-04-22 14:32:19,096 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=23
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
2020-04-22 14:32:15 
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at or
2020-04-22 14:32:15 g.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 14:32:15 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:32:15 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 14:32:16 Last login: Wed Apr 22 14:32:14 2020 from 10.10.30.233

2020-04-22 14:32:16 su hadoop

2020-04-22 14:32:16 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:32:16 [hadoop@hadoop-01 root]$ 
2020-04-22 14:32:16 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:32:16 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 14:32:16 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:32:16 2020-04-22 14:32:20,397 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 14:32:20,423 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:32:16 2020-04-22 14:32:20,500 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 14:32:20,504 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:32:16 2020-04-22 14:32:20,703 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 14:32:20,718 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 14:32:20,723 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:32:17 注: /tmp/sqoop-hadoop/compile/5fe2b6158236dc957373035e981737d8/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:32:21,654 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/5fe2b6158236dc957373035e981737d8/user_info.jar
2020-04-22 14:32:21,661 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 14:32:21,661 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 14:32:21,661 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 14:32:21,661 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 14:32:21,661 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-22 14:32:17 2020-04-22 14:32:21,667 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 14:32:21,667 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 14:32:18 2020-04-22 14:32:21,765 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 14:32:18 2020-04-22 14:32:22,391 WARN mapreduce.ExportJobBase: Input path hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=24 does not exist
2020-04-22 14:32:22,411 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 14:32:22,412 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 14:32:18 2020-04-22 14:32:22,413 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:32:19 2020-04-22 14:32:22,845 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0418

2020-04-22 14:32:21 2020-04-22 14:32:25,019 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0418
2020-04-22 14:32:25,023 ERROR tool.ExportTool: Encountered IOException running export job: 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://hadoop-cluster/data/hive/user/page_add/y=2020/m=03/d=24
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:332)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:274)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51)
	at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
2020-04-22 14:32:21 
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324)
	at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301)
	at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442)
	at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)
	at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:73)
	at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at or
2020-04-22 14:32:21 g.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

2020-04-22 14:32:21 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:32:21 日统计数据已完成
2020-04-22 14:33:12 开始处理对日统计数据
2020-04-22 14:33:12 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \
--hive-table user_info \
--hive-overwrite
2020-04-22 14:33:12 Last login: Wed Apr 22 14:32:19 2020 from 10.10.30.233

2020-04-22 14:33:12 su hadoop

2020-04-22 14:33:12 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:33:12 [hadoop@hadoop-01 root]$ 
2020-04-22 14:33:12 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:33:12 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> --hive-table user_info \
> --hive-overwrite

2020-04-22 14:33:12 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:33:13 2020-04-22 14:33:16,899 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 14:33:13 2020-04-22 14:33:16,925 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:33:13 2020-04-22 14:33:16,970 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-22 14:33:16,979 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 14:33:16,981 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:33:13 2020-04-22 14:33:17,199 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:33:17,203 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:33:13 2020-04-22 14:33:17,219 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:33:14 注: /tmp/sqoop-hadoop/compile/b41fb34f18f9373b3733a12ac7ec6d68/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:33:18,218 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/b41fb34f18f9373b3733a12ac7ec6d68/user_info.jar

2020-04-22 14:33:15 2020-04-22 14:33:18,902 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-22 14:33:18,906 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 14:33:18,907 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 14:33:18,911 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 14:33:18,913 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:33:18,922 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:33:15 2020-04-22 14:33:19,417 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0419

2020-04-22 14:33:16 2020-04-22 14:33:20,289 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 14:33:16 2020-04-22 14:33:20,332 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 14:33:16 2020-04-22 14:33:20,502 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0419
2020-04-22 14:33:20,503 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 14:33:16 2020-04-22 14:33:20,693 INFO conf.Configuration: resource-types.xml not found
2020-04-22 14:33:20,693 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 14:33:17 2020-04-22 14:33:20,751 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0419

2020-04-22 14:33:17 2020-04-22 14:33:20,780 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0419/
2020-04-22 14:33:20,781 INFO mapreduce.Job: Running job: job_1587346943308_0419

2020-04-22 14:33:23 2020-04-22 14:33:26,869 INFO mapreduce.Job: Job job_1587346943308_0419 running in uber mode : false
2020-04-22 14:33:26,871 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 14:33:28 2020-04-22 14:33:31,939 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 14:33:31,950 INFO mapreduce.Job: Job job_1587346943308_0419 completed successfully

2020-04-22 14:33:28 2020-04-22 14:33:32,047 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2586
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2586
		Total vcore-milliseconds taken by all map tasks=2586
		Total megabyte-milliseconds taken by all map tasks=2648064
	Map-Reduce Framework
		Map input records=0
		Map output records=0
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=54
		CPU time spent (ms)=830
	
2020-04-22 14:33:28 	Physical memory (bytes) snapshot=242946048
		Virtual memory (bytes) snapshot=2446467072
		Total committed heap usage (bytes)=188743680
		Peak Map Physical memory (bytes)=242946048
		Peak Map Virtual memory (bytes)=2446467072
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 14:33:32,052 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 13.1227 seconds (0 bytes/sec)
2020-04-22 14:33:32,055 INFO mapreduce.ImportJobBase: Retrieved 0 records.
2020-04-22 14:33:32,055 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info
2020-04-22 14:33:32,069 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 14:33:32,071 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 14:33:28 2020-04-22 14:33:32,097 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 14:33:32,103 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 14:33:28 2020-04-22 14:33:32,266 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 14:33:29 2020-04-22 14:33:32,822 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-22 14:33:32,857 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 14:33:29 2020-04-22 14:33:33,338 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 14:33:33,338 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 14:33:33,339 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 14:33:33,339 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 14:33:33,343 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:33:31 2020-04-22 14:33:35,453 INFO hive.HiveImport: Hive Session ID = 71daa29b-ee41-4970-b228-c1d4f43cf2ae

2020-04-22 14:33:31 2020-04-22 14:33:35,563 INFO hive.HiveImport: 
2020-04-22 14:33:35,563 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:33:39 2020-04-22 14:33:42,742 INFO hive.HiveImport: Hive Session ID = 8d2be2e0-3b0f-4482-9f2c-9cdd699e20df

2020-04-22 14:33:40 2020-04-22 14:33:44,344 INFO hive.HiveImport: OK
2020-04-22 14:33:44,355 INFO hive.HiveImport: Time taken: 1.538 seconds

2020-04-22 14:33:40 2020-04-22 14:33:44,526 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 14:33:41 2020-04-22 14:33:45,111 INFO hive.HiveImport: OK
2020-04-22 14:33:45,129 INFO hive.HiveImport: Time taken: 0.744 seconds

2020-04-22 14:33:41 2020-04-22 14:33:45,634 INFO hive.HiveImport: Hive import complete.

2020-04-22 14:33:42 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:33:42 Last login: Wed Apr 22 14:33:16 2020 from 10.10.30.233

2020-04-22 14:33:42 su hadoop

2020-04-22 14:33:42 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:33:42 [hadoop@hadoop-01 root]$ 
2020-04-22 14:33:42 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:33:42 cat hive/statistic/RegionbyDay.hive

-- 按天统计专区信息
-- 进入数据库
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
-- 1、按天去重 日访问用户信息
insert overwrite table visit_user_page_day partition(y,m,d)  select param["parentColumnId"] as parent_column_id,user_type,user_id ,'' area_code,min(create_time) as create_time,y,m,d from events_type_log
where events_type in ('operationDetails','operationPage') and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param["parentColumnId"],user_type,user_id,y,m,d cluster by user_id,user_type;




--3、 按天去除重复播放用户
insert overwrite table play_user_day partition(y,m,d)  select param['parentColumnId'] as parent_column_id,user_type,user_id ,'' area_code,sum(1) as play_count,sum(cast(nvl(param['timePosition'],'0') as
2020-04-22 14:33:42  bigint)) as duration,y,m,d from events_type_log
where events_type='operateResumePoint' and param['operateType']='add' and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param['parentColumnId'],user_type,user_id,y,m,d cluster by user_id,user_type;


-- 统计日页面访问用户数，统计日播放用户数，统计日播放次数
-- 统计页面访问用户数
insert  overwrite table  region_count_day partition(t_date)
select a.parent_column_id,a.user_type,sum(a.page_user_count) as page_user_count,sum(play_user_count) as play_user_count,sum(a.duration) as duration,sum(a.play_count) as play_count,a.t as t_date from (
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type, count(1) as page_user_count,0 as play_user_count,0 as duration,0 as play_count from visit_user_page_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
UNION ALL
select concat(y,'-',m,'-',d) as t,p
2020-04-22 14:33:42 arent_column_id,user_type,0 as page_user_count,count(1) as play_user_count, sum(duration) as duration,sum(play_count) as play_count from play_user_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
) a group by a.t,a.parent_column_id,a.user_type;

-- 统计 播放次数
-- select sum(cast(nvl(data['timePosition'],'0') as bigint)) as duration,count(1) as play_count from events_type_log where y=2020 and m='04' and d='01' and data['operateType']='add';


[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:33:42 Last login: Wed Apr 22 14:33:46 2020 from 10.10.30.233

2020-04-22 14:33:42 su hadoop

2020-04-22 14:33:42 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:33:42 [hadoop@hadoop-01 root]$ 
2020-04-22 14:33:42 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:33:42 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 14:33:43 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 14:33:43 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 14:33:43 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:33:45 Hive Session ID = c1e5d406-77f4-4c97-9683-5f766d5ba5f4

2020-04-22 14:33:45 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:33:53 Hive Session ID = 1bb922cc-ff7d-4d68-a329-9bd83a5b79ee

2020-04-22 14:33:53 OK
Time taken: 0.791 seconds

2020-04-22 14:33:55 Query ID = hadoop_20200422143357_604167fb-dbb0-4e71-9d1e-befa592193e7
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 14:33:55 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:33:57 Starting Job = job_1587346943308_0420, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0420/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0420

2020-04-22 14:34:03 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 14:34:03 2020-04-22 14:34:07,716 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:34:13 2020-04-22 14:34:17,209 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.28 sec

2020-04-22 14:34:14 2020-04-22 14:34:18,244 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.47 sec

2020-04-22 14:34:20 2020-04-22 14:34:24,467 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.62 sec

2020-04-22 14:34:21 MapReduce Total cumulative CPU time: 20 seconds 620 msec

2020-04-22 14:34:21 Ended Job = job_1587346943308_0420

2020-04-22 14:34:21 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:34:22 Starting Job = job_1587346943308_0421, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0421/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0421

2020-04-22 14:34:32 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 14:34:32 2020-04-22 14:34:36,373 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:34:38 2020-04-22 14:34:42,593 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.88 sec

2020-04-22 14:34:46 2020-04-22 14:34:49,856 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.23 sec

2020-04-22 14:34:47 MapReduce Total cumulative CPU time: 7 seconds 230 msec
Ended Job = job_1587346943308_0421

2020-04-22 14:34:47 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 14:34:47 


2020-04-22 14:34:47 	 Time taken to load dynamic partitions: 0.315 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3

2020-04-22 14:34:47 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:34:48 Starting Job = job_1587346943308_0422, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0422/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0422

2020-04-22 14:34:58 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 14:34:58 2020-04-22 14:35:02,031 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:35:04 2020-04-22 14:35:08,244 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.4 sec

2020-04-22 14:35:10 2020-04-22 14:35:14,434 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.22 sec

2020-04-22 14:35:11 MapReduce Total cumulative CPU time: 4 seconds 220 msec

2020-04-22 14:35:11 Ended Job = job_1587346943308_0422

2020-04-22 14:35:12 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 20.62 sec   HDFS Read: 193048982 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.23 sec   HDFS Read: 1476283 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.22 sec   HDFS Read: 16676 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 32 seconds 70 msec
OK
Time taken: 78.678 seconds

2020-04-22 14:35:12 Query ID = hadoop_20200422143516_5bf6fda4-2e72-4425-b21d-503743d042d1
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 14:35:12 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:35:13 Starting Job = job_1587346943308_0423, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0423/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0423

2020-04-22 14:35:23 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2020-04-22 14:35:26,898 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:35:31 2020-04-22 14:35:35,165 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 5.2 sec

2020-04-22 14:35:33 2020-04-22 14:35:37,247 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.65 sec

2020-04-22 14:35:38 2020-04-22 14:35:42,399 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.92 sec

2020-04-22 14:35:39 MapReduce Total cumulative CPU time: 16 seconds 920 msec
Ended Job = job_1587346943308_0423

2020-04-22 14:35:39 Launching Job 2 out of 3

2020-04-22 14:35:39 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:35:40 Starting Job = job_1587346943308_0424, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0424/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0424

2020-04-22 14:35:50 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 14:35:50 2020-04-22 14:35:54,186 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:35:57 2020-04-22 14:36:01,419 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.7 sec

2020-04-22 14:36:03 2020-04-22 14:36:07,615 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.51 sec

2020-04-22 14:36:04 MapReduce Total cumulative CPU time: 7 seconds 510 msec
Ended Job = job_1587346943308_0424

2020-04-22 14:36:04 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 14:36:05 


2020-04-22 14:36:05 	 Time taken to load dynamic partitions: 0.316 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:36:06 Starting Job = job_1587346943308_0425, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0425/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0425

2020-04-22 14:36:16 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 14:36:16 2020-04-22 14:36:20,190 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:36:21 2020-04-22 14:36:25,430 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.53 sec

2020-04-22 14:36:27 2020-04-22 14:36:31,602 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.2 sec

2020-04-22 14:36:28 MapReduce Total cumulative CPU time: 4 seconds 200 msec

2020-04-22 14:36:28 Ended Job = job_1587346943308_0425

2020-04-22 14:36:29 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 16.92 sec   HDFS Read: 84990506 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.51 sec   HDFS Read: 893596 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.2 sec   HDFS Read: 19219 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 28 seconds 630 msec
OK

2020-04-22 14:36:29 Time taken: 76.59 seconds

2020-04-22 14:36:31 Query ID = hadoop_20200422143633_91640378-73c4-4df9-910b-0dc498ed0344
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 14:36:31 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:36:31 Starting Job = job_1587346943308_0426, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0426/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0426

2020-04-22 14:36:39 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 14:36:39 2020-04-22 14:36:43,515 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:36:48 2020-04-22 14:36:51,782 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.43 sec

2020-04-22 14:36:55 2020-04-22 14:36:58,987 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.12 sec

2020-04-22 14:36:56 MapReduce Total cumulative CPU time: 11 seconds 120 msec
Ended Job = job_1587346943308_0426
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:36:56 Starting Job = job_1587346943308_0427, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0427/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0427

2020-04-22 14:37:08 Hadoop job information for Stage-4: number of mappers: 2; number of reducers: 1

2020-04-22 14:37:08 2020-04-22 14:37:11,772 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:37:15 2020-04-22 14:37:19,046 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 6.98 sec

2020-04-22 14:37:22 2020-04-22 14:37:26,262 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 10.61 sec

2020-04-22 14:37:23 MapReduce Total cumulative CPU time: 10 seconds 610 msec
Ended Job = job_1587346943308_0427
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:37:24 Starting Job = job_1587346943308_0428, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0428/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0428

2020-04-22 14:37:34 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2020-04-22 14:37:38,032 Stage-2 map = 0%,  reduce = 0%

2020-04-22 14:37:41 2020-04-22 14:37:45,229 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.12 sec

2020-04-22 14:37:46 2020-04-22 14:37:50,366 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.01 sec

2020-04-22 14:37:47 MapReduce Total cumulative CPU time: 6 seconds 10 msec
Ended Job = job_1587346943308_0428

2020-04-22 14:37:47 Loading data to table yn_hadoop.region_count_day partition (t_date=null)

2020-04-22 14:37:47 


2020-04-22 14:37:47 	 Time taken to load dynamic partitions: 0.222 seconds
	 Time taken for adding to write entity : 0.0 seconds

2020-04-22 14:37:48 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 11.12 sec   HDFS Read: 883908 HDFS Write: 182 SUCCESS
Stage-Stage-4: Map: 2  Reduce: 1   Cumulative CPU: 10.61 sec   HDFS Read: 489429 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.01 sec   HDFS Read: 21763 HDFS Write: 822 SUCCESS
Total MapReduce CPU Time Spent: 27 seconds 740 msec
OK
Time taken: 78.887 seconds

2020-04-22 14:37:48 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:37:48 Last login: Wed Apr 22 14:33:46 2020 from 10.10.30.233

2020-04-22 14:37:48 su hadoop

2020-04-22 14:37:48 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:37:48 [hadoop@hadoop-01 root]$ 
2020-04-22 14:37:48 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:37:48 cat hive/statistic/add_user_day.hive

2020-04-22 14:37:48 -- 2、天新增用户数据
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
insert overwrite table add_page_user_day partition(y,m,d)
select vud.parent_column_id,vud.user_type,vud.user_id,vud.area_code,vud.create_time,substr(vud.create_time,1,4) y,substr(vud.create_time,6,2) m,substr(vud.create_time,9,2) d   from(
  select parent_column_id,user_type,user_id,area_code,min(create_time) as create_time from visit_user_page_day
  where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by parent_column_id,user_type,user_id,area_code
) vud
 left join user_info aud on aud.user_id=vud.user_id and aud.user_type=vud.user_type and  aud.parent_column_id=vud.parent_column_id
 where aud.user_id is null;
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:37:49 Last login: Wed Apr 22 14:37:52 2020 from 10.10.30.233

2020-04-22 14:37:49 su hadoop

2020-04-22 14:37:49 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:37:49 [hadoop@hadoop-01 root]$ 
2020-04-22 14:37:49 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:37:49 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 14:37:49 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 14:37:50 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 14:37:52 Hive Session ID = 75f71cfd-607e-4fce-99d9-4fda5c709514

Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 14:37:58 Hive Session ID = 782ffe03-c2ef-45f9-89a3-20ebcdbd9fb0

2020-04-22 14:37:59 OK
Time taken: 0.8 seconds

2020-04-22 14:38:02 Query ID = hadoop_20200422143803_4f70ecd5-90ca-436a-a205-189b17f13133
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:38:04 Starting Job = job_1587346943308_0429, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0429/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0429

2020-04-22 14:38:10 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 14:38:10 2020-04-22 14:38:14,135 Stage-1 map = 0%,  reduce = 0%

2020-04-22 14:38:17 2020-04-22 14:38:21,499 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 8.2 sec

2020-04-22 14:38:25 2020-04-22 14:38:28,751 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.51 sec

2020-04-22 14:38:26 MapReduce Total cumulative CPU time: 12 seconds 510 msec
Ended Job = job_1587346943308_0429

2020-04-22 14:38:26 SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]

2020-04-22 14:38:32 SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]2020-04-22 14:38:36	Dump the side-table for tag: 1 with group count: 0 into file: file:/tmp/hive/75f71cfd-607e-4fce-99d9-4fda5c709514/hive_2020-04-22_14-38-03_447_2706393006664990003-1/-local-10004/HashTable-Stage-6/MapJoin-mapfile01--.hashtable

2020-04-22 14:38:32 2020-04-22 14:38:36	Uploaded 1 File to: file:/tmp/hive/75f71cfd-607e-4fce-99d9-4fda5c709514/hive_2020-04-22_14-38-03_447_2706393006664990003-1/-local-10004/HashTable-Stage-6/MapJoin-mapfile01--.hashtable (260 bytes)
2020-04-22 14:38:36	End of local task; Time Taken: 0.733 sec.

2020-04-22 14:38:33 Execution completed successfully
MapredLocal task succeeded

2020-04-22 14:38:33 Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-22 14:38:34 Starting Job = job_1587346943308_0430, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0430/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0430

2020-04-22 14:38:40 Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 0
2020-04-22 14:38:43,906 Stage-6 map = 0%,  reduce = 0%

2020-04-22 14:38:46 2020-04-22 14:38:50,158 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 5.05 sec

2020-04-22 14:38:48 MapReduce Total cumulative CPU time: 5 seconds 50 msec
Ended Job = job_1587346943308_0430

2020-04-22 14:38:48 Loading data to table yn_hadoop.add_page_user_day partition (y=null, m=null, d=null)

2020-04-22 14:38:48 


2020-04-22 14:38:49 	 Time taken to load dynamic partitions: 0.501 seconds
	 Time taken for adding to write entity : 0.002 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 14:38:50 Starting Job = job_1587346943308_0431, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0431/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0431

2020-04-22 14:38:59 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 14:39:02,953 Stage-4 map = 0%,  reduce = 0%

2020-04-22 14:39:04 2020-04-22 14:39:08,131 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.39 sec

2020-04-22 14:39:10 2020-04-22 14:39:14,329 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.95 sec

2020-04-22 14:39:11 MapReduce Total cumulative CPU time: 3 seconds 950 msec
Ended Job = job_1587346943308_0431

2020-04-22 14:39:12 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 12.51 sec   HDFS Read: 884386 HDFS Write: 888924 SUCCESS
Stage-Stage-6: Map: 1   Cumulative CPU: 5.05 sec   HDFS Read: 899146 HDFS Write: 622379 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.95 sec   HDFS Read: 16678 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 21 seconds 510 msec
OK

2020-04-22 14:39:12 Time taken: 72.737 seconds

2020-04-22 14:39:12 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:39:12 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 14:39:13 Last login: Wed Apr 22 14:37:52 2020 from 10.10.30.233

2020-04-22 14:39:13 su hadoop

2020-04-22 14:39:13 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:39:13 [hadoop@hadoop-01 root]$ 
2020-04-22 14:39:13 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:39:13 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 14:39:13 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:39:13 2020-04-22 14:39:17,547 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 14:39:13 2020-04-22 14:39:17,574 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:39:13 2020-04-22 14:39:17,650 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.

2020-04-22 14:39:13 2020-04-22 14:39:17,653 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:39:14 2020-04-22 14:39:17,882 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 14:39:17,905 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 14:39:17,912 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:39:15 注: /tmp/sqoop-hadoop/compile/0719e6fd4ae7bb99a5624600886037ab/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:39:18,877 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/0719e6fd4ae7bb99a5624600886037ab/user_info.jar
2020-04-22 14:39:18,884 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 14:39:18,884 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 14:39:18,884 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 14:39:18,884 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 14:39:18,884 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 14:39:18,890 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 14:39:18,890 INFO Configuration.deprecation: mapred.j
2020-04-22 14:39:15 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 14:39:15 2020-04-22 14:39:18,992 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 14:39:15 2020-04-22 14:39:19,714 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 14:39:19,716 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 14:39:19,716 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:39:16 2020-04-22 14:39:20,256 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0432

2020-04-22 14:39:18 2020-04-22 14:39:22,056 INFO input.FileInputFormat: Total input files to process : 1
2020-04-22 14:39:22,058 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 14:39:18 2020-04-22 14:39:22,106 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-22 14:39:18 2020-04-22 14:39:22,134 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 14:39:18 2020-04-22 14:39:22,236 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0432
2020-04-22 14:39:22,238 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 14:39:18 2020-04-22 14:39:22,392 INFO conf.Configuration: resource-types.xml not found
2020-04-22 14:39:22,392 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 14:39:18 2020-04-22 14:39:22,439 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0432

2020-04-22 14:39:18 2020-04-22 14:39:22,463 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0432/
2020-04-22 14:39:22,464 INFO mapreduce.Job: Running job: job_1587346943308_0432

2020-04-22 14:39:24 2020-04-22 14:39:28,548 INFO mapreduce.Job: Job job_1587346943308_0432 running in uber mode : false
2020-04-22 14:39:28,550 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 14:39:31 2020-04-22 14:39:35,724 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 14:39:35,733 INFO mapreduce.Job: Job job_1587346943308_0432 completed successfully

2020-04-22 14:39:32 2020-04-22 14:39:35,845 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=503452
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=21709
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=21709
		Total vcore-milliseconds taken by all map tasks=21709
		Total megabyte-milliseconds taken by all map tasks=22230016
	Map-Reduce Framework
		Map input records=11333
		Map output records=11333
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=840
		CPU time
2020-04-22 14:39:32  spent (ms)=4220
		Physical memory (bytes) snapshot=949460992
		Virtual memory (bytes) snapshot=9777074176
		Total committed heap usage (bytes)=753401856
		Peak Map Physical memory (bytes)=243040256
		Peak Map Virtual memory (bytes)=2446131200
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 14:39:35,860 INFO mapreduce.ExportJobBase: Transferred 491.6523 KB in 16.1284 seconds (30.4836 KB/sec)

2020-04-22 14:39:32 2020-04-22 14:39:35,864 INFO mapreduce.ExportJobBase: Exported 11333 records.

2020-04-22 14:39:32 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:39:32 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 14:39:32 Last login: Wed Apr 22 14:39:26 2020 from 10.10.30.233

2020-04-22 14:39:32 su hadoop

2020-04-22 14:39:32 [root@hadoop-01 ~]# su hadoop

2020-04-22 14:39:32 [hadoop@hadoop-01 root]$ 
2020-04-22 14:39:32 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:39:32 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 14:39:32 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 14:39:33 2020-04-22 14:39:37,230 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 14:39:37,255 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 14:39:33 2020-04-22 14:39:37,339 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 14:39:37,342 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 14:39:33 2020-04-22 14:39:37,624 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 14:39:33 2020-04-22 14:39:37,679 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 14:39:37,687 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 14:39:35 注: /tmp/sqoop-hadoop/compile/6171d39a08b335b6bdd148f0df66b03f/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 14:39:38,865 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/6171d39a08b335b6bdd148f0df66b03f/user_info.jar

2020-04-22 14:39:35 2020-04-22 14:39:38,873 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 14:39:38,873 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 14:39:38,873 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 14:39:38,873 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 14:39:38,873 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-22 14:39:35 2020-04-22 14:39:38,879 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 14:39:38,879 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 14:39:35 2020-04-22 14:39:38,986 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 14:39:36 2020-04-22 14:39:39,845 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 14:39:39,846 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 14:39:39,847 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 14:39:36 2020-04-22 14:39:40,286 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0433

2020-04-22 14:39:39 2020-04-22 14:39:42,997 INFO input.FileInputFormat: Total input files to process : 1
2020-04-22 14:39:42,999 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 14:39:39 2020-04-22 14:39:43,070 INFO mapreduce.JobSubmitter: number of splits:4
2020-04-22 14:39:43,104 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 14:39:39 2020-04-22 14:39:43,200 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0433
2020-04-22 14:39:43,201 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 14:39:39 2020-04-22 14:39:43,369 INFO conf.Configuration: resource-types.xml not found
2020-04-22 14:39:43,370 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 14:39:39 2020-04-22 14:39:43,418 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0433

2020-04-22 14:39:39 2020-04-22 14:39:43,447 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0433/
2020-04-22 14:39:43,448 INFO mapreduce.Job: Running job: job_1587346943308_0433

2020-04-22 14:39:45 2020-04-22 14:39:49,535 INFO mapreduce.Job: Job job_1587346943308_0433 running in uber mode : false
2020-04-22 14:39:49,537 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 14:39:52 2020-04-22 14:39:56,620 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 14:39:56,631 INFO mapreduce.Job: Job job_1587346943308_0433 completed successfully

2020-04-22 14:39:53 2020-04-22 14:39:56,743 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=395458
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20254
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20254
		Total vcore-milliseconds taken by all map tasks=20254
		Total megabyte-milliseconds taken by all map tasks=20740096
	Map-Reduce Framework
		Map input records=4555
		Map output records=4555
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=639
		CPU time s
2020-04-22 14:39:53 pent (ms)=3670
		Physical memory (bytes) snapshot=941572096
		Virtual memory (bytes) snapshot=9779130368
		Total committed heap usage (bytes)=744488960
		Peak Map Physical memory (bytes)=242208768
		Peak Map Virtual memory (bytes)=2446028800
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 14:39:56,750 INFO mapreduce.ExportJobBase: Transferred 386.1895 KB in 16.8941 seconds (22.8594 KB/sec)
2020-04-22 14:39:56,754 INFO mapreduce.ExportJobBase: Exported 4555 records.

2020-04-22 14:39:53 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 14:39:53 日统计数据已完成
2020-04-22 17:18:04 开始处理对日统计数据
2020-04-22 17:18:04 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \
--hive-table user_info \
--hive-overwrite
2020-04-22 17:18:04 Last login: Wed Apr 22 16:36:47 2020 from 10.10.30.249

2020-04-22 17:18:04 su hadoop

2020-04-22 17:18:04 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:18:04 [hadoop@hadoop-01 root]$ 
2020-04-22 17:18:04 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:18:04 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> --hive-table user_info \
> --hive-overwrite

2020-04-22 17:18:04 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:18:05 2020-04-22 17:18:09,149 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 17:18:09,183 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:18:05 2020-04-22 17:18:09,238 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-22 17:18:09,245 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 17:18:09,248 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:18:05 2020-04-22 17:18:09,487 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 17:18:09,492 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 17:18:09,514 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:18:06 注: /tmp/sqoop-hadoop/compile/3f3e033a622756dd5f4c6384e0dacb9c/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 17:18:10,491 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/3f3e033a622756dd5f4c6384e0dacb9c/user_info.jar

2020-04-22 17:18:07 2020-04-22 17:18:11,236 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-22 17:18:11,241 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 17:18:11,242 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 17:18:11,245 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 17:18:11,247 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 17:18:11,256 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:18:07 2020-04-22 17:18:11,736 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0434

2020-04-22 17:18:09 2020-04-22 17:18:13,485 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 17:18:09 2020-04-22 17:18:13,526 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 17:18:10 2020-04-22 17:18:14,148 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0434
2020-04-22 17:18:14,151 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:18:10 2020-04-22 17:18:14,346 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:18:14,346 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:18:10 2020-04-22 17:18:14,393 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0434

2020-04-22 17:18:10 2020-04-22 17:18:14,418 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0434/
2020-04-22 17:18:14,418 INFO mapreduce.Job: Running job: job_1587346943308_0434

2020-04-22 17:18:16 2020-04-22 17:18:20,490 INFO mapreduce.Job: Job job_1587346943308_0434 running in uber mode : false
2020-04-22 17:18:20,492 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 17:18:21 2020-04-22 17:18:25,558 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-22 17:18:21 2020-04-22 17:18:25,569 INFO mapreduce.Job: Job job_1587346943308_0434 completed successfully

2020-04-22 17:18:21 2020-04-22 17:18:25,683 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2801
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2801
		Total vcore-milliseconds taken by all map tasks=2801
		Total megabyte-milliseconds taken by all map tasks=2868224
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=64
		CPU time spen
2020-04-22 17:18:21 t (ms)=1180
		Physical memory (bytes) snapshot=242913280
		Virtual memory (bytes) snapshot=2447069184
		Total committed heap usage (bytes)=191889408
		Peak Map Physical memory (bytes)=242913280
		Peak Map Virtual memory (bytes)=2447069184
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572
2020-04-22 17:18:25,691 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 14.4259 seconds (41.8742 KB/sec)

2020-04-22 17:18:21 2020-04-22 17:18:25,695 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-22 17:18:25,695 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info

2020-04-22 17:18:21 2020-04-22 17:18:25,707 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 17:18:21 2020-04-22 17:18:25,708 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 17:18:21 2020-04-22 17:18:25,730 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 17:18:25,738 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 17:18:22 2020-04-22 17:18:25,915 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 17:18:22 2020-04-22 17:18:26,500 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-22 17:18:26,527 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 17:18:23 2020-04-22 17:18:26,988 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 17:18:26,988 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 17:18:26,988 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 17:18:26,988 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 17:18:23 2020-04-22 17:18:27,001 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 17:18:25 2020-04-22 17:18:29,063 INFO hive.HiveImport: Hive Session ID = adc6b078-966d-4b7a-b041-4adfa7305691

2020-04-22 17:18:25 2020-04-22 17:18:29,149 INFO hive.HiveImport: 
2020-04-22 17:18:29,149 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 17:18:32 2020-04-22 17:18:35,986 INFO hive.HiveImport: Hive Session ID = f051782a-c1dc-4224-9eff-ff81a0a4d55d

2020-04-22 17:18:33 2020-04-22 17:18:37,545 INFO hive.HiveImport: OK

2020-04-22 17:18:33 2020-04-22 17:18:37,556 INFO hive.HiveImport: Time taken: 1.493 seconds

2020-04-22 17:18:34 2020-04-22 17:18:37,898 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 17:18:34 2020-04-22 17:18:38,458 INFO hive.HiveImport: OK
2020-04-22 17:18:38,473 INFO hive.HiveImport: Time taken: 0.89 seconds

2020-04-22 17:18:35 2020-04-22 17:18:38,974 INFO hive.HiveImport: Hive import complete.

2020-04-22 17:18:35 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:18:35 Last login: Wed Apr 22 17:18:08 2020 from 10.10.30.233

2020-04-22 17:18:35 su hadoop

2020-04-22 17:18:35 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:18:35 [hadoop@hadoop-01 root]$ 
2020-04-22 17:18:35 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:18:35 cat hive/statistic/RegionbyDay.hive

-- 按天统计专区信息
-- 进入数据库
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
-- 1、按天去重 日访问用户信息
insert overwrite table visit_user_page_day partition(y,m,d)  select param["parentColumnId"] as parent_column_id,user_type,user_id ,'' area_code,min(create_time) as create_time,y,m,d from events_type_log
where events_type in ('operationDetails','operationPage') and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param["parentColumnId"],user_type,user_id,y,m,d cluster by user_id,user_type;




--3、 按天去除重复播放用户
insert overwrite table play_user_day partition(y,m,d)  select param['parentColumnId'] as parent_column_id,user_type,user_id ,'' area_code,sum(1) as play_count,sum(cast(nvl(param['timePosition'],'0') as
2020-04-22 17:18:35  bigint)) as duration,y,m,d from events_type_log
where events_type='operateResumePoint' and param['operateType']='add' and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param['parentColumnId'],user_type,user_id,y,m,d cluster by user_id,user_type;



-- 创建临时存放统计数据的表；
-- 统计日页面访问用户数，统计日播放用户数，统计日播放次数
-- 统计页面访问用户数
truncate table
insert  overwrite table  app_visit_count_day
select a.t as t_date,a.parent_column_id,a.user_type,sum(a.page_user_count) as page_user_count,sum(play_user_count) as play_user_count,sum(a.duration) as duration,sum(a.play_count) as play_count from (
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type, count(1) as page_user_count,0 as play_user_count,0 as duration,0 as play_count from visit_user_page_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
2020-04-22 17:18:35 
UNION ALL
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type,0 as page_user_count,count(1) as play_user_count, sum(duration) as duration,sum(play_count) as play_count from play_user_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
) a group by a.t,a.parent_column_id,a.user_type;

-- 统计 播放次数
-- select sum(cast(nvl(data['timePosition'],'0') as bigint)) as duration,count(1) as play_count from events_type_log where y=2020 and m='04' and d='01' and data['operateType']='add';


[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:18:36 Last login: Wed Apr 22 17:18:39 2020 from 10.10.30.233

2020-04-22 17:18:36 su hadoop

2020-04-22 17:18:36 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:18:36 [hadoop@hadoop-01 root]$ 
2020-04-22 17:18:36 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:18:36 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 17:18:36 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 17:18:37 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 17:18:37 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 17:18:39 Hive Session ID = 631e525f-9e80-474b-911a-03884fee0246

2020-04-22 17:18:39 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 17:18:46 Hive Session ID = 3417d2cd-0003-43bf-8097-d2d94fcd6fa9

2020-04-22 17:18:47 OK
Time taken: 0.802 seconds

2020-04-22 17:18:48 Query ID = hadoop_20200422171850_7cf8794b-3195-4944-955e-fd0e7a5d89ad
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 17:18:48 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:18:50 Starting Job = job_1587346943308_0435, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0435/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0435

2020-04-22 17:18:55 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 17:18:55 2020-04-22 17:18:59,673 Stage-1 map = 0%,  reduce = 0%

2020-04-22 17:19:06 2020-04-22 17:19:10,068 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.03 sec

2020-04-22 17:19:08 2020-04-22 17:19:12,137 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.1 sec

2020-04-22 17:19:11 2020-04-22 17:19:15,267 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.05 sec

2020-04-22 17:19:12 MapReduce Total cumulative CPU time: 20 seconds 50 msec

2020-04-22 17:19:12 Ended Job = job_1587346943308_0435

2020-04-22 17:19:12 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:19:13 Starting Job = job_1587346943308_0436, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0436/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0436

2020-04-22 17:19:23 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 17:19:23 2020-04-22 17:19:27,572 Stage-2 map = 0%,  reduce = 0%

2020-04-22 17:19:30 2020-04-22 17:19:33,787 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.26 sec

2020-04-22 17:19:38 2020-04-22 17:19:42,044 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.77 sec

2020-04-22 17:19:39 MapReduce Total cumulative CPU time: 6 seconds 770 msec
Ended Job = job_1587346943308_0436

2020-04-22 17:19:39 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 17:19:39 


2020-04-22 17:19:39 	 Time taken to load dynamic partitions: 0.357 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:19:41 Starting Job = job_1587346943308_0437, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0437/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0437

2020-04-22 17:19:50 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 17:19:50 2020-04-22 17:19:54,155 Stage-4 map = 0%,  reduce = 0%

2020-04-22 17:19:55 2020-04-22 17:19:59,330 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.58 sec

2020-04-22 17:20:02 2020-04-22 17:20:06,551 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.2 sec

2020-04-22 17:20:03 MapReduce Total cumulative CPU time: 4 seconds 200 msec
Ended Job = job_1587346943308_0437

2020-04-22 17:20:04 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 20.05 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 6.77 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.2 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 20 msec
OK
Time taken: 77.525 seconds

2020-04-22 17:20:04 Query ID = hadoop_20200422172008_f6ffd105-2df3-4a14-93a7-042a6e50ef10
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:20:05 Starting Job = job_1587346943308_0438, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0438/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0438

2020-04-22 17:20:15 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2020-04-22 17:20:19,499 Stage-1 map = 0%,  reduce = 0%

2020-04-22 17:20:24 2020-04-22 17:20:27,773 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 5.03 sec

2020-04-22 17:20:25 2020-04-22 17:20:28,821 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.73 sec

2020-04-22 17:20:31 2020-04-22 17:20:35,000 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.98 sec

2020-04-22 17:20:32 MapReduce Total cumulative CPU time: 16 seconds 980 msec
Ended Job = job_1587346943308_0438

2020-04-22 17:20:32 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:20:32 Starting Job = job_1587346943308_0439, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0439/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0439

2020-04-22 17:20:42 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 17:20:43 2020-04-22 17:20:46,799 Stage-2 map = 0%,  reduce = 0%

2020-04-22 17:20:49 2020-04-22 17:20:52,993 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.58 sec

2020-04-22 17:20:57 2020-04-22 17:21:01,273 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.13 sec

2020-04-22 17:20:58 MapReduce Total cumulative CPU time: 7 seconds 130 msec
Ended Job = job_1587346943308_0439

2020-04-22 17:20:58 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 17:20:58 


2020-04-22 17:20:58 	 Time taken to load dynamic partitions: 0.269 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:20:59 Starting Job = job_1587346943308_0440, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0440/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0440

2020-04-22 17:21:08 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 17:21:09 2020-04-22 17:21:12,778 Stage-4 map = 0%,  reduce = 0%

2020-04-22 17:21:15 2020-04-22 17:21:18,985 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec

2020-04-22 17:21:21 2020-04-22 17:21:25,172 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.07 sec

2020-04-22 17:21:22 MapReduce Total cumulative CPU time: 4 seconds 70 msec
Ended Job = job_1587346943308_0440

2020-04-22 17:21:22 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 16.98 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.13 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.07 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 28 seconds 180 msec
OK

2020-04-22 17:21:22 Time taken: 78.116 seconds

2020-04-22 17:21:22 NoViableAltException(158@[212:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:116)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4513)
	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:45148)
	at org.apache.hadoop.hive.ql.parse.HiveParser.tablePartitionPrefix(HiveParser.java:12566)
	at org.apache.hadoop.hive.ql.parse.HiveParser.truncateTableStatement(HiveParser.java:7057)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:4323)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2494)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1420)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtil
2020-04-22 17:21:22 s.java:74)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:335)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:471)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:487)
	at org.apache.hadoop.hive.cli.CliDr
2020-04-22 17:21:22 iver.executeDriver(CliDriver.java:793)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)

2020-04-22 17:21:22 FAILED: ParseException line 6:0 cannot recognize input near 'insert' 'overwrite' 'table' in table name

2020-04-22 17:21:23 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:21:23 Last login: Wed Apr 22 17:18:39 2020 from 10.10.30.233

2020-04-22 17:21:23 su hadoop

2020-04-22 17:21:23 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:21:23 [hadoop@hadoop-01 root]$ 
2020-04-22 17:21:23 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:21:23 cat hive/statistic/add_user_day.hive
-- 2、天新增用户数据
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
insert overwrite table add_page_user_day partition(y,m,d)
select vud.parent_column_id,vud.user_type,vud.user_id,vud.area_code,vud.create_time,substr(vud.create_time,1,4) y,substr(vud.create_time,6,2) m,substr(vud.create_time,9,2) d   from(
  select parent_column_id,user_type,user_id,area_code,min(create_time) as create_time from visit_user_page_day
  where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by parent_column_id,user_type,user_id,area_code
) vud
 left join user_info aud on aud.user_id=vud.user_id and aud.user_type=vud.user_type and  aud.parent_column_id=vud.parent_column_id
 where aud.user_id is null;
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:21:23 Last login: Wed Apr 22 17:21:27 2020 from 10.10.30.233

2020-04-22 17:21:23 su hadoop

2020-04-22 17:21:23 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:21:23 [hadoop@hadoop-01 root]$ 
2020-04-22 17:21:23 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:21:23 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 17:21:24 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 17:21:24 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 17:21:24 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 17:21:26 Hive Session ID = 0faf60ff-4a1a-43e8-adab-dd6e2cdd0fc0

2020-04-22 17:21:26 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 17:21:33 Hive Session ID = 9e9396c9-1228-4110-aa14-d330fbfc55b3

2020-04-22 17:21:34 OK

2020-04-22 17:21:34 Time taken: 0.85 seconds

2020-04-22 17:21:37 Query ID = hadoop_20200422172138_d020324d-5bf4-49a4-b393-13c0f2691f1e
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:21:39 Starting Job = job_1587346943308_0441, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0441/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0441

2020-04-22 17:21:45 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 17:21:45 2020-04-22 17:21:49,366 Stage-1 map = 0%,  reduce = 0%

2020-04-22 17:21:52 2020-04-22 17:21:56,746 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 8.12 sec

2020-04-22 17:21:58 2020-04-22 17:22:01,926 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.96 sec

2020-04-22 17:21:59 MapReduce Total cumulative CPU time: 11 seconds 960 msec

2020-04-22 17:21:59 Ended Job = job_1587346943308_0441

2020-04-22 17:21:59 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 17:22:04 2020-04-22 17:22:08	Starting to launch local task to process map join;	maximum memory = 239075328

2020-04-22 17:22:07 Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-22 17:22:09 Starting Job = job_1587346943308_0442, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0442/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0442

2020-04-22 17:22:15 Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 0
2020-04-22 17:22:18,990 Stage-6 map = 0%,  reduce = 0%

2020-04-22 17:22:21 2020-04-22 17:22:25,207 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 3.85 sec

2020-04-22 17:22:22 MapReduce Total cumulative CPU time: 3 seconds 850 msec
Ended Job = job_1587346943308_0442

2020-04-22 17:22:22 Loading data to table yn_hadoop.add_page_user_day partition (y=null, m=null, d=null)

2020-04-22 17:22:22 


2020-04-22 17:22:22 	 Time taken to load dynamic partitions: 0.024 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:22:23 Starting Job = job_1587346943308_0443, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0443/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0443

2020-04-22 17:22:34 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 17:22:38,146 Stage-4 map = 0%,  reduce = 0%

2020-04-22 17:22:39 2020-04-22 17:22:43,323 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.39 sec

2020-04-22 17:22:46 2020-04-22 17:22:50,551 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.94 sec

2020-04-22 17:22:47 MapReduce Total cumulative CPU time: 3 seconds 940 msec
Ended Job = job_1587346943308_0443

2020-04-22 17:22:48 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 11.96 sec   HDFS Read: 884386 HDFS Write: 888924 SUCCESS
Stage-Stage-6: Map: 1   Cumulative CPU: 3.85 sec   HDFS Read: 899152 HDFS Write: 98 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.94 sec   HDFS Read: 13143 HDFS Write: 87 SUCCESS
Total MapReduce CPU Time Spent: 19 seconds 750 msec
OK

2020-04-22 17:22:48 Time taken: 73.233 seconds

2020-04-22 17:22:48 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:22:48 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 17:22:48 Last login: Wed Apr 22 17:21:27 2020 from 10.10.30.233

2020-04-22 17:22:48 su hadoop

2020-04-22 17:22:48 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:22:48 [hadoop@hadoop-01 root]$ 
2020-04-22 17:22:48 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:22:48 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 17:22:48 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:22:49 2020-04-22 17:22:53,116 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 17:22:53,142 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:22:49 2020-04-22 17:22:53,227 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 17:22:53,231 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:22:49 2020-04-22 17:22:53,430 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 17:22:53,445 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 17:22:49 2020-04-22 17:22:53,451 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:22:50 注: /tmp/sqoop-hadoop/compile/25e1b0b9120bd20313970ebd57adfe32/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 17:22:54,420 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/25e1b0b9120bd20313970ebd57adfe32/user_info.jar
2020-04-22 17:22:54,427 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 17:22:54,427 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 17:22:54,427 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 17:22:54,427 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 17:22:54,427 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-22 17:22:50 2020-04-22 17:22:54,433 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 17:22:54,433 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 17:22:50 2020-04-22 17:22:54,536 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 17:22:51 2020-04-22 17:22:55,333 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 17:22:55,335 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 17:22:55,335 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:22:52 2020-04-22 17:22:55,767 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0444

2020-04-22 17:22:53 2020-04-22 17:22:57,013 INFO input.FileInputFormat: Total input files to process : 1
2020-04-22 17:22:57,015 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:22:53 2020-04-22 17:22:57,063 INFO mapreduce.JobSubmitter: number of splits:4
2020-04-22 17:22:57,091 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 17:22:53 2020-04-22 17:22:57,581 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0444
2020-04-22 17:22:57,583 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:22:54 2020-04-22 17:22:57,775 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:22:57,775 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:22:54 2020-04-22 17:22:57,822 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0444
2020-04-22 17:22:57,848 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0444/
2020-04-22 17:22:57,848 INFO mapreduce.Job: Running job: job_1587346943308_0444

2020-04-22 17:23:00 2020-04-22 17:23:03,935 INFO mapreduce.Job: Job job_1587346943308_0444 running in uber mode : false
2020-04-22 17:23:03,937 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 17:23:07 2020-04-22 17:23:11,121 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 17:23:11,130 INFO mapreduce.Job: Job job_1587346943308_0444 completed successfully

2020-04-22 17:23:07 2020-04-22 17:23:11,241 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=503452
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20711
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20711
		Total vcore-milliseconds taken by all map tasks=20711
		Total megabyte-milliseconds taken by all map tasks=21208064
	Map-Reduce Framework
		Map input records=11333
		Map output records=11333
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=828
		CPU time
2020-04-22 17:23:07  spent (ms)=3970
		Physical memory (bytes) snapshot=925540352
		Virtual memory (bytes) snapshot=9777258496
		Total committed heap usage (bytes)=742916096
		Peak Map Physical memory (bytes)=241979392
		Peak Map Virtual memory (bytes)=2447278080
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 17:23:11,248 INFO mapreduce.ExportJobBase: Transferred 491.6523 KB in 15.9034 seconds (30.9148 KB/sec)
2020-04-22 17:23:11,252 INFO mapreduce.ExportJobBase: Exported 11333 records.

2020-04-22 17:23:07 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:23:07 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 17:23:08 Last login: Wed Apr 22 17:22:52 2020 from 10.10.30.233

2020-04-22 17:23:08 su hadoop

2020-04-22 17:23:08 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:23:08 [hadoop@hadoop-01 root]$ 
2020-04-22 17:23:08 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:23:08 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 17:23:08 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:23:08 2020-04-22 17:23:12,524 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 17:23:12,550 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:23:08 2020-04-22 17:23:12,630 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 17:23:12,633 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:23:09 2020-04-22 17:23:12,838 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 17:23:09 2020-04-22 17:23:12,870 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 17:23:09 2020-04-22 17:23:12,875 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:23:10 注: /tmp/sqoop-hadoop/compile/ac557060784c18cece6029284587614c/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 17:23:13,851 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/ac557060784c18cece6029284587614c/user_info.jar
2020-04-22 17:23:13,858 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 17:23:13,859 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 17:23:13,859 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 17:23:13,859 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 17:23:13,859 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-22 17:23:10 2020-04-22 17:23:13,865 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 17:23:13,865 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 17:23:10 2020-04-22 17:23:13,970 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 17:23:11 2020-04-22 17:23:14,797 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 17:23:14,798 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 17:23:14,799 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:23:11 2020-04-22 17:23:15,210 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0445

2020-04-22 17:23:12 2020-04-22 17:23:16,476 INFO input.FileInputFormat: Total input files to process : 1
2020-04-22 17:23:16,479 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:23:12 2020-04-22 17:23:16,530 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-22 17:23:12 2020-04-22 17:23:16,569 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 17:23:12 2020-04-22 17:23:16,653 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0445
2020-04-22 17:23:16,655 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:23:13 2020-04-22 17:23:16,829 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:23:16,830 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:23:13 2020-04-22 17:23:16,877 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0445

2020-04-22 17:23:13 2020-04-22 17:23:16,903 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0445/
2020-04-22 17:23:16,903 INFO mapreduce.Job: Running job: job_1587346943308_0445

2020-04-22 17:23:19 2020-04-22 17:23:22,987 INFO mapreduce.Job: Job job_1587346943308_0445 running in uber mode : false
2020-04-22 17:23:22,989 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 17:23:26 2020-04-22 17:23:30,073 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-22 17:23:27 2020-04-22 17:23:31,088 INFO mapreduce.Job: Job job_1587346943308_0445 completed successfully

2020-04-22 17:23:27 2020-04-22 17:23:31,208 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=395458
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=21068
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=21068
		Total vcore-milliseconds taken by all map tasks=21068
		Total megabyte-milliseconds taken by all map tasks=21573632
	Map-Reduce Framework
		Map input records=4555
		Map output records=4555
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=770
		CPU time s
2020-04-22 17:23:27 pent (ms)=3850
		Physical memory (bytes) snapshot=949612544
		Virtual memory (bytes) snapshot=9783070720
		Total committed heap usage (bytes)=731906048
		Peak Map Physical memory (bytes)=241926144
		Peak Map Virtual memory (bytes)=2446938112
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 17:23:31,215 INFO mapreduce.ExportJobBase: Transferred 386.1895 KB in 16.4072 seconds (23.5378 KB/sec)

2020-04-22 17:23:27 2020-04-22 17:23:31,219 INFO mapreduce.ExportJobBase: Exported 4555 records.

2020-04-22 17:23:27 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:23:27 日统计数据已完成
2020-04-22 17:45:01 开始处理对日统计数据
2020-04-22 17:45:01 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \
--hive-table user_info \
--hive-overwrite
2020-04-22 17:45:01 Last login: Wed Apr 22 17:35:46 2020 from 10.10.30.233

2020-04-22 17:45:01 su hadoop

2020-04-22 17:45:01 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:45:01 [hadoop@hadoop-01 root]$ 
2020-04-22 17:45:01 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:45:01 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> --hive-table user_info \
> --hive-overwrite

2020-04-22 17:45:01 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:45:02 2020-04-22 17:45:06,005 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 17:45:02 2020-04-22 17:45:06,031 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:45:02 2020-04-22 17:45:06,080 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-22 17:45:06,088 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 17:45:06,090 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:45:02 2020-04-22 17:45:06,327 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 17:45:02 2020-04-22 17:45:06,332 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 17:45:02 2020-04-22 17:45:06,355 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:45:03 注: /tmp/sqoop-hadoop/compile/d817650f05c7fa161eba00afa4b4eceb/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 17:45:07,348 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/d817650f05c7fa161eba00afa4b4eceb/user_info.jar

2020-04-22 17:45:04 2020-04-22 17:45:08,070 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-22 17:45:08,075 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 17:45:08,076 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 17:45:08,080 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 17:45:08,082 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 17:45:08,091 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:45:04 2020-04-22 17:45:08,551 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0454

2020-04-22 17:45:08 2020-04-22 17:45:11,971 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 17:45:08 2020-04-22 17:45:12,030 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 17:45:08 2020-04-22 17:45:12,203 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0454
2020-04-22 17:45:12,204 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:45:08 2020-04-22 17:45:12,391 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:45:12,391 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:45:08 2020-04-22 17:45:12,449 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0454

2020-04-22 17:45:08 2020-04-22 17:45:12,477 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0454/
2020-04-22 17:45:12,477 INFO mapreduce.Job: Running job: job_1587346943308_0454

2020-04-22 17:45:13 2020-04-22 17:45:17,578 INFO mapreduce.Job: Job job_1587346943308_0454 running in uber mode : false
2020-04-22 17:45:17,580 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 17:45:19 2020-04-22 17:45:22,774 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 17:45:22,785 INFO mapreduce.Job: Job job_1587346943308_0454 completed successfully

2020-04-22 17:45:19 2020-04-22 17:45:22,898 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2729
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2729
		Total vcore-milliseconds taken by all map tasks=2729
		Total megabyte-milliseconds taken by all map tasks=2794496
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=53
		CPU time spen
2020-04-22 17:45:19 t (ms)=1330
		Physical memory (bytes) snapshot=243695616
		Virtual memory (bytes) snapshot=2447794176
		Total committed heap usage (bytes)=189267968
		Peak Map Physical memory (bytes)=243695616
		Peak Map Virtual memory (bytes)=2447794176
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572
2020-04-22 17:45:22,905 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 14.8058 seconds (40.7997 KB/sec)
2020-04-22 17:45:22,910 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-22 17:45:22,910 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info

2020-04-22 17:45:19 2020-04-22 17:45:22,920 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 17:45:19 2020-04-22 17:45:22,922 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 17:45:19 2020-04-22 17:45:22,943 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 17:45:22,950 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 17:45:19 2020-04-22 17:45:23,133 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 17:45:19 2020-04-22 17:45:23,708 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-22 17:45:23,733 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 17:45:20 2020-04-22 17:45:24,208 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 17:45:24,209 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 17:45:24,209 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 17:45:24,209 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 17:45:24,213 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 17:45:22 2020-04-22 17:45:26,252 INFO hive.HiveImport: Hive Session ID = 9a4793f6-1785-487e-a742-99005ae0a9c2

2020-04-22 17:45:22 2020-04-22 17:45:26,334 INFO hive.HiveImport: 
2020-04-22 17:45:26,334 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 17:45:29 2020-04-22 17:45:33,661 INFO hive.HiveImport: Hive Session ID = 396ec3ff-707d-435c-a13c-75fa0f15ca54

2020-04-22 17:45:31 2020-04-22 17:45:35,107 INFO hive.HiveImport: OK
2020-04-22 17:45:35,118 INFO hive.HiveImport: Time taken: 1.365 seconds

2020-04-22 17:45:31 2020-04-22 17:45:35,416 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 17:45:32 2020-04-22 17:45:35,973 INFO hive.HiveImport: OK
2020-04-22 17:45:35,985 INFO hive.HiveImport: Time taken: 0.844 seconds

2020-04-22 17:45:32 2020-04-22 17:45:36,471 INFO hive.HiveImport: Hive import complete.

2020-04-22 17:45:33 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:45:33 Last login: Wed Apr 22 17:45:05 2020 from 10.10.30.233

2020-04-22 17:45:33 su hadoop

2020-04-22 17:45:33 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:45:33 [hadoop@hadoop-01 root]$ 
2020-04-22 17:45:33 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:45:33 cat hive/statistic/RegionbyDay.hive

2020-04-22 17:45:33 
-- 按天统计专区信息
-- 进入数据库
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
-- 1、按天去重 日访问用户信息
insert overwrite table visit_user_page_day partition(y,m,d)  select param["parentColumnId"] as parent_column_id,user_type,user_id ,'' area_code,min(create_time) as create_time,y,m,d from events_type_log
where events_type in ('operationDetails','operationPage') and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param["parentColumnId"],user_type,user_id,y,m,d cluster by user_id,user_type;




--3、 按天去除重复播放用户
insert overwrite table play_user_day partition(y,m,d)  select param['parentColumnId'] as parent_column_id,user_type,user_id ,'' area_code,sum(1) as play_count,sum(cast(nvl(param['timePosition'],'0') as bigint)) as duration,y,m,d from even
2020-04-22 17:45:33 ts_type_log
where events_type='operateResumePoint' and param['operateType']='add' and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param['parentColumnId'],user_type,user_id,y,m,d cluster by user_id,user_type;



-- 创建临时存放统计数据的表；
-- 统计日页面访问用户数，统计日播放用户数，统计日播放次数
-- 统计页面访问用户数
truncate table
insert  overwrite table app_visit_count_day
select a.t as t_date,a.parent_column_id,a.user_type,sum(a.page_user_count) as page_user_count,sum(play_user_count) as play_user_count,sum(a.play_count) as play_count,sum(a.duration) as duration from (
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type, count(1) as page_user_count,0 as play_user_count,0 as duration,0 as play_count from visit_user_page_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
UNION ALL
select concat(y,'-',m,'
2020-04-22 17:45:33 -',d) as t,parent_column_id,user_type,0 as page_user_count,count(1) as play_user_count, sum(duration) as duration,sum(play_count) as play_count from play_user_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
) a group by a.t,a.parent_column_id,a.user_type;

-- 统计 播放次数
-- select sum(cast(nvl(data['timePosition'],'0') as bigint)) as duration,count(1) as play_count from events_type_log where y=2020 and m='04' and d='01' and data['operateType']='add';


[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:45:33 Last login: Wed Apr 22 17:45:37 2020 from 10.10.30.233

2020-04-22 17:45:33 su hadoop

2020-04-22 17:45:33 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:45:33 [hadoop@hadoop-01 root]$ 
2020-04-22 17:45:33 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:45:33 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 17:45:34 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 17:45:34 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 17:45:36 Hive Session ID = 1642cbf5-6dad-42ae-bc0f-ef4a0d1500ae

2020-04-22 17:45:36 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 17:45:43 Hive Session ID = 0bf87a66-ddb7-4dac-a955-1111fd841544

2020-04-22 17:45:44 OK
Time taken: 0.811 seconds

2020-04-22 17:45:45 Query ID = hadoop_20200422174547_6b0559e1-1ba5-4ade-b863-f863620e3ca2
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 17:45:45 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:45:47 Starting Job = job_1587346943308_0455, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0455/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0455

2020-04-22 17:45:53 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 17:45:53 2020-04-22 17:45:57,598 Stage-1 map = 0%,  reduce = 0%

2020-04-22 17:46:03 2020-04-22 17:46:06,976 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.08 sec

2020-04-22 17:46:05 2020-04-22 17:46:09,056 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 15.73 sec

2020-04-22 17:46:10 2020-04-22 17:46:14,249 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.11 sec

2020-04-22 17:46:11 MapReduce Total cumulative CPU time: 20 seconds 110 msec

2020-04-22 17:46:11 Ended Job = job_1587346943308_0455

2020-04-22 17:46:11 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:46:12 Starting Job = job_1587346943308_0456, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0456/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0456

2020-04-22 17:46:22 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2020-04-22 17:46:26,579 Stage-2 map = 0%,  reduce = 0%

2020-04-22 17:46:29 2020-04-22 17:46:32,790 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.7 sec

2020-04-22 17:46:35 2020-04-22 17:46:38,982 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.15 sec

2020-04-22 17:46:36 MapReduce Total cumulative CPU time: 7 seconds 150 msec
Ended Job = job_1587346943308_0456

2020-04-22 17:46:36 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 17:46:36 


2020-04-22 17:46:36 	 Time taken to load dynamic partitions: 0.308 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:46:37 Starting Job = job_1587346943308_0457, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0457/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0457

2020-04-22 17:46:47 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 17:46:47 2020-04-22 17:46:51,569 Stage-4 map = 0%,  reduce = 0%

2020-04-22 17:46:54 2020-04-22 17:46:57,770 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec

2020-04-22 17:47:01 2020-04-22 17:47:04,972 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.16 sec

2020-04-22 17:47:02 MapReduce Total cumulative CPU time: 4 seconds 160 msec
Ended Job = job_1587346943308_0457

2020-04-22 17:47:03 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 20.11 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.15 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.16 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 420 msec
OK
Time taken: 78.857 seconds

2020-04-22 17:47:03 Query ID = hadoop_20200422174706_9e142877-0dd8-4ca0-bdb4-f7779987971a
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:47:04 Starting Job = job_1587346943308_0458, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0458/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0458

2020-04-22 17:47:13 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2020-04-22 17:47:17,292 Stage-1 map = 0%,  reduce = 0%

2020-04-22 17:47:21 2020-04-22 17:47:25,552 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 5.01 sec

2020-04-22 17:47:23 2020-04-22 17:47:27,610 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.83 sec

2020-04-22 17:47:27 2020-04-22 17:47:31,733 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.77 sec

2020-04-22 17:47:29 MapReduce Total cumulative CPU time: 16 seconds 770 msec
Ended Job = job_1587346943308_0458

2020-04-22 17:47:29 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:47:30 Starting Job = job_1587346943308_0459, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0459/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0459

2020-04-22 17:47:39 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 17:47:39 2020-04-22 17:47:42,950 Stage-2 map = 0%,  reduce = 0%

2020-04-22 17:47:45 2020-04-22 17:47:49,174 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.52 sec

2020-04-22 17:47:53 2020-04-22 17:47:57,414 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.73 sec

2020-04-22 17:47:54 MapReduce Total cumulative CPU time: 7 seconds 730 msec
Ended Job = job_1587346943308_0459

2020-04-22 17:47:54 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 17:47:54 


2020-04-22 17:47:55 	 Time taken to load dynamic partitions: 0.331 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:47:56 Starting Job = job_1587346943308_0460, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0460/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0460

2020-04-22 17:48:05 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 17:48:05 2020-04-22 17:48:09,005 Stage-4 map = 0%,  reduce = 0%

2020-04-22 17:48:11 2020-04-22 17:48:15,201 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.53 sec

2020-04-22 17:48:16 2020-04-22 17:48:20,371 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.32 sec

2020-04-22 17:48:17 MapReduce Total cumulative CPU time: 4 seconds 320 msec
Ended Job = job_1587346943308_0460

2020-04-22 17:48:17 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 16.77 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.73 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.32 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 28 seconds 820 msec
OK
Time taken: 74.847 seconds

2020-04-22 17:48:17 NoViableAltException(158@[212:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:116)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4513)
	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:45148)
	at org.apache.hadoop.hive.ql.parse.HiveParser.tablePartitionPrefix(HiveParser.java:12566)
	at org.apache.hadoop.hive.ql.parse.HiveParser.truncateTableStatement(HiveParser.java:7057)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:4323)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2494)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1420)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtil
2020-04-22 17:48:17 s.java:74)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:335)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:471)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:487)
	at org.apache.hadoop.hive.cli.CliDr
2020-04-22 17:48:17 iver.executeDriver(CliDriver.java:793)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)

2020-04-22 17:48:17 FAILED: ParseException line 6:0 cannot recognize input near 'insert' 'overwrite' 'table' in table name

2020-04-22 17:48:18 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:48:18 Last login: Wed Apr 22 17:45:37 2020 from 10.10.30.233

2020-04-22 17:48:18 su hadoop

2020-04-22 17:48:18 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:48:18 [hadoop@hadoop-01 root]$ 
2020-04-22 17:48:18 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:48:18 cat hive/statistic/add_user_day.hive

2020-04-22 17:48:18 -- 2、天新增用户数据
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
insert overwrite table add_page_user_day partition(y,m,d)
select vud.parent_column_id,vud.user_type,vud.user_id,vud.area_code,vud.create_time,substr(vud.create_time,1,4) y,substr(vud.create_time,6,2) m,substr(vud.create_time,9,2) d   from(
  select parent_column_id,user_type,user_id,area_code,min(create_time) as create_time from visit_user_page_day
  where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by parent_column_id,user_type,user_id,area_code
) vud
 left join user_info aud on aud.user_id=vud.user_id and aud.user_type=vud.user_type and  aud.parent_column_id=vud.parent_column_id
 where aud.user_id is null;
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:48:18 Last login: Wed Apr 22 17:48:22 2020 from 10.10.30.233

2020-04-22 17:48:18 su hadoop

2020-04-22 17:48:18 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:48:19 [hadoop@hadoop-01 root]$ 
2020-04-22 17:48:19 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:48:19 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 17:48:19 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 17:48:20 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 17:48:22 Hive Session ID = 0068439e-76d0-4369-8757-8e7355c02eba

2020-04-22 17:48:22 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 17:48:28 Hive Session ID = 0f17b55e-d4ff-46d1-866d-127f5eebac98

2020-04-22 17:48:29 OK
Time taken: 0.816 seconds

2020-04-22 17:48:32 Query ID = hadoop_20200422174833_21f36803-8302-47c2-99eb-93fca3b3bf19
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 17:48:32 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:48:34 Starting Job = job_1587346943308_0461, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0461/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0461

2020-04-22 17:48:40 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1

2020-04-22 17:48:40 2020-04-22 17:48:44,226 Stage-1 map = 0%,  reduce = 0%

2020-04-22 17:48:47 2020-04-22 17:48:51,498 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.53 sec

2020-04-22 17:48:53 2020-04-22 17:48:57,711 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.09 sec

2020-04-22 17:48:54 MapReduce Total cumulative CPU time: 8 seconds 90 msec

2020-04-22 17:48:55 Ended Job = job_1587346943308_0461

2020-04-22 17:48:55 SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]

2020-04-22 17:49:00 2020-04-22 17:49:04	Starting to launch local task to process map join;	maximum memory = 239075328

2020-04-22 17:49:01 2020-04-22 17:49:05	Dump the side-table for tag: 1 with group count: 15888 into file: file:/tmp/hive/0068439e-76d0-4369-8757-8e7355c02eba/hive_2020-04-22_17-48-33_450_5463397242957734791-1/-local-10004/HashTable-Stage-6/MapJoin-mapfile01--.hashtable

2020-04-22 17:49:02 Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-22 17:49:03 Starting Job = job_1587346943308_0462, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0462/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0462

2020-04-22 17:49:09 Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 0
2020-04-22 17:49:12,917 Stage-6 map = 0%,  reduce = 0%

2020-04-22 17:49:16 2020-04-22 17:49:20,166 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 3.86 sec

2020-04-22 17:49:17 MapReduce Total cumulative CPU time: 3 seconds 860 msec
Ended Job = job_1587346943308_0462

2020-04-22 17:49:17 Loading data to table yn_hadoop.add_page_user_day partition (y=null, m=null, d=null)

2020-04-22 17:49:17 


2020-04-22 17:49:17 	 Time taken to load dynamic partitions: 0.027 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:49:18 Starting Job = job_1587346943308_0463, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0463/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0463

2020-04-22 17:49:28 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-22 17:49:32,468 Stage-4 map = 0%,  reduce = 0%

2020-04-22 17:49:34 2020-04-22 17:49:38,677 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.42 sec

2020-04-22 17:49:41 2020-04-22 17:49:44,875 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.81 sec

2020-04-22 17:49:42 MapReduce Total cumulative CPU time: 3 seconds 810 msec
Ended Job = job_1587346943308_0463

2020-04-22 17:49:42 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 8.09 sec   HDFS Read: 877540 HDFS Write: 888924 SUCCESS
Stage-Stage-6: Map: 1   Cumulative CPU: 3.86 sec   HDFS Read: 899152 HDFS Write: 98 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.81 sec   HDFS Read: 13143 HDFS Write: 87 SUCCESS
Total MapReduce CPU Time Spent: 15 seconds 760 msec
OK

2020-04-22 17:49:42 Time taken: 72.67 seconds

2020-04-22 17:49:42 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:49:42 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/reports/app/visit_count/day \
--table app_visit_count_day \
--columns t_date,parent_column_id,user_type,page_user_count,play_user_count,play_count,duration \
--update-key t_date,parent_column_id,user_type \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 17:49:43 Last login: Wed Apr 22 17:48:22 2020 from 10.10.30.233

2020-04-22 17:49:43 su hadoop

2020-04-22 17:49:43 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:49:43 [hadoop@hadoop-01 root]$ 
2020-04-22 17:49:43 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:49:43 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/reports/app/visit_count/day \
> --table app_visit_count_day \
> --columns t_date,parent_column_id,user_type,page_user_count,play_user_count,pl ay_count,duration \
> --update-key t_date,parent_column_id,user_type \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 17:49:43 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:49:43 2020-04-22 17:49:47,421 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 17:49:43 2020-04-22 17:49:47,449 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:49:43 2020-04-22 17:49:47,527 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 17:49:47,530 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:49:43 2020-04-22 17:49:47,728 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `app_visit_count_day` AS t LIMIT 1

2020-04-22 17:49:43 2020-04-22 17:49:47,744 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `app_visit_count_day` AS t LIMIT 1

2020-04-22 17:49:43 2020-04-22 17:49:47,749 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:49:44 注: /tmp/sqoop-hadoop/compile/c46c59e3f9631f8c9f664104d7a30b8b/app_visit_count_day.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 17:49:48,712 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/c46c59e3f9631f8c9f664104d7a30b8b/app_visit_count_day.jar
2020-04-22 17:49:48,720 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 17:49:48,720 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 17:49:48,720 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 17:49:48,720 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 17:49:48,720 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 17:49:48,726 INFO mapreduce.ExportJobBase: Beginning export of app_visit_count_day
2020-04-22 17:49:48,726 INFO Confi
2020-04-22 17:49:44 guration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 17:49:45 2020-04-22 17:49:48,829 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 17:49:45 2020-04-22 17:49:49,617 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 17:49:49,619 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 17:49:49,619 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:49:46 2020-04-22 17:49:50,021 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0464

2020-04-22 17:49:47 2020-04-22 17:49:51,379 INFO input.FileInputFormat: Total input files to process : 1
2020-04-22 17:49:51,381 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:49:47 2020-04-22 17:49:51,425 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-22 17:49:47 2020-04-22 17:49:51,453 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 17:49:47 2020-04-22 17:49:51,528 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0464
2020-04-22 17:49:51,530 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:49:47 2020-04-22 17:49:51,712 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:49:51,712 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:49:47 2020-04-22 17:49:51,758 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0464

2020-04-22 17:49:48 2020-04-22 17:49:51,783 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0464/
2020-04-22 17:49:51,784 INFO mapreduce.Job: Running job: job_1587346943308_0464

2020-04-22 17:49:54 2020-04-22 17:49:57,869 INFO mapreduce.Job: Job job_1587346943308_0464 running in uber mode : false
2020-04-22 17:49:57,871 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 17:50:02 2020-04-22 17:50:05,948 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 17:50:05,959 INFO mapreduce.Job: Job job_1587346943308_0464 completed successfully

2020-04-22 17:50:02 2020-04-22 17:50:06,076 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928672
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=947
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=23061
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=23061
		Total vcore-milliseconds taken by all map tasks=23061
		Total megabyte-milliseconds taken by all map tasks=23614464
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=681
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=839
		CPU time spent (ms)
2020-04-22 17:50:02 =3330
		Physical memory (bytes) snapshot=960344064
		Virtual memory (bytes) snapshot=9776197632
		Total committed heap usage (bytes)=743440384
		Peak Map Physical memory (bytes)=241750016
		Peak Map Virtual memory (bytes)=2445025280
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 17:50:06,083 INFO mapreduce.ExportJobBase: Transferred 947 bytes in 16.4555 seconds (57.5492 bytes/sec)
2020-04-22 17:50:06,088 INFO mapreduce.ExportJobBase: Exported 2 records.

2020-04-22 17:50:02 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:50:02 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 17:50:02 Last login: Wed Apr 22 17:49:46 2020 from 10.10.30.233

2020-04-22 17:50:02 su hadoop

2020-04-22 17:50:02 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:50:02 [hadoop@hadoop-01 root]$ 
2020-04-22 17:50:02 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:50:02 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 17:50:03 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:50:03 2020-04-22 17:50:07,330 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 17:50:07,355 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:50:03 2020-04-22 17:50:07,436 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 17:50:07,439 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:50:03 2020-04-22 17:50:07,643 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 17:50:07,658 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 17:50:03 2020-04-22 17:50:07,663 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:50:04 注: /tmp/sqoop-hadoop/compile/95c586ff5902247dd2753bf69150860a/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 17:50:08,621 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/95c586ff5902247dd2753bf69150860a/user_info.jar
2020-04-22 17:50:08,628 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 17:50:08,628 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 17:50:08,628 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 17:50:08,628 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 17:50:08,628 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 17:50:08,634 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 17:50:08,634 INFO Configuration.deprecation: mapred.j
2020-04-22 17:50:04 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 17:50:04 2020-04-22 17:50:08,740 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 17:50:05 2020-04-22 17:50:09,549 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 17:50:09,550 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 17:50:09,551 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:50:06 2020-04-22 17:50:10,002 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0465

2020-04-22 17:50:07 2020-04-22 17:50:11,688 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:50:07 2020-04-22 17:50:11,690 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:50:07 2020-04-22 17:50:11,739 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-22 17:50:07 2020-04-22 17:50:11,766 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 17:50:08 2020-04-22 17:50:11,844 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0465
2020-04-22 17:50:11,846 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:50:08 2020-04-22 17:50:12,039 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:50:12,039 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:50:08 2020-04-22 17:50:12,103 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0465

2020-04-22 17:50:08 2020-04-22 17:50:12,138 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0465/
2020-04-22 17:50:12,139 INFO mapreduce.Job: Running job: job_1587346943308_0465

2020-04-22 17:50:14 2020-04-22 17:50:18,232 INFO mapreduce.Job: Job job_1587346943308_0465 running in uber mode : false
2020-04-22 17:50:18,234 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 17:50:21 2020-04-22 17:50:25,314 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-22 17:50:22 2020-04-22 17:50:26,330 INFO mapreduce.Job: Job job_1587346943308_0465 completed successfully

2020-04-22 17:50:22 2020-04-22 17:50:26,445 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=503452
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=21394
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=21394
		Total vcore-milliseconds taken by all map tasks=21394
		Total megabyte-milliseconds taken by all map tasks=21907456
	Map-Reduce Framework
		Map input records=11333
		Map output records=11333
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=766
		CPU time
2020-04-22 17:50:22  spent (ms)=4180
		Physical memory (bytes) snapshot=974077952
		Virtual memory (bytes) snapshot=9789521920
		Total committed heap usage (bytes)=731906048
		Peak Map Physical memory (bytes)=255217664
		Peak Map Virtual memory (bytes)=2454110208
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 17:50:26,452 INFO mapreduce.ExportJobBase: Transferred 491.6523 KB in 16.8923 seconds (29.1052 KB/sec)
2020-04-22 17:50:26,456 INFO mapreduce.ExportJobBase: Exported 11333 records.

2020-04-22 17:50:23 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:50:23 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 17:50:23 Last login: Wed Apr 22 17:50:06 2020 from 10.10.30.233

2020-04-22 17:50:23 su hadoop

2020-04-22 17:50:23 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:50:23 [hadoop@hadoop-01 root]$ 
2020-04-22 17:50:23 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:50:23 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 17:50:23 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:50:23 2020-04-22 17:50:27,694 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 17:50:27,721 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:50:24 2020-04-22 17:50:27,798 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 17:50:27,802 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:50:24 2020-04-22 17:50:28,003 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 17:50:24 2020-04-22 17:50:28,027 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 17:50:28,035 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:50:25 注: /tmp/sqoop-hadoop/compile/55800364470a482cc0735b2e046dd815/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 17:50:29,044 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/55800364470a482cc0735b2e046dd815/user_info.jar
2020-04-22 17:50:29,050 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 17:50:29,050 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 17:50:29,050 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 17:50:29,050 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 17:50:29,050 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 17:50:29,056 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 17:50:29,056 INFO Configuration.deprecation: mapred.j
2020-04-22 17:50:25 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 17:50:25 2020-04-22 17:50:29,167 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 17:50:26 2020-04-22 17:50:29,957 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 17:50:29,958 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 17:50:29,959 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:50:26 2020-04-22 17:50:30,372 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0466

2020-04-22 17:50:27 2020-04-22 17:50:31,203 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:50:27 2020-04-22 17:50:31,205 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:50:27 2020-04-22 17:50:31,251 INFO mapreduce.JobSubmitter: number of splits:4
2020-04-22 17:50:31,280 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 17:50:28 2020-04-22 17:50:31,754 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0466
2020-04-22 17:50:31,756 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:50:28 2020-04-22 17:50:31,983 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:50:31,983 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:50:28 2020-04-22 17:50:32,034 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0466

2020-04-22 17:50:28 2020-04-22 17:50:32,059 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0466/
2020-04-22 17:50:32,060 INFO mapreduce.Job: Running job: job_1587346943308_0466

2020-04-22 17:51:03 开始处理对日统计数据
2020-04-22 17:51:03 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \
--hive-table user_info \
--hive-overwrite
2020-04-22 17:51:03 Last login: Wed Apr 22 17:50:27 2020 from 10.10.30.233

2020-04-22 17:51:03 su hadoop

2020-04-22 17:51:04 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:51:04 [hadoop@hadoop-01 root]$ 
2020-04-22 17:51:04 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:51:04 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> --hive-table user_info \
> --hive-overwrite

2020-04-22 17:51:04 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:51:04 2020-04-22 17:51:08,380 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-22 17:51:04 2020-04-22 17:51:08,412 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:51:04 2020-04-22 17:51:08,458 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-22 17:51:08,466 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-22 17:51:08,468 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:51:04 2020-04-22 17:51:08,747 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 17:51:08,751 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 17:51:04 2020-04-22 17:51:08,767 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:51:06 注: /tmp/sqoop-hadoop/compile/c82bce82da915b99b2f651fe9fa5712b/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 17:51:09,832 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/c82bce82da915b99b2f651fe9fa5712b/user_info.jar

2020-04-22 17:51:06 2020-04-22 17:51:10,624 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-22 17:51:10,634 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-22 17:51:10,635 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-22 17:51:10,643 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-22 17:51:10,646 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 17:51:10,661 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:51:07 2020-04-22 17:51:11,163 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0467

2020-04-22 17:51:09 2020-04-22 17:51:12,847 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-22 17:51:09 2020-04-22 17:51:12,886 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-22 17:51:09 2020-04-22 17:51:13,462 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0467
2020-04-22 17:51:13,465 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:51:09 2020-04-22 17:51:13,706 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:51:13,706 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:51:09 2020-04-22 17:51:13,759 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0467

2020-04-22 17:51:09 2020-04-22 17:51:13,784 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0467/
2020-04-22 17:51:13,785 INFO mapreduce.Job: Running job: job_1587346943308_0467

2020-04-22 17:51:16 2020-04-22 17:51:19,864 INFO mapreduce.Job: Job job_1587346943308_0467 running in uber mode : false
2020-04-22 17:51:19,866 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 17:51:21 2020-04-22 17:51:24,931 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 17:51:24,942 INFO mapreduce.Job: Job job_1587346943308_0467 completed successfully

2020-04-22 17:51:21 2020-04-22 17:51:25,060 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2848
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2848
		Total vcore-milliseconds taken by all map tasks=2848
		Total megabyte-milliseconds taken by all map tasks=2916352
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=68
		CPU time spen
2020-04-22 17:51:21 t (ms)=1190
		Physical memory (bytes) snapshot=249298944
		Virtual memory (bytes) snapshot=2448744448
		Total committed heap usage (bytes)=190316544
		Peak Map Physical memory (bytes)=249298944
		Peak Map Virtual memory (bytes)=2448744448
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572
2020-04-22 17:51:25,067 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 14.3944 seconds (41.9658 KB/sec)
2020-04-22 17:51:25,071 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-22 17:51:25,071 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info
2020-04-22 17:51:25,084 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-22 17:51:25,086 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-22 17:51:21 2020-04-22 17:51:25,108 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-22 17:51:25,116 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-22 17:51:21 2020-04-22 17:51:25,287 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 17:51:22 2020-04-22 17:51:25,823 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)
2020-04-22 17:51:25,847 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-22 17:51:22 2020-04-22 17:51:26,311 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-22 17:51:26,312 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 17:51:26,312 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-22 17:51:26,312 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-22 17:51:26,323 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 17:51:24 2020-04-22 17:51:28,433 INFO hive.HiveImport: Hive Session ID = 9710c47e-1092-45dc-a1d3-dbe97c169849

2020-04-22 17:51:24 2020-04-22 17:51:28,485 INFO hive.HiveImport: 
2020-04-22 17:51:28,485 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 17:51:32 2020-04-22 17:51:35,974 INFO hive.HiveImport: Hive Session ID = 39eba06e-f4d7-4e3d-b194-f1eff0746feb

2020-04-22 17:51:33 2020-04-22 17:51:37,454 INFO hive.HiveImport: OK

2020-04-22 17:51:33 2020-04-22 17:51:37,465 INFO hive.HiveImport: Time taken: 1.419 seconds

2020-04-22 17:51:34 2020-04-22 17:51:37,744 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-22 17:51:34 2020-04-22 17:51:38,308 INFO hive.HiveImport: OK

2020-04-22 17:51:34 2020-04-22 17:51:38,326 INFO hive.HiveImport: Time taken: 0.832 seconds

2020-04-22 17:51:35 2020-04-22 17:51:38,824 INFO hive.HiveImport: Hive import complete.

2020-04-22 17:51:35 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:51:35 Last login: Wed Apr 22 17:51:07 2020 from 10.10.30.233

2020-04-22 17:51:35 su hadoop

2020-04-22 17:51:35 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:51:35 [hadoop@hadoop-01 root]$ 
2020-04-22 17:51:35 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:51:35 cat hive/statistic/RegionbyDay.hive

-- 按天统计专区信息
-- 进入数据库
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
-- 1、按天去重 日访问用户信息
insert overwrite table visit_user_page_day partition(y,m,d)  select param["parentColumnId"] as parent_column_id,user_type,user_id ,'' area_code,min(create_time) as create_time,y,m,d from events_type_log
where events_type in ('operationDetails','operationPage') and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param["parentColumnId"],user_type,user_id,y,m,d cluster by user_id,user_type;




--3、 按天去除重复播放用户
insert overwrite table play_user_day partition(y,m,d)  select param['parentColumnId'] as parent_column_id,user_type,user_id ,'' area_code,sum(1) as play_count,sum(cast(nvl(param['timePosition'],'0') as
2020-04-22 17:51:35  bigint)) as duration,y,m,d from events_type_log
where events_type='operateResumePoint' and param['operateType']='add' and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param['parentColumnId'],user_type,user_id,y,m,d cluster by user_id,user_type;



-- 创建临时存放统计数据的表；
-- 统计日页面访问用户数，统计日播放用户数，统计日播放次数
-- 统计页面访问用户数
truncate table app_visit_count_day;
insert  overwrite table app_visit_count_day
select a.t as t_date,a.parent_column_id,a.user_type,sum(a.page_user_count) as page_user_count,sum(play_user_count) as play_user_count,sum(a.play_count) as play_count,sum(a.duration) as duration from (
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type, count(1) as page_user_count,0 as play_user_count,0 as duration,0 as play_count from visit_user_page_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent
2020-04-22 17:51:35 _column_id,user_type
UNION ALL
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type,0 as page_user_count,count(1) as play_user_count, sum(duration) as duration,sum(play_count) as play_count from play_user_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
) a group by a.t,a.parent_column_id,a.user_type;

-- 统计 播放次数
-- select sum(cast(nvl(data['timePosition'],'0') as bigint)) as duration,count(1) as play_count from events_type_log where y=2020 and m='04' and d='01' and data['operateType']='add';


[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:51:35 Last login: Wed Apr 22 17:51:39 2020 from 10.10.30.233

2020-04-22 17:51:35 su hadoop

2020-04-22 17:51:36 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:51:36 [hadoop@hadoop-01 root]$ 
2020-04-22 17:51:36 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:51:36 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/RegionbyDay.hive

2020-04-22 17:51:36 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 17:51:37 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 17:51:39 Hive Session ID = c5aa0a5c-7afb-4f90-b6e2-627d7ff13959

2020-04-22 17:51:39 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 17:51:46 Hive Session ID = 2ffecedc-2ff7-46d3-8321-fe06bc405c25

2020-04-22 17:51:46 OK
Time taken: 0.827 seconds

2020-04-22 17:51:48 Query ID = hadoop_20200422175150_d2cae7c4-c030-4a4c-9c80-39599e831bc3
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 17:51:48 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:51:51 Starting Job = job_1587346943308_0468, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0468/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0468

2020-04-22 17:51:56 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 17:51:56 2020-04-22 17:52:00,328 Stage-1 map = 0%,  reduce = 0%

2020-04-22 17:52:06 2020-04-22 17:52:10,761 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 7.2 sec

2020-04-22 17:52:08 2020-04-22 17:52:11,801 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.69 sec

2020-04-22 17:52:12 2020-04-22 17:52:15,952 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.84 sec

2020-04-22 17:52:13 MapReduce Total cumulative CPU time: 20 seconds 840 msec
Ended Job = job_1587346943308_0468

2020-04-22 17:52:13 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:52:13 Starting Job = job_1587346943308_0469, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0469/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0469

2020-04-22 17:52:25 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2020-04-22 17:52:28,835 Stage-2 map = 0%,  reduce = 0%

2020-04-22 17:52:31 2020-04-22 17:52:35,040 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.68 sec

2020-04-22 17:52:38 2020-04-22 17:52:42,295 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.21 sec

2020-04-22 17:52:39 MapReduce Total cumulative CPU time: 7 seconds 210 msec

2020-04-22 17:52:39 Ended Job = job_1587346943308_0469

2020-04-22 17:52:39 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-22 17:52:39 


2020-04-22 17:52:40 	 Time taken to load dynamic partitions: 0.369 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:52:41 Starting Job = job_1587346943308_0470, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0470/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0470

2020-04-22 17:52:51 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 17:52:51 2020-04-22 17:52:55,021 Stage-4 map = 0%,  reduce = 0%

2020-04-22 17:52:56 2020-04-22 17:53:00,183 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.47 sec

2020-04-22 17:53:02 2020-04-22 17:53:06,370 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.16 sec

2020-04-22 17:53:03 MapReduce Total cumulative CPU time: 4 seconds 160 msec
Ended Job = job_1587346943308_0470

2020-04-22 17:53:04 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 20.84 sec   HDFS Read: 193048983 HDFS Write: 1465764 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.21 sec   HDFS Read: 1476290 HDFS Write: 869126 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.16 sec   HDFS Read: 16679 HDFS Write: 4721 SUCCESS
Total MapReduce CPU Time Spent: 32 seconds 210 msec
OK
Time taken: 77.467 seconds

2020-04-22 17:53:04 Query ID = hadoop_20200422175308_ddd07f46-a841-409a-9235-420b5ce37fb1
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 17:53:04 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:53:05 Starting Job = job_1587346943308_0471, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0471/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0471

2020-04-22 17:53:14 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 17:53:14 2020-04-22 17:53:18,324 Stage-1 map = 0%,  reduce = 0%

2020-04-22 17:53:22 2020-04-22 17:53:26,592 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 5.24 sec

2020-04-22 17:53:24 2020-04-22 17:53:28,660 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.67 sec

2020-04-22 17:53:31 2020-04-22 17:53:34,828 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.96 sec

2020-04-22 17:53:32 MapReduce Total cumulative CPU time: 16 seconds 960 msec
Ended Job = job_1587346943308_0471

2020-04-22 17:53:32 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:53:32 Starting Job = job_1587346943308_0472, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0472/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0472

2020-04-22 17:53:42 Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1

2020-04-22 17:53:42 2020-04-22 17:53:46,625 Stage-2 map = 0%,  reduce = 0%

2020-04-22 17:53:49 2020-04-22 17:53:52,835 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec

2020-04-22 17:53:56 2020-04-22 17:54:00,041 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.31 sec

2020-04-22 17:53:57 MapReduce Total cumulative CPU time: 8 seconds 310 msec
Ended Job = job_1587346943308_0472

2020-04-22 17:53:57 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-22 17:53:57 


2020-04-22 17:53:57 	 Time taken to load dynamic partitions: 0.262 seconds
	 Time taken for adding to write entity : 0.001 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:53:57 Starting Job = job_1587346943308_0473, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0473/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0473

2020-04-22 17:54:08 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 17:54:08 2020-04-22 17:54:11,902 Stage-4 map = 0%,  reduce = 0%

2020-04-22 17:54:14 2020-04-22 17:54:18,078 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.49 sec

2020-04-22 17:54:19 2020-04-22 17:54:23,243 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.15 sec

2020-04-22 17:54:20 MapReduce Total cumulative CPU time: 4 seconds 150 msec
Ended Job = job_1587346943308_0473

2020-04-22 17:54:20 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 16.96 sec   HDFS Read: 84990507 HDFS Write: 881951 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 8.31 sec   HDFS Read: 893603 HDFS Write: 474338 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.15 sec   HDFS Read: 19222 HDFS Write: 6134 SUCCESS
Total MapReduce CPU Time Spent: 29 seconds 420 msec
OK
Time taken: 76.302 seconds

2020-04-22 17:54:20 OK
Time taken: 0.113 seconds

2020-04-22 17:54:23 Query ID = hadoop_20200422175424_06c27d38-93fb-4b94-b077-397b0d302615
Total jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:54:23 Starting Job = job_1587346943308_0474, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0474/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0474

2020-04-22 17:54:31 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2020-04-22 17:54:35,490 Stage-1 map = 0%,  reduce = 0%

2020-04-22 17:54:40 2020-04-22 17:54:43,756 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.8 sec

2020-04-22 17:54:46 2020-04-22 17:54:49,947 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.68 sec

2020-04-22 17:54:47 MapReduce Total cumulative CPU time: 11 seconds 680 msec
Ended Job = job_1587346943308_0474

2020-04-22 17:54:47 Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:54:47 Starting Job = job_1587346943308_0475, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0475/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0475

2020-04-22 17:54:57 Hadoop job information for Stage-5: number of mappers: 2; number of reducers: 1

2020-04-22 17:54:57 2020-04-22 17:55:01,674 Stage-5 map = 0%,  reduce = 0%

2020-04-22 17:55:06 2020-04-22 17:55:09,923 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 7.05 sec

2020-04-22 17:55:12 2020-04-22 17:55:16,104 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 10.77 sec

2020-04-22 17:55:14 MapReduce Total cumulative CPU time: 10 seconds 770 msec
Ended Job = job_1587346943308_0475
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:55:14 Starting Job = job_1587346943308_0476, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0476/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0476

2020-04-22 17:55:25 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2020-04-22 17:55:28,911 Stage-2 map = 0%,  reduce = 0%

2020-04-22 17:55:31 2020-04-22 17:55:35,108 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.46 sec

2020-04-22 17:55:37 2020-04-22 17:55:41,318 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.96 sec

2020-04-22 17:55:38 MapReduce Total cumulative CPU time: 6 seconds 960 msec

2020-04-22 17:55:38 Ended Job = job_1587346943308_0476

2020-04-22 17:55:38 Loading data to table yn_hadoop.app_visit_count_day

2020-04-22 17:55:38 Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:55:39 Starting Job = job_1587346943308_0477, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0477/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0477

2020-04-22 17:55:49 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 17:55:49 2020-04-22 17:55:53,173 Stage-4 map = 0%,  reduce = 0%

2020-04-22 17:55:54 2020-04-22 17:55:58,353 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.37 sec

2020-04-22 17:56:00 2020-04-22 17:56:04,543 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.9 sec

2020-04-22 17:56:02 MapReduce Total cumulative CPU time: 3 seconds 900 msec
Ended Job = job_1587346943308_0477

2020-04-22 17:56:03 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 11.68 sec   HDFS Read: 883908 HDFS Write: 182 SUCCESS
Stage-Stage-5: Map: 2  Reduce: 1   Cumulative CPU: 10.77 sec   HDFS Read: 489429 HDFS Write: 196 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 6.96 sec   HDFS Read: 19853 HDFS Write: 522 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.9 sec   HDFS Read: 13367 HDFS Write: 399 SUCCESS
Total MapReduce CPU Time Spent: 33 seconds 310 msec
OK
Time taken: 102.236 seconds

2020-04-22 17:56:03 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:56:03 Last login: Wed Apr 22 17:51:39 2020 from 10.10.30.233

2020-04-22 17:56:03 su hadoop

2020-04-22 17:56:03 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:56:03 [hadoop@hadoop-01 root]$ 
2020-04-22 17:56:03 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:56:03 cat hive/statistic/add_user_day.hive

2020-04-22 17:56:03 -- 2、天新增用户数据
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
insert overwrite table add_page_user_day partition(y,m,d)
select vud.parent_column_id,vud.user_type,vud.user_id,vud.area_code,vud.create_time,substr(vud.create_time,1,4) y,substr(vud.create_time,6,2) m,substr(vud.create_time,9,2) d   from(
  select parent_column_id,user_type,user_id,area_code,min(create_time) as create_time from visit_user_page_day
  where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by parent_column_id,user_type,user_id,area_code
) vud
 left join user_info aud on aud.user_id=vud.user_id and aud.user_type=vud.user_type and  aud.parent_column_id=vud.parent_column_id
 where aud.user_id is null;
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:56:04 Last login: Wed Apr 22 17:56:07 2020 from 10.10.30.233

2020-04-22 17:56:04 su hadoop

2020-04-22 17:56:04 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:56:04 [hadoop@hadoop-01 root]$ 
2020-04-22 17:56:04 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:56:04 hive -d startDate=2020-03-23 -d endDate=202 0-03-25 -f hive/statistic/add_user_day.hive

2020-04-22 17:56:04 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-22 17:56:05 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-22 17:56:05 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-22 17:56:07 Hive Session ID = 404c5ed2-bdd9-424f-86b5-cd84e1ec732a

2020-04-22 17:56:07 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-22 17:56:14 Hive Session ID = 15940721-c292-4aec-93cf-b57da760defa

2020-04-22 17:56:14 OK
Time taken: 0.798 seconds

2020-04-22 17:56:17 Query ID = hadoop_20200422175618_048007eb-5a19-4d61-b986-4a374deeafba
Total jobs = 3
Launching Job 1 out of 3

2020-04-22 17:56:17 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:56:19 Starting Job = job_1587346943308_0478, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0478/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0478

2020-04-22 17:56:26 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-22 17:56:26 2020-04-22 17:56:29,962 Stage-1 map = 0%,  reduce = 0%

2020-04-22 17:56:33 2020-04-22 17:56:37,315 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 8.16 sec

2020-04-22 17:56:40 2020-04-22 17:56:44,574 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.11 sec

2020-04-22 17:56:42 MapReduce Total cumulative CPU time: 12 seconds 110 msec

2020-04-22 17:56:42 Ended Job = job_1587346943308_0478

2020-04-22 17:56:49 2020-04-22 17:56:53	Dump the side-table for tag: 1 with group count: 15888 into file: file:/tmp/hive/404c5ed2-bdd9-424f-86b5-cd84e1ec732a/hive_2020-04-22_17-56-18_742_8632477285193831592-1/-local-10004/HashTable-Stage-6/MapJoin-mapfile01--.hashtable

2020-04-22 17:56:49 2020-04-22 17:56:53	Uploaded 1 File to: file:/tmp/hive/404c5ed2-bdd9-424f-86b5-cd84e1ec732a/hive_2020-04-22_17-56-18_742_8632477285193831592-1/-local-10004/HashTable-Stage-6/MapJoin-mapfile01--.hashtable (557990 bytes)
2020-04-22 17:56:53	End of local task; Time Taken: 1.075 sec.

2020-04-22 17:56:50 Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-22 17:56:51 Starting Job = job_1587346943308_0479, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0479/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0479

2020-04-22 17:56:57 Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 0
2020-04-22 17:57:01,216 Stage-6 map = 0%,  reduce = 0%

2020-04-22 17:57:03 2020-04-22 17:57:07,420 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 3.7 sec

2020-04-22 17:57:04 MapReduce Total cumulative CPU time: 3 seconds 700 msec

2020-04-22 17:57:04 Ended Job = job_1587346943308_0479

2020-04-22 17:57:04 Loading data to table yn_hadoop.add_page_user_day partition (y=null, m=null, d=null)

2020-04-22 17:57:04 


2020-04-22 17:57:04 	 Time taken to load dynamic partitions: 0.03 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3

2020-04-22 17:57:04 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-22 17:57:05 Starting Job = job_1587346943308_0480, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0480/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0480

2020-04-22 17:57:16 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-22 17:57:16 2020-04-22 17:57:20,326 Stage-4 map = 0%,  reduce = 0%

2020-04-22 17:57:22 2020-04-22 17:57:26,538 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec

2020-04-22 17:57:28 2020-04-22 17:57:32,747 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.43 sec

2020-04-22 17:57:29 MapReduce Total cumulative CPU time: 4 seconds 430 msec

2020-04-22 17:57:30 Ended Job = job_1587346943308_0480

2020-04-22 17:57:30 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 12.11 sec   HDFS Read: 884386 HDFS Write: 888924 SUCCESS
Stage-Stage-6: Map: 1   Cumulative CPU: 3.7 sec   HDFS Read: 899152 HDFS Write: 98 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.43 sec   HDFS Read: 13143 HDFS Write: 87 SUCCESS
Total MapReduce CPU Time Spent: 20 seconds 240 msec
OK
Time taken: 75.246 seconds

2020-04-22 17:57:30 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:57:30 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/reports/app/visit_count/day \
--table app_visit_count_day \
--columns t_date,parent_column_id,user_type,page_user_count,play_user_count,play_count,duration \
--update-key t_date,parent_column_id,user_type \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 17:57:30 Last login: Wed Apr 22 17:56:08 2020 from 10.10.30.233

2020-04-22 17:57:30 su hadoop

2020-04-22 17:57:30 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:57:30 [hadoop@hadoop-01 root]$ 
2020-04-22 17:57:30 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:57:30 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/reports/app/visit_count/day \
> --table app_visit_count_day \
> --columns t_date,parent_column_id,user_type,page_user_count,play_user_count,pl ay_count,duration \
> --update-key t_date,parent_column_id,user_type \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 17:57:31 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:57:31 2020-04-22 17:57:35,326 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 17:57:35,351 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:57:31 2020-04-22 17:57:35,436 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.

2020-04-22 17:57:31 2020-04-22 17:57:35,439 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:57:31 2020-04-22 17:57:35,640 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `app_visit_count_day` AS t LIMIT 1
2020-04-22 17:57:35,656 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `app_visit_count_day` AS t LIMIT 1

2020-04-22 17:57:31 2020-04-22 17:57:35,662 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:57:32 注: /tmp/sqoop-hadoop/compile/974d4f8c15e43db99502a36feb29fb19/app_visit_count_day.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 17:57:36,640 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/974d4f8c15e43db99502a36feb29fb19/app_visit_count_day.jar
2020-04-22 17:57:36,647 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 17:57:36,647 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 17:57:36,647 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 17:57:36,647 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 17:57:36,647 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 17:57:36,653 INFO mapreduce.ExportJobBase: Beginning export of app_visit_count_day
2020-04-22 17:57:36,653 INFO Confi
2020-04-22 17:57:32 guration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 17:57:33 2020-04-22 17:57:36,756 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 17:57:33 2020-04-22 17:57:37,512 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 17:57:37,514 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 17:57:37,514 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:57:34 2020-04-22 17:57:37,943 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0481

2020-04-22 17:57:36 2020-04-22 17:57:40,077 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:57:36 2020-04-22 17:57:40,081 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:57:36 2020-04-22 17:57:40,141 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-22 17:57:36 2020-04-22 17:57:40,171 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 17:57:36 2020-04-22 17:57:40,662 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0481
2020-04-22 17:57:40,664 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:57:37 2020-04-22 17:57:40,861 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:57:40,861 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:57:37 2020-04-22 17:57:40,907 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0481

2020-04-22 17:57:37 2020-04-22 17:57:40,933 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0481/
2020-04-22 17:57:40,933 INFO mapreduce.Job: Running job: job_1587346943308_0481

2020-04-22 17:57:43 2020-04-22 17:57:47,021 INFO mapreduce.Job: Job job_1587346943308_0481 running in uber mode : false
2020-04-22 17:57:47,023 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 17:57:50 2020-04-22 17:57:54,101 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 17:57:54,112 INFO mapreduce.Job: Job job_1587346943308_0481 completed successfully

2020-04-22 17:57:50 2020-04-22 17:57:54,225 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928672
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=947
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20675
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20675
		Total vcore-milliseconds taken by all map tasks=20675
		Total megabyte-milliseconds taken by all map tasks=21171200
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=681
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=735
		CPU time spent (ms)
2020-04-22 17:57:50 =3160
		Physical memory (bytes) snapshot=958656512
		Virtual memory (bytes) snapshot=9773993984
		Total committed heap usage (bytes)=756547584
		Peak Map Physical memory (bytes)=243552256
		Peak Map Virtual memory (bytes)=2445684736
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 17:57:54,233 INFO mapreduce.ExportJobBase: Transferred 947 bytes in 16.709 seconds (56.6759 bytes/sec)
2020-04-22 17:57:54,237 INFO mapreduce.ExportJobBase: Exported 2 records.

2020-04-22 17:57:50 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:57:50 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 17:57:51 Last login: Wed Apr 22 17:57:34 2020 from 10.10.30.233

2020-04-22 17:57:51 su hadoop

2020-04-22 17:57:51 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:57:51 [hadoop@hadoop-01 root]$ 
2020-04-22 17:57:51 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:57:51 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 17:57:51 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:57:51 2020-04-22 17:57:55,512 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 17:57:55,541 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:57:51 2020-04-22 17:57:55,616 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-22 17:57:55,619 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:57:52 2020-04-22 17:57:55,818 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 17:57:55,833 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-22 17:57:55,838 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:57:53 注: /tmp/sqoop-hadoop/compile/2267127199bfadbd8613a4cd989600e8/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。

2020-04-22 17:57:53 2020-04-22 17:57:56,822 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/2267127199bfadbd8613a4cd989600e8/user_info.jar
2020-04-22 17:57:56,829 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 17:57:56,829 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 17:57:56,829 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 17:57:56,829 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 17:57:56,829 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-22 17:57:53 2020-04-22 17:57:56,835 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 17:57:56,835 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 17:57:53 2020-04-22 17:57:56,933 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 17:57:53 2020-04-22 17:57:57,709 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 17:57:57,711 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 17:57:57,711 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:57:54 2020-04-22 17:57:58,131 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0482

2020-04-22 17:57:55 2020-04-22 17:57:59,389 INFO input.FileInputFormat: Total input files to process : 1
2020-04-22 17:57:59,391 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:57:55 2020-04-22 17:57:59,440 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-22 17:57:55 2020-04-22 17:57:59,470 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 17:57:56 2020-04-22 17:57:59,965 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0482
2020-04-22 17:57:59,967 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:57:56 2020-04-22 17:58:00,166 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:58:00,167 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:57:56 2020-04-22 17:58:00,219 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0482

2020-04-22 17:57:56 2020-04-22 17:58:00,244 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0482/
2020-04-22 17:58:00,245 INFO mapreduce.Job: Running job: job_1587346943308_0482

2020-04-22 17:58:02 2020-04-22 17:58:06,328 INFO mapreduce.Job: Job job_1587346943308_0482 running in uber mode : false
2020-04-22 17:58:06,330 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 17:58:09 2020-04-22 17:58:13,407 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 17:58:13,417 INFO mapreduce.Job: Job job_1587346943308_0482 completed successfully

2020-04-22 17:58:09 2020-04-22 17:58:13,529 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=503452
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=21080
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=21080
		Total vcore-milliseconds taken by all map tasks=21080
		Total megabyte-milliseconds taken by all map tasks=21585920
	Map-Reduce Framework
		Map input records=11333
		Map output records=11333
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=804
		CPU time
2020-04-22 17:58:09  spent (ms)=3890
		Physical memory (bytes) snapshot=936173568
		Virtual memory (bytes) snapshot=9781690368
		Total committed heap usage (bytes)=734003200
		Peak Map Physical memory (bytes)=242212864
		Peak Map Virtual memory (bytes)=2447220736
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 17:58:13,536 INFO mapreduce.ExportJobBase: Transferred 491.6523 KB in 15.8164 seconds (31.0849 KB/sec)
2020-04-22 17:58:13,540 INFO mapreduce.ExportJobBase: Exported 11333 records.

2020-04-22 17:58:10 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:58:10 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-22 17:58:10 Last login: Wed Apr 22 17:57:54 2020 from 10.10.30.233

2020-04-22 17:58:10 su hadoop

2020-04-22 17:58:10 [root@hadoop-01 ~]# su hadoop

2020-04-22 17:58:10 [hadoop@hadoop-01 root]$ 
2020-04-22 17:58:10 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:58:10 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-22 17:58:10 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-22 17:58:11 2020-04-22 17:58:14,788 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-22 17:58:14,813 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-22 17:58:11 2020-04-22 17:58:14,895 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.

2020-04-22 17:58:11 2020-04-22 17:58:14,899 INFO tool.CodeGenTool: Beginning code generation

2020-04-22 17:58:11 2020-04-22 17:58:15,108 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 17:58:11 2020-04-22 17:58:15,141 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-22 17:58:11 2020-04-22 17:58:15,149 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-22 17:58:12 注: /tmp/sqoop-hadoop/compile/5cd20c4787b62bd1af8bd223be9a9ccc/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-22 17:58:16,186 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/5cd20c4787b62bd1af8bd223be9a9ccc/user_info.jar
2020-04-22 17:58:16,193 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-22 17:58:16,193 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-22 17:58:16,193 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-22 17:58:16,193 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-22 17:58:16,193 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-22 17:58:16,199 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-22 17:58:16,199 INFO Configuration.deprecation: mapred.j
2020-04-22 17:58:12 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-22 17:58:12 2020-04-22 17:58:16,302 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-22 17:58:13 2020-04-22 17:58:17,062 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-22 17:58:17,063 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-22 17:58:17,064 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-22 17:58:13 2020-04-22 17:58:17,528 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0483

2020-04-22 17:58:15 2020-04-22 17:58:19,663 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:58:15 2020-04-22 17:58:19,665 INFO input.FileInputFormat: Total input files to process : 1

2020-04-22 17:58:15 2020-04-22 17:58:19,711 INFO mapreduce.JobSubmitter: number of splits:4
2020-04-22 17:58:19,740 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-22 17:58:16 2020-04-22 17:58:20,215 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0483
2020-04-22 17:58:20,217 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-22 17:58:16 2020-04-22 17:58:20,418 INFO conf.Configuration: resource-types.xml not found
2020-04-22 17:58:20,418 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-22 17:58:16 2020-04-22 17:58:20,469 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0483

2020-04-22 17:58:16 2020-04-22 17:58:20,498 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0483/

2020-04-22 17:58:16 2020-04-22 17:58:20,499 INFO mapreduce.Job: Running job: job_1587346943308_0483

2020-04-22 17:58:21 2020-04-22 17:58:25,602 INFO mapreduce.Job: Job job_1587346943308_0483 running in uber mode : false
2020-04-22 17:58:25,604 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-22 17:58:30 2020-04-22 17:58:33,770 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-22 17:58:33,779 INFO mapreduce.Job: Job job_1587346943308_0483 completed successfully

2020-04-22 17:58:30 2020-04-22 17:58:33,892 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=395458
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20573
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20573
		Total vcore-milliseconds taken by all map tasks=20573
		Total megabyte-milliseconds taken by all map tasks=21066752
	Map-Reduce Framework
		Map input records=4555
		Map output records=4555
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=895
		CPU time s
2020-04-22 17:58:30 pent (ms)=3730
		Physical memory (bytes) snapshot=941195264
		Virtual memory (bytes) snapshot=9778278400
		Total committed heap usage (bytes)=746586112
		Peak Map Physical memory (bytes)=242622464
		Peak Map Virtual memory (bytes)=2445336576
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-22 17:58:33,899 INFO mapreduce.ExportJobBase: Transferred 386.1895 KB in 16.8263 seconds (22.9515 KB/sec)
2020-04-22 17:58:33,903 INFO mapreduce.ExportJobBase: Exported 4555 records.

2020-04-22 17:58:30 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-22 17:58:30 日统计数据已完成
