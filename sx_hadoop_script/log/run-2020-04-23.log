2020-04-23 09:16:36 开始备份hive待处理日志数据...
2020-04-23 09:16:37 Last login: Thu Apr 23 09:12:27 2020 from 10.10.30.233

2020-04-23 09:16:37 su hadoop

2020-04-23 09:16:37 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:16:37 [hadoop@hadoop-01 root]$ 
2020-04-23 09:16:37 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:16:37 hadoop fs -rm -R -f hdfs://192.168.15.21:90 00/data/hive/backup/

2020-04-23 09:16:38 2020-04-23 09:16:42,602 INFO fs.TrashPolicyDefault: Moved: 'hdfs://192.168.15.21:9000/data/hive/backup' to trash at: hdfs://192.168.15.21:9000/user/hadoop/.Trash/Current/data/hive/backup

2020-04-23 09:16:39 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:16:39 2020
 3 2020-04-23 09:16:39 23
2020-04-23 09:16:39 Last login: Thu Apr 23 09:16:41 2020 from 10.10.30.233

2020-04-23 09:16:39 su hadoop

2020-04-23 09:16:39 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:16:39 [hadoop@hadoop-01 root]$ 
2020-04-23 09:16:39 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:16:39 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-23

2020-04-23 09:16:41 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:16:41 Last login: Thu Apr 23 09:16:43 2020 from 10.10.30.233

2020-04-23 09:16:41 su hadoop

2020-04-23 09:16:41 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:16:41 [hadoop@hadoop-01 root]$ 
2020-04-23 09:16:41 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:16:41 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/23/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-23

2020-04-23 09:16:48 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:16:48 Last login: Thu Apr 23 09:16:45 2020 from 10.10.30.233

2020-04-23 09:16:48 su hadoop

2020-04-23 09:16:48 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:16:48 [hadoop@hadoop-01 root]$ 
2020-04-23 09:16:48 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:16:48 cat hive/loadLogs.hive
use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs 
2020-04-23 09:16:48 PARTITION (y='${year}',m='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:16:48 Last login: Thu Apr 23 09:16:52 2020 from 10.10.30.233

2020-04-23 09:16:48 su hadoop

2020-04-23 09:16:49 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:16:49 [hadoop@hadoop-01 root]$ 
2020-04-23 09:16:49 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:16:49 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-23 -d day=23 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:16:49 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:16:50 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:16:52 Hive Session ID = e9039ac9-2223-45d5-ae38-99a856c7214f

Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:16:58 Hive Session ID = a718096c-f821-4a60-b4ae-b88abeebe8d2

2020-04-23 09:16:59 OK
Time taken: 0.795 seconds

2020-04-23 09:17:00 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=23)

2020-04-23 09:17:04 OK
Time taken: 5.168 seconds

2020-04-23 09:17:05 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:05 备份hive待处理日志数据已完成。
2020-04-23 09:17:05 2020
 3 2020-04-23 09:17:05 24
2020-04-23 09:17:05 Last login: Thu Apr 23 09:16:53 2020 from 10.10.30.233

2020-04-23 09:17:05 su hadoop

2020-04-23 09:17:05 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:17:05 [hadoop@hadoop-01 root]$ 
2020-04-23 09:17:05 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:05 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-24

2020-04-23 09:17:07 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:07 Last login: Thu Apr 23 09:17:09 2020 from 10.10.30.233

2020-04-23 09:17:07 su hadoop

2020-04-23 09:17:07 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:17:07 [hadoop@hadoop-01 root]$ 
2020-04-23 09:17:07 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:07 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/24/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-24

2020-04-23 09:17:14 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:15 Last login: Thu Apr 23 09:17:11 2020 from 10.10.30.233

2020-04-23 09:17:15 su hadoop

2020-04-23 09:17:15 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:17:15 [hadoop@hadoop-01 root]$ 
2020-04-23 09:17:15 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:15 cat hive/loadLogs.hive
use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs 
2020-04-23 09:17:15 PARTITION (y='${year}',m='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:15 Last login: Thu Apr 23 09:17:19 2020 from 10.10.30.233

2020-04-23 09:17:15 su hadoop

2020-04-23 09:17:15 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:17:15 [hadoop@hadoop-01 root]$ 
2020-04-23 09:17:15 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:15 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-24 -d day=24 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:17:16 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:17:16 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:17:18 Hive Session ID = 39f8d8f0-7572-42d8-9ca3-bc47ab549b71

2020-04-23 09:17:18 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:17:25 Hive Session ID = cdf01c84-08d0-425a-896f-eb51de529840

2020-04-23 09:17:26 OK
Time taken: 0.815 seconds

2020-04-23 09:17:27 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=24)

2020-04-23 09:17:31 OK

2020-04-23 09:17:31 Time taken: 5.175 seconds

2020-04-23 09:17:32 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:32 备份hive待处理日志数据已完成。
2020-04-23 09:17:32 2020
 3 2020-04-23 09:17:32 25
2020-04-23 09:17:32 Last login: Thu Apr 23 09:17:19 2020 from 10.10.30.233

2020-04-23 09:17:32 su hadoop

2020-04-23 09:17:32 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:17:32 [hadoop@hadoop-01 root]$ 
2020-04-23 09:17:32 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:32 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-25

2020-04-23 09:17:34 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:34 Last login: Thu Apr 23 09:17:36 2020 from 10.10.30.233

2020-04-23 09:17:34 su hadoop

2020-04-23 09:17:34 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:17:34 [hadoop@hadoop-01 root]$ 
2020-04-23 09:17:34 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:34 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/25/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-25

2020-04-23 09:17:41 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:41 Last login: Thu Apr 23 09:17:38 2020 from 10.10.30.233

2020-04-23 09:17:41 su hadoop

2020-04-23 09:17:41 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:17:41 [hadoop@hadoop-01 root]$ 
2020-04-23 09:17:41 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:41 cat hive/loadLogs.hive

2020-04-23 09:17:41 use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs PARTITION (y='${year}',m
2020-04-23 09:17:41 ='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:41 Last login: Thu Apr 23 09:17:45 2020 from 10.10.30.233

2020-04-23 09:17:41 su hadoop

2020-04-23 09:17:41 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:17:41 [hadoop@hadoop-01 root]$ 
2020-04-23 09:17:41 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:41 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-25 -d day=25 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:17:42 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:17:42 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:17:44 Hive Session ID = 40421768-3858-40f6-88a9-2e2151adb02e

2020-04-23 09:17:44 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:17:51 Hive Session ID = b82309fb-6927-49ff-a7b4-9d012e056ed8

2020-04-23 09:17:52 OK
Time taken: 0.808 seconds

2020-04-23 09:17:53 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=25)

2020-04-23 09:17:57 OK
Time taken: 5.144 seconds

2020-04-23 09:17:58 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:58 备份hive待处理日志数据已完成。
2020-04-23 09:17:58 2020
 3 2020-04-23 09:17:58 26
2020-04-23 09:17:58 Last login: Thu Apr 23 09:17:51 2020 from 10.10.30.233

2020-04-23 09:17:58 su hadoop

2020-04-23 09:17:58 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:17:58 [hadoop@hadoop-01 root]$ 
2020-04-23 09:17:58 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:17:58 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-26

2020-04-23 09:18:00 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:00 Last login: Thu Apr 23 09:18:02 2020 from 10.10.30.233

2020-04-23 09:18:00 su hadoop

2020-04-23 09:18:00 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:18:00 [hadoop@hadoop-01 root]$ 
2020-04-23 09:18:00 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:00 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/26/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-26

2020-04-23 09:18:12 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:12 Last login: Thu Apr 23 09:18:04 2020 from 10.10.30.233

2020-04-23 09:18:12 su hadoop

2020-04-23 09:18:12 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:18:12 [hadoop@hadoop-01 root]$ 
2020-04-23 09:18:12 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:12 cat hive/loadLogs.hive
use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs 
2020-04-23 09:18:12 PARTITION (y='${year}',m='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:13 Last login: Thu Apr 23 09:18:16 2020 from 10.10.30.233

2020-04-23 09:18:13 su hadoop

2020-04-23 09:18:13 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:18:13 [hadoop@hadoop-01 root]$ 
2020-04-23 09:18:13 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:13 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-26 -d day=26 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:18:13 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:18:14 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:18:16 Hive Session ID = 7cf3d9c7-1ecb-4b05-8d62-d577bc2cfcff

2020-04-23 09:18:16 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:18:23 Hive Session ID = ccdb85ab-f84e-445a-b11d-8d910df8ec4a

2020-04-23 09:18:24 OK

2020-04-23 09:18:24 Time taken: 0.811 seconds

2020-04-23 09:18:25 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=26)

2020-04-23 09:18:29 OK
Time taken: 5.675 seconds

2020-04-23 09:18:30 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:30 备份hive待处理日志数据已完成。
2020-04-23 09:18:30 2020
 3 2020-04-23 09:18:30 27
2020-04-23 09:18:30 Last login: Thu Apr 23 09:18:17 2020 from 10.10.30.233

2020-04-23 09:18:30 su hadoop

2020-04-23 09:18:30 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:18:30 [hadoop@hadoop-01 root]$ 
2020-04-23 09:18:30 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:30 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-27

2020-04-23 09:18:32 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:33 Last login: Thu Apr 23 09:18:34 2020 from 10.10.30.233

2020-04-23 09:18:33 su hadoop

2020-04-23 09:18:33 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:18:33 [hadoop@hadoop-01 root]$ 
2020-04-23 09:18:33 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:33 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/27/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-27

2020-04-23 09:18:41 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:41 Last login: Thu Apr 23 09:18:37 2020 from 10.10.30.233

2020-04-23 09:18:41 su hadoop

2020-04-23 09:18:41 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:18:41 [hadoop@hadoop-01 root]$ 
2020-04-23 09:18:41 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:41 cat hive/loadLogs.hive

2020-04-23 09:18:41 use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs PARTITION (y='${year}',m
2020-04-23 09:18:41 ='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:42 Last login: Thu Apr 23 09:18:45 2020 from 10.10.30.233

2020-04-23 09:18:42 su hadoop

2020-04-23 09:18:42 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:18:42 [hadoop@hadoop-01 root]$ 
2020-04-23 09:18:42 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:42 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-27 -d day=27 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:18:42 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:18:43 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:18:45 Hive Session ID = 5433293b-90d2-4b5e-bdba-f5044c3728da

2020-04-23 09:18:45 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:18:52 Hive Session ID = d82fa43a-39cd-4d2a-8897-9cd1a53267e2

2020-04-23 09:18:52 OK
Time taken: 0.802 seconds

2020-04-23 09:18:54 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=27)

2020-04-23 09:18:57 OK

2020-04-23 09:18:57 Time taken: 4.323 seconds

2020-04-23 09:18:57 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:57 备份hive待处理日志数据已完成。
2020-04-23 09:18:57 2020
 3 2020-04-23 09:18:57 28
2020-04-23 09:18:58 Last login: Thu Apr 23 09:18:46 2020 from 10.10.30.233

2020-04-23 09:18:58 su hadoop

2020-04-23 09:18:58 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:18:58 [hadoop@hadoop-01 root]$ 
2020-04-23 09:18:58 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:18:58 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-28

2020-04-23 09:18:59 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:00 Last login: Thu Apr 23 09:19:02 2020 from 10.10.30.233

2020-04-23 09:19:00 su hadoop

2020-04-23 09:19:00 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:19:00 [hadoop@hadoop-01 root]$ 
2020-04-23 09:19:00 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:00 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/28/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-28

2020-04-23 09:19:10 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:10 Last login: Thu Apr 23 09:19:04 2020 from 10.10.30.233

2020-04-23 09:19:10 su hadoop

2020-04-23 09:19:10 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:19:10 [hadoop@hadoop-01 root]$ 
2020-04-23 09:19:10 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:10 cat hive/loadLogs.hive
use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs 
2020-04-23 09:19:10 PARTITION (y='${year}',m='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:11 Last login: Thu Apr 23 09:19:14 2020 from 10.10.30.233

2020-04-23 09:19:11 su hadoop

2020-04-23 09:19:11 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:19:11 [hadoop@hadoop-01 root]$ 
2020-04-23 09:19:11 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:11 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-28 -d day=28 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:19:11 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:19:12 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:19:14 Hive Session ID = 9d06bfb9-f1e2-49b3-b1c0-606f2872c9ad

Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:19:20 Hive Session ID = a2457092-aa43-48e4-8c05-780321e949b7

2020-04-23 09:19:21 OK
Time taken: 0.806 seconds

2020-04-23 09:19:23 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=28)

2020-04-23 09:19:25 OK
Time taken: 3.854 seconds

2020-04-23 09:19:26 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:26 备份hive待处理日志数据已完成。
2020-04-23 09:19:26 2020
 3 2020-04-23 09:19:26 29
2020-04-23 09:19:26 Last login: Thu Apr 23 09:19:15 2020 from 10.10.30.233

2020-04-23 09:19:26 su hadoop

2020-04-23 09:19:26 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:19:26 [hadoop@hadoop-01 root]$ 
2020-04-23 09:19:26 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:26 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-29

2020-04-23 09:19:28 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:28 Last login: Thu Apr 23 09:19:30 2020 from 10.10.30.233

2020-04-23 09:19:28 su hadoop

2020-04-23 09:19:28 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:19:28 [hadoop@hadoop-01 root]$ 
2020-04-23 09:19:28 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:28 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/29/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-29

2020-04-23 09:19:39 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:40 Last login: Thu Apr 23 09:19:32 2020 from 10.10.30.233

2020-04-23 09:19:40 su hadoop

2020-04-23 09:19:40 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:19:40 [hadoop@hadoop-01 root]$ 
2020-04-23 09:19:40 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:40 cat hive/loadLogs.hive

2020-04-23 09:19:40 use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs PARTITION (y='${year}',m
2020-04-23 09:19:40 ='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:40 Last login: Thu Apr 23 09:19:44 2020 from 10.10.30.233

2020-04-23 09:19:40 su hadoop

2020-04-23 09:19:40 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:19:40 [hadoop@hadoop-01 root]$ 
2020-04-23 09:19:40 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:40 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-29 -d day=29 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:19:41 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:19:41 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:19:43 Hive Session ID = 56670fef-8cdd-4761-80fe-3008a5eed38f

2020-04-23 09:19:43 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:19:50 Hive Session ID = ae3750b1-d20e-49bc-b843-f9d5f4b0d97b

2020-04-23 09:19:51 OK
Time taken: 0.854 seconds

2020-04-23 09:19:52 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=29)

2020-04-23 09:19:55 OK
Time taken: 4.248 seconds

2020-04-23 09:19:56 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:56 备份hive待处理日志数据已完成。
2020-04-23 09:19:56 2020
 3 2020-04-23 09:19:56 30
2020-04-23 09:19:56 Last login: Thu Apr 23 09:19:44 2020 from 10.10.30.233

2020-04-23 09:19:56 su hadoop

2020-04-23 09:19:56 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:19:56 [hadoop@hadoop-01 root]$ 
2020-04-23 09:19:56 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:56 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-30

2020-04-23 09:19:58 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:58 Last login: Thu Apr 23 09:20:00 2020 from 10.10.30.233

2020-04-23 09:19:58 su hadoop

2020-04-23 09:19:58 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:19:58 [hadoop@hadoop-01 root]$ 
2020-04-23 09:19:58 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:19:58 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/30/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-30

2020-04-23 09:20:09 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:10 Last login: Thu Apr 23 09:20:02 2020 from 10.10.30.233

2020-04-23 09:20:10 su hadoop

2020-04-23 09:20:10 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:20:10 [hadoop@hadoop-01 root]$ 
2020-04-23 09:20:10 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:10 cat hive/loadLogs.hive

2020-04-23 09:20:10 use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs PARTITION (y='${year}',m
2020-04-23 09:20:10 ='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:10 Last login: Thu Apr 23 09:20:14 2020 from 10.10.30.233

2020-04-23 09:20:10 su hadoop

2020-04-23 09:20:10 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:20:10 [hadoop@hadoop-01 root]$ 
2020-04-23 09:20:10 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:10 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-30 -d day=30 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:20:11 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:20:11 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:20:13 Hive Session ID = f86263f5-e42e-4b65-b470-3c06fca9cab5

2020-04-23 09:20:13 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:20:20 Hive Session ID = 33a56cbf-eaec-4da3-ab29-2e53405ed92c

2020-04-23 09:20:21 OK
Time taken: 0.875 seconds

2020-04-23 09:20:22 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=30)

2020-04-23 09:20:24 OK
Time taken: 3.684 seconds

2020-04-23 09:20:25 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:25 备份hive待处理日志数据已完成。
2020-04-23 09:20:25 2020
 3 2020-04-23 09:20:25 31
2020-04-23 09:20:25 Last login: Thu Apr 23 09:20:14 2020 from 10.10.30.233

2020-04-23 09:20:25 su hadoop

2020-04-23 09:20:25 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:20:25 [hadoop@hadoop-01 root]$ 
2020-04-23 09:20:25 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:25 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-03-31

2020-04-23 09:20:27 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:27 Last login: Thu Apr 23 09:20:29 2020 from 10.10.30.233

2020-04-23 09:20:27 su hadoop

2020-04-23 09:20:27 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:20:27 [hadoop@hadoop-01 root]$ 
2020-04-23 09:20:27 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:27 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/03/31/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-03-31

2020-04-23 09:20:38 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:38 Last login: Thu Apr 23 09:20:31 2020 from 10.10.30.233

2020-04-23 09:20:38 su hadoop

2020-04-23 09:20:38 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:20:38 [hadoop@hadoop-01 root]$ 
2020-04-23 09:20:38 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:38 cat hive/loadLogs.hive

2020-04-23 09:20:38 use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs PARTITION (y='${year}',m
2020-04-23 09:20:38 ='${month}',d='${day}');
2020-04-23 09:20:38 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:39 Last login: Thu Apr 23 09:20:42 2020 from 10.10.30.233

2020-04-23 09:20:39 su hadoop

2020-04-23 09:20:39 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:20:39 [hadoop@hadoop-01 root]$ 
2020-04-23 09:20:39 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:39 hive -d month=03 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-03-31 -d day=31 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:20:39 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:20:40 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:20:42 Hive Session ID = 73090f6f-26c1-49a7-ab66-752d0cf8cdad

2020-04-23 09:20:42 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:20:48 Hive Session ID = 7cecdca0-53d0-4e9d-aeab-c5f3a61257e0

2020-04-23 09:20:49 OK
Time taken: 0.797 seconds

2020-04-23 09:20:50 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=03, d=31)

2020-04-23 09:20:54 OK

2020-04-23 09:20:54 Time taken: 4.623 seconds

2020-04-23 09:20:55 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:55 备份hive待处理日志数据已完成。
2020-04-23 09:20:55 2020
 4 12020-04-23 09:20:55 Last login: Thu Apr 23 09:20:43 2020 from 10.10.30.233

2020-04-23 09:20:55 su hadoop

2020-04-23 09:20:55 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:20:55 [hadoop@hadoop-01 root]$ 
2020-04-23 09:20:55 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:55 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-04-01

2020-04-23 09:20:57 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:57 Last login: Thu Apr 23 09:20:59 2020 from 10.10.30.233

2020-04-23 09:20:57 su hadoop

2020-04-23 09:20:57 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:20:57 [hadoop@hadoop-01 root]$ 
2020-04-23 09:20:57 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:20:57 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/04/01/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-04-01

2020-04-23 09:21:08 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:08 Last login: Thu Apr 23 09:21:01 2020 from 10.10.30.233

2020-04-23 09:21:08 su hadoop

2020-04-23 09:21:08 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:21:08 [hadoop@hadoop-01 root]$ 
2020-04-23 09:21:08 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:08 cat hive/loadLogs.hive

2020-04-23 09:21:08 use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs PARTITION (y='${year}',m
2020-04-23 09:21:08 ='${month}',d='${day}');
2020-04-23 09:21:08 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:08 Last login: Thu Apr 23 09:21:12 2020 from 10.10.30.233

2020-04-23 09:21:08 su hadoop

2020-04-23 09:21:08 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:21:08 [hadoop@hadoop-01 root]$ 
2020-04-23 09:21:08 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:08 hive -d month=04 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-04-01 -d day=01 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:21:09 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:21:09 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-23 09:21:09 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:21:12 Hive Session ID = d035fb55-6d54-4e6c-8abe-7a89d74d8d7f

Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:21:18 Hive Session ID = 8f64b990-2c1b-4fcb-8a6b-56ae421a77cf

2020-04-23 09:21:19 OK
Time taken: 0.83 seconds

2020-04-23 09:21:20 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=04, d=01)

2020-04-23 09:21:23 OK
Time taken: 3.813 seconds

2020-04-23 09:21:23 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:24 备份hive待处理日志数据已完成。
2020-04-23 09:21:24 2020
 4 22020-04-23 09:21:24 Last login: Thu Apr 23 09:21:12 2020 from 10.10.30.233

2020-04-23 09:21:24 su hadoop

2020-04-23 09:21:24 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:21:24 [hadoop@hadoop-01 root]$ 
2020-04-23 09:21:24 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:24 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-04-02

2020-04-23 09:21:25 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:26 Last login: Thu Apr 23 09:21:28 2020 from 10.10.30.233

2020-04-23 09:21:26 su hadoop

2020-04-23 09:21:26 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:21:26 [hadoop@hadoop-01 root]$ 
2020-04-23 09:21:26 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:26 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/04/02/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-04-02

2020-04-23 09:21:36 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:36 Last login: Thu Apr 23 09:21:30 2020 from 10.10.30.233

2020-04-23 09:21:36 su hadoop

2020-04-23 09:21:36 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:21:36 [hadoop@hadoop-01 root]$ 
2020-04-23 09:21:36 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:36 cat hive/loadLogs.hive
use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs 
2020-04-23 09:21:36 PARTITION (y='${year}',m='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:36 Last login: Thu Apr 23 09:21:40 2020 from 10.10.30.233

2020-04-23 09:21:36 su hadoop

2020-04-23 09:21:36 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:21:36 [hadoop@hadoop-01 root]$ 
2020-04-23 09:21:36 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:36 hive -d month=04 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-04-02 -d day=02 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:21:37 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:21:37 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:21:40 Hive Session ID = c2a3a3b6-7cf8-404d-a4fe-e104f3630b3e

2020-04-23 09:21:40 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:21:46 Hive Session ID = 948aee5d-0c53-4c6f-bab2-2a697cb06900

2020-04-23 09:21:47 OK
Time taken: 0.794 seconds

2020-04-23 09:21:48 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=04, d=02)

2020-04-23 09:21:52 OK
Time taken: 5.019 seconds

2020-04-23 09:21:53 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:53 备份hive待处理日志数据已完成。
2020-04-23 09:21:53 2020
 4 32020-04-23 09:21:53 Last login: Thu Apr 23 09:21:40 2020 from 10.10.30.233

2020-04-23 09:21:53 su hadoop

2020-04-23 09:21:53 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:21:53 [hadoop@hadoop-01 root]$ 
2020-04-23 09:21:53 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:53 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-04-03

2020-04-23 09:21:55 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:55 Last login: Thu Apr 23 09:21:57 2020 from 10.10.30.233

2020-04-23 09:21:55 su hadoop

2020-04-23 09:21:55 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:21:55 [hadoop@hadoop-01 root]$ 
2020-04-23 09:21:55 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:21:55 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/04/03/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-04-03

2020-04-23 09:22:06 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:06 Last login: Thu Apr 23 09:21:59 2020 from 10.10.30.233

2020-04-23 09:22:06 su hadoop

2020-04-23 09:22:06 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:22:06 [hadoop@hadoop-01 root]$ 
2020-04-23 09:22:06 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:06 cat hive/loadLogs.hive
use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs 
2020-04-23 09:22:06 PARTITION (y='${year}',m='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:06 Last login: Thu Apr 23 09:22:10 2020 from 10.10.30.233

2020-04-23 09:22:06 su hadoop

2020-04-23 09:22:06 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:22:06 [hadoop@hadoop-01 root]$ 
2020-04-23 09:22:06 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:06 hive -d month=04 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-04-03 -d day=03 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:22:07 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:22:07 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:22:09 Hive Session ID = 9f0d6774-76f9-48f8-b15b-d9d350109687

2020-04-23 09:22:09 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:22:16 Hive Session ID = bf764cc2-736a-4381-ab3f-f7d2ac408e00

2020-04-23 09:22:17 OK
Time taken: 0.84 seconds

2020-04-23 09:22:18 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=04, d=03)

2020-04-23 09:22:21 OK
Time taken: 4.056 seconds

2020-04-23 09:22:22 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:22 备份hive待处理日志数据已完成。
2020-04-23 09:22:22 2020
 4 42020-04-23 09:22:22 Last login: Thu Apr 23 09:22:10 2020 from 10.10.30.233

2020-04-23 09:22:22 su hadoop

2020-04-23 09:22:22 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:22:22 [hadoop@hadoop-01 root]$ 
2020-04-23 09:22:22 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:22 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-04-04

2020-04-23 09:22:24 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:24 Last login: Thu Apr 23 09:22:26 2020 from 10.10.30.233

2020-04-23 09:22:24 su hadoop

2020-04-23 09:22:24 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:22:24 [hadoop@hadoop-01 root]$ 
2020-04-23 09:22:24 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:24 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/04/04/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-04-04

2020-04-23 09:22:37 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:37 Last login: Thu Apr 23 09:22:28 2020 from 10.10.30.233

2020-04-23 09:22:37 su hadoop

2020-04-23 09:22:38 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:22:38 [hadoop@hadoop-01 root]$ 
2020-04-23 09:22:38 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:38 cat hive/loadLogs.hive

2020-04-23 09:22:38 use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs PARTITION (y='${year}',m
2020-04-23 09:22:38 ='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:38 Last login: Thu Apr 23 09:22:42 2020 from 10.10.30.233

2020-04-23 09:22:38 su hadoop

2020-04-23 09:22:38 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:22:38 [hadoop@hadoop-01 root]$ 
2020-04-23 09:22:38 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:38 hive -d month=04 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-04-04 -d day=04 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:22:39 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:22:39 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:22:41 Hive Session ID = bc7df552-c824-4c6b-a0c2-5255ce1ae330

2020-04-23 09:22:41 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:22:48 Hive Session ID = a05572d2-be78-4a8b-85b0-48fe11ccec81

2020-04-23 09:22:49 OK
Time taken: 0.956 seconds

2020-04-23 09:22:50 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=04, d=04)

2020-04-23 09:22:54 OK
Time taken: 5.29 seconds

2020-04-23 09:22:55 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:55 备份hive待处理日志数据已完成。
2020-04-23 09:22:55 2020
 4 52020-04-23 09:22:55 Last login: Thu Apr 23 09:22:42 2020 from 10.10.30.233

2020-04-23 09:22:55 su hadoop

2020-04-23 09:22:55 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:22:55 [hadoop@hadoop-01 root]$ 
2020-04-23 09:22:55 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:55 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-04-05

2020-04-23 09:22:57 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:57 Last login: Thu Apr 23 09:22:59 2020 from 10.10.30.233

2020-04-23 09:22:57 su hadoop

2020-04-23 09:22:57 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:22:57 [hadoop@hadoop-01 root]$ 
2020-04-23 09:22:57 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:22:57 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/04/05/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-04-05

2020-04-23 09:23:10 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:10 Last login: Thu Apr 23 09:23:01 2020 from 10.10.30.233

2020-04-23 09:23:10 su hadoop

2020-04-23 09:23:10 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:23:10 [hadoop@hadoop-01 root]$ 
2020-04-23 09:23:10 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:10 cat hive/loadLogs.hive
use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs 
2020-04-23 09:23:10 PARTITION (y='${year}',m='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:10 Last login: Thu Apr 23 09:23:14 2020 from 10.10.30.233

2020-04-23 09:23:10 su hadoop

2020-04-23 09:23:10 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:23:11 [hadoop@hadoop-01 root]$ 
2020-04-23 09:23:11 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:11 hive -d month=04 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-04-05 -d day=05 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:23:11 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:23:12 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-23 09:23:12 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:23:14 Hive Session ID = 04a05a97-aa4d-4535-b828-327798b08ca3

2020-04-23 09:23:14 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:23:20 Hive Session ID = 535b3454-92a2-4e5f-93a2-43cdac98659b

2020-04-23 09:23:21 OK
Time taken: 0.828 seconds

2020-04-23 09:23:22 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=04, d=05)

2020-04-23 09:23:25 OK
Time taken: 3.999 seconds

2020-04-23 09:23:26 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:26 备份hive待处理日志数据已完成。
2020-04-23 09:23:26 2020
 4 62020-04-23 09:23:26 Last login: Thu Apr 23 09:23:15 2020 from 10.10.30.233

2020-04-23 09:23:26 su hadoop

2020-04-23 09:23:26 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:23:26 [hadoop@hadoop-01 root]$ 
2020-04-23 09:23:26 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:26 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-04-06

2020-04-23 09:23:28 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:28 Last login: Thu Apr 23 09:23:30 2020 from 10.10.30.233

2020-04-23 09:23:28 su hadoop

2020-04-23 09:23:28 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:23:28 [hadoop@hadoop-01 root]$ 
2020-04-23 09:23:28 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:28 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/04/06/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-04-06

2020-04-23 09:23:44 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:44 Last login: Thu Apr 23 09:23:32 2020 from 10.10.30.233

2020-04-23 09:23:44 su hadoop

2020-04-23 09:23:44 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:23:44 [hadoop@hadoop-01 root]$ 
2020-04-23 09:23:44 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:44 cat hive/loadLogs.hive

2020-04-23 09:23:44 use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs PARTITION (y='${year}',m
2020-04-23 09:23:44 ='${month}',d='${day}');
2020-04-23 09:23:44 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:44 Last login: Thu Apr 23 09:23:48 2020 from 10.10.30.233

2020-04-23 09:23:44 su hadoop

2020-04-23 09:23:45 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:23:45 [hadoop@hadoop-01 root]$ 
2020-04-23 09:23:45 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:23:45 hive -d month=04 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-04-06 -d day=06 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:23:45 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:23:46 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:23:48 Hive Session ID = f6e21bc0-717c-419b-b71e-d943dd7e64c7

Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:23:54 Hive Session ID = de718f86-be23-4cbb-a70d-6d21a8666b26

2020-04-23 09:23:55 OK
Time taken: 0.782 seconds

2020-04-23 09:23:56 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=04, d=06)

2020-04-23 09:24:02 OK

2020-04-23 09:24:02 Time taken: 6.42 seconds

2020-04-23 09:24:02 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:02 备份hive待处理日志数据已完成。
2020-04-23 09:24:02 2020
 4 72020-04-23 09:24:03 Last login: Thu Apr 23 09:23:49 2020 from 10.10.30.233

2020-04-23 09:24:03 su hadoop

2020-04-23 09:24:03 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:24:03 [hadoop@hadoop-01 root]$ 
2020-04-23 09:24:03 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:03 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-04-07

2020-04-23 09:24:05 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:05 Last login: Thu Apr 23 09:24:07 2020 from 10.10.30.233

2020-04-23 09:24:05 su hadoop

2020-04-23 09:24:05 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:24:05 [hadoop@hadoop-01 root]$ 
2020-04-23 09:24:05 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:05 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/04/07/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-04-07

2020-04-23 09:24:14 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:14 Last login: Thu Apr 23 09:24:09 2020 from 10.10.30.233

2020-04-23 09:24:14 su hadoop

2020-04-23 09:24:14 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:24:14 [hadoop@hadoop-01 root]$ 
2020-04-23 09:24:14 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:14 cat hive/loadLogs.hive
use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs 
2020-04-23 09:24:14 PARTITION (y='${year}',m='${month}',d='${day}');
2020-04-23 09:24:14 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:14 Last login: Thu Apr 23 09:24:18 2020 from 10.10.30.233

2020-04-23 09:24:14 su hadoop

2020-04-23 09:24:14 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:24:14 [hadoop@hadoop-01 root]$ 
2020-04-23 09:24:14 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:14 hive -d month=04 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-04-07 -d day=07 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:24:15 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:24:15 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-23 09:24:15 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:24:17 Hive Session ID = 363d7bbb-e72b-4a26-9862-7ea5defc44be

2020-04-23 09:24:17 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:24:24 Hive Session ID = 42168103-e0b5-4936-a453-b5d96e1249ad

2020-04-23 09:24:25 OK
Time taken: 0.822 seconds

2020-04-23 09:24:26 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=04, d=07)

2020-04-23 09:24:29 OK
Time taken: 4.415 seconds

2020-04-23 09:24:30 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:30 备份hive待处理日志数据已完成。
2020-04-23 09:24:30 2020
 4 82020-04-23 09:24:30 Last login: Thu Apr 23 09:24:18 2020 from 10.10.30.233

2020-04-23 09:24:30 su hadoop

2020-04-23 09:24:30 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:24:30 [hadoop@hadoop-01 root]$ 
2020-04-23 09:24:30 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:30 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-04-08

2020-04-23 09:24:32 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:32 Last login: Thu Apr 23 09:24:34 2020 from 10.10.30.233

2020-04-23 09:24:32 su hadoop

2020-04-23 09:24:32 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:24:32 [hadoop@hadoop-01 root]$ 
2020-04-23 09:24:32 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:32 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/04/08/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-04-08

2020-04-23 09:24:49 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:49 Last login: Thu Apr 23 09:24:36 2020 from 10.10.30.233

2020-04-23 09:24:49 su hadoop

2020-04-23 09:24:49 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:24:49 [hadoop@hadoop-01 root]$ 
2020-04-23 09:24:49 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:49 cat hive/loadLogs.hive

2020-04-23 09:24:49 use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs PARTITION (y='${year}',m
2020-04-23 09:24:49 ='${month}',d='${day}');
2020-04-23 09:24:49 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:49 Last login: Thu Apr 23 09:24:53 2020 from 10.10.30.233

2020-04-23 09:24:49 su hadoop

2020-04-23 09:24:49 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:24:49 [hadoop@hadoop-01 root]$ 
2020-04-23 09:24:49 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:24:49 hive -d month=04 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-04-08 -d day=08 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:24:50 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:24:50 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:24:52 Hive Session ID = 316199c5-3a67-4453-a6e4-8902e6832bea

2020-04-23 09:24:52 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:24:59 Hive Session ID = 5ab9a245-44bd-4468-97de-670698878898

2020-04-23 09:25:00 OK
Time taken: 0.798 seconds

2020-04-23 09:25:01 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=04, d=08)

2020-04-23 09:25:04 OK
Time taken: 4.056 seconds

2020-04-23 09:25:05 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:05 备份hive待处理日志数据已完成。
2020-04-23 09:25:05 2020
 4 92020-04-23 09:25:05 Last login: Thu Apr 23 09:24:53 2020 from 10.10.30.233

2020-04-23 09:25:05 su hadoop

2020-04-23 09:25:05 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:25:05 [hadoop@hadoop-01 root]$ 
2020-04-23 09:25:05 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:05 hadoop fs -mkdir -p hdfs://192.168.15.21:90 00/data/hive/backup/2020-04-09

2020-04-23 09:25:07 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:07 Last login: Thu Apr 23 09:25:09 2020 from 10.10.30.233

2020-04-23 09:25:07 su hadoop

2020-04-23 09:25:07 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:25:07 [hadoop@hadoop-01 root]$ 
2020-04-23 09:25:07 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:07 hadoop fs -cp -p hdfs://192.168.15.21:9000/ data/logs/2020/04/09/*/* hdfs://192.168.15.21:9000/data/hive/backup/2020-04-09

2020-04-23 09:25:10 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:10 Last login: Thu Apr 23 09:25:11 2020 from 10.10.30.233

2020-04-23 09:25:10 su hadoop

2020-04-23 09:25:11 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:25:11 [hadoop@hadoop-01 root]$ 
2020-04-23 09:25:11 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:11 cat hive/loadLogs.hive
use yn_hadoop;
--设置作业名
set mapred.job.name = hive_load_yn_logs;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
--Map输入合并大小
set mapreduce.input.fileinputformat.split.maxsize=300000000;
set mapreduce.input.fileinputformat.split.minsize=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=100000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
--设置reduce数目
set hive.exec.reducers.bytes.per.reducer= 300000000;
set hive.exec.reducers.max=300;
set mapred.reduce.tasks=10;
--控制当map任务执行到哪个比例的时候就可以开始为reduce task申请资源
set mapreduce.job.reduce.slowstart.completedmaps=0.001;

-- 加载日志
LOAD DATA  INPATH '${hive.back.url}/*' overwrite INTO TABLE yn_logs 
2020-04-23 09:25:11 PARTITION (y='${year}',m='${month}',d='${day}');[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:11 Last login: Thu Apr 23 09:25:15 2020 from 10.10.30.233

2020-04-23 09:25:11 su hadoop

2020-04-23 09:25:11 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:25:11 [hadoop@hadoop-01 root]$ 
2020-04-23 09:25:11 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:11 hive -d month=04 -d hive.back.url=hdfs://19 2.168.15.21:9000/data/hive/backup/2020-04-09 -d day=09 -d year=2020 -f hive/load Logs.hive

2020-04-23 09:25:11 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:25:12 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:25:14 Hive Session ID = 5bcf9418-6df7-4b4b-80bb-8641184dc2c1

2020-04-23 09:25:14 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:25:21 Hive Session ID = ea896db7-7a36-4f97-aa8f-c9215ff20f5e

2020-04-23 09:25:22 OK
Time taken: 0.823 seconds

2020-04-23 09:25:23 Loading data to table yn_hadoop.yn_logs partition (y=2020, m=04, d=09)

2020-04-23 09:25:24 OK
Time taken: 2.101 seconds

2020-04-23 09:25:24 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:24 备份hive待处理日志数据已完成。
2020-04-23 09:25:24 开始处理对yn_logs进行分类处理
2020-04-23 09:25:25 Last login: Thu Apr 23 09:25:15 2020 from 10.10.30.233

2020-04-23 09:25:25 su hadoop

2020-04-23 09:25:25 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:25:25 [hadoop@hadoop-01 root]$ 
2020-04-23 09:25:25 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:25 cat hive/eventsType.hive
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set mapreduce.map.memory.mb = 4096;
set mapreduce.reduce.memory.mb = 4096;
set mapreduce.map.java.opts=-Xmx3278m;
set mapreduce.reduce.java.opts=-Xmx3278m;
set hive.exec.parallel=true;
set hive.exec.parallel.thread.number=16;   # 并行度
set mapreduce.job.reduce.slowstart.completedmaps=0.01;
set hive.merge.mapfiles = true;    --#在Map-only的任务结束时合并小文件
set hive.merge.mapredfiles = true; --#在Map-Reduce的任务结束时合并小文件
set hive.merge.size.per.task = 256000000; -- #合并文件的大小
set hive.merge.smallfiles.avgsize=16000000;

insert overwrite table events_type_log partition(events_type,y,m,d) select mac,sn,yl.user_id,user_type,create_time,str_to_map(data,';','=') as param,events_type,substr(create_time,1,4) y,substr(create_time,6,2) m,substr(create_time,9,2) d  from yn_logs yl
left join clean_user c on c.user_
2020-04-23 09:25:25 id = yl.user_id
where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' and create_time>'${startDate}' and create_time<='${endDate}' and c.user_id is null
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:25 Last login: Thu Apr 23 09:25:29 2020 from 10.10.30.233

2020-04-23 09:25:25 su hadoop

2020-04-23 09:25:25 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:25:25 [hadoop@hadoop-01 root]$ 
2020-04-23 09:25:25 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:25:25 hive -d startDate=2020-03-23 -d endDate=202 0-04-09 -f hive/eventsType.hive

2020-04-23 09:25:26 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:25:26 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2020-04-23 09:25:26 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:25:28 Hive Session ID = a78b5a18-ff27-4035-b878-19a669e8c50a

2020-04-23 09:25:28 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:25:35 Hive Session ID = ba7ff1c2-7bab-4662-bfdc-f0574abb8b37

2020-04-23 09:25:36 OK
Time taken: 0.874 seconds

2020-04-23 09:25:39 Query ID = hadoop_20200423092540_8a6fd36e-88d2-415f-8fb7-db54f4815c21
Total jobs = 3

2020-04-23 09:25:40 SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:25:49 2020-04-23 09:25:49	Starting to launch local task to process map join;	maximum memory = 239075328

2020-04-23 09:25:49 2020-04-23 09:25:49	Dump the side-table for tag: 1 with group count: 9 into file: file:/tmp/hive/a78b5a18-ff27-4035-b878-19a669e8c50a/hive_2020-04-23_09-25-40_439_5187586188087448594-1/-local-10003/HashTable-Stage-9/MapJoin-mapfile01--.hashtable
2020-04-23 09:25:50	Uploaded 1 File to: file:/tmp/hive/a78b5a18-ff27-4035-b878-19a669e8c50a/hive_2020-04-23_09-25-40_439_5187586188087448594-1/-local-10003/HashTable-Stage-9/MapJoin-mapfile01--.hashtable (488 bytes)
2020-04-23 09:25:50	End of local task; Time Taken: 0.505 sec.

2020-04-23 09:25:50 Execution completed successfully
MapredLocal task succeeded

2020-04-23 09:25:51 Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-23 09:25:54 Starting Job = job_1587346943308_0484, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0484/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0484

2020-04-23 09:26:00 Hadoop job information for Stage-9: number of mappers: 19; number of reducers: 0

2020-04-23 09:26:00 2020-04-23 09:26:00,993 Stage-9 map = 0%,  reduce = 0%

2020-04-23 09:26:19 2020-04-23 09:26:19,634 Stage-9 map = 4%,  reduce = 0%, Cumulative CPU 67.97 sec

2020-04-23 09:26:20 2020-04-23 09:26:20,661 Stage-9 map = 5%,  reduce = 0%, Cumulative CPU 85.04 sec

2020-04-23 09:26:24 2020-04-23 09:26:24,784 Stage-9 map = 6%,  reduce = 0%, Cumulative CPU 91.33 sec

2020-04-23 09:26:25 2020-04-23 09:26:25,810 Stage-9 map = 8%,  reduce = 0%, Cumulative CPU 110.28 sec

2020-04-23 09:26:26 2020-04-23 09:26:26,832 Stage-9 map = 9%,  reduce = 0%, Cumulative CPU 116.56 sec

2020-04-23 09:26:30 2020-04-23 09:26:30,939 Stage-9 map = 10%,  reduce = 0%, Cumulative CPU 123.18 sec

2020-04-23 09:26:31 2020-04-23 09:26:31,971 Stage-9 map = 13%,  reduce = 0%, Cumulative CPU 148.98 sec

2020-04-23 09:26:36 2020-04-23 09:26:37,095 Stage-9 map = 14%,  reduce = 0%, Cumulative CPU 155.15 sec

2020-04-23 09:26:37 2020-04-23 09:26:38,135 Stage-9 map = 16%,  reduce = 0%, Cumulative CPU 180.35 sec

2020-04-23 09:26:43 2020-04-23 09:26:43,253 Stage-9 map = 17%,  reduce = 0%, Cumulative CPU 186.56 sec

2020-04-23 09:26:44 2020-04-23 09:26:44,285 Stage-9 map = 20%,  reduce = 0%, Cumulative CPU 211.44 sec

2020-04-23 09:26:49 2020-04-23 09:26:49,411 Stage-9 map = 22%,  reduce = 0%, Cumulative CPU 223.8 sec

2020-04-23 09:26:50 2020-04-23 09:26:50,441 Stage-9 map = 24%,  reduce = 0%, Cumulative CPU 243.57 sec

2020-04-23 09:26:52 2020-04-23 09:26:52,497 Stage-9 map = 25%,  reduce = 0%, Cumulative CPU 247.1 sec

2020-04-23 09:26:56 2020-04-23 09:26:56,616 Stage-9 map = 26%,  reduce = 0%, Cumulative CPU 256.32 sec

2020-04-23 09:27:05 2020-04-23 09:27:05,853 Stage-9 map = 27%,  reduce = 0%, Cumulative CPU 269.34 sec

2020-04-23 09:27:10 2020-04-23 09:27:10,989 Stage-9 map = 28%,  reduce = 0%, Cumulative CPU 306.06 sec

2020-04-23 09:27:11 2020-04-23 09:27:12,022 Stage-9 map = 29%,  reduce = 0%, Cumulative CPU 322.53 sec

2020-04-23 09:27:13 2020-04-23 09:27:14,061 Stage-9 map = 30%,  reduce = 0%, Cumulative CPU 334.76 sec

2020-04-23 09:27:15 2020-04-23 09:27:16,106 Stage-9 map = 31%,  reduce = 0%, Cumulative CPU 341.37 sec

2020-04-23 09:27:17 2020-04-23 09:27:18,150 Stage-9 map = 33%,  reduce = 0%, Cumulative CPU 354.84 sec

2020-04-23 09:27:19 2020-04-23 09:27:20,202 Stage-9 map = 34%,  reduce = 0%, Cumulative CPU 366.76 sec

2020-04-23 09:27:21 2020-04-23 09:27:22,255 Stage-9 map = 35%,  reduce = 0%, Cumulative CPU 372.99 sec

2020-04-23 09:27:23 2020-04-23 09:27:23,284 Stage-9 map = 36%,  reduce = 0%, Cumulative CPU 379.08 sec

2020-04-23 09:27:26 2020-04-23 09:27:26,357 Stage-9 map = 38%,  reduce = 0%, Cumulative CPU 397.23 sec

2020-04-23 09:27:28 2020-04-23 09:27:28,409 Stage-9 map = 39%,  reduce = 0%, Cumulative CPU 403.64 sec

2020-04-23 09:27:29 2020-04-23 09:27:29,436 Stage-9 map = 40%,  reduce = 0%, Cumulative CPU 409.37 sec

2020-04-23 09:27:30 2020-04-23 09:27:30,464 Stage-9 map = 41%,  reduce = 0%, Cumulative CPU 415.72 sec

2020-04-23 09:27:32 2020-04-23 09:27:32,507 Stage-9 map = 42%,  reduce = 0%, Cumulative CPU 427.89 sec

2020-04-23 09:27:34 2020-04-23 09:27:34,545 Stage-9 map = 43%,  reduce = 0%, Cumulative CPU 434.12 sec

2020-04-23 09:27:36 2020-04-23 09:27:36,587 Stage-9 map = 44%,  reduce = 0%, Cumulative CPU 446.17 sec

2020-04-23 09:27:38 2020-04-23 09:27:38,631 Stage-9 map = 45%,  reduce = 0%, Cumulative CPU 457.69 sec

2020-04-23 09:27:40 2020-04-23 09:27:40,675 Stage-9 map = 46%,  reduce = 0%, Cumulative CPU 463.84 sec

2020-04-23 09:27:41 2020-04-23 09:27:41,707 Stage-9 map = 47%,  reduce = 0%, Cumulative CPU 469.13 sec

2020-04-23 09:27:42 2020-04-23 09:27:42,740 Stage-9 map = 48%,  reduce = 0%, Cumulative CPU 475.21 sec

2020-04-23 09:27:43 2020-04-23 09:27:43,758 Stage-9 map = 50%,  reduce = 0%, Cumulative CPU 487.55 sec

2020-04-23 09:27:47 2020-04-23 09:27:47,848 Stage-9 map = 51%,  reduce = 0%, Cumulative CPU 496.33 sec

2020-04-23 09:27:49 2020-04-23 09:27:49,893 Stage-9 map = 52%,  reduce = 0%, Cumulative CPU 506.07 sec

2020-04-23 09:27:52 2020-04-23 09:27:52,975 Stage-9 map = 53%,  reduce = 0%, Cumulative CPU 508.92 sec

2020-04-23 09:27:59 2020-04-23 09:28:00,167 Stage-9 map = 54%,  reduce = 0%, Cumulative CPU 525.97 sec

2020-04-23 09:28:03 2020-04-23 09:28:04,261 Stage-9 map = 55%,  reduce = 0%, Cumulative CPU 544.33 sec

2020-04-23 09:28:05 2020-04-23 09:28:05,287 Stage-9 map = 56%,  reduce = 0%, Cumulative CPU 566.97 sec

2020-04-23 09:28:06 2020-04-23 09:28:06,314 Stage-9 map = 57%,  reduce = 0%, Cumulative CPU 582.91 sec

2020-04-23 09:28:09 2020-04-23 09:28:09,394 Stage-9 map = 58%,  reduce = 0%, Cumulative CPU 600.07 sec

2020-04-23 09:28:11 2020-04-23 09:28:11,432 Stage-9 map = 60%,  reduce = 0%, Cumulative CPU 619.05 sec

2020-04-23 09:28:15 2020-04-23 09:28:15,514 Stage-9 map = 61%,  reduce = 0%, Cumulative CPU 632.16 sec

2020-04-23 09:28:16 2020-04-23 09:28:16,540 Stage-9 map = 62%,  reduce = 0%, Cumulative CPU 638.43 sec

2020-04-23 09:28:17 2020-04-23 09:28:17,559 Stage-9 map = 64%,  reduce = 0%, Cumulative CPU 650.97 sec

2020-04-23 09:28:21 2020-04-23 09:28:21,651 Stage-9 map = 65%,  reduce = 0%, Cumulative CPU 663.63 sec

2020-04-23 09:28:22 2020-04-23 09:28:22,669 Stage-9 map = 66%,  reduce = 0%, Cumulative CPU 669.96 sec

2020-04-23 09:28:23 2020-04-23 09:28:23,687 Stage-9 map = 68%,  reduce = 0%, Cumulative CPU 682.5 sec

2020-04-23 09:28:27 2020-04-23 09:28:27,774 Stage-9 map = 69%,  reduce = 0%, Cumulative CPU 694.58 sec

2020-04-23 09:28:28 2020-04-23 09:28:28,797 Stage-9 map = 71%,  reduce = 0%, Cumulative CPU 706.91 sec

2020-04-23 09:28:29 2020-04-23 09:28:29,816 Stage-9 map = 72%,  reduce = 0%, Cumulative CPU 713.05 sec

2020-04-23 09:28:30 2020-04-23 09:28:30,833 Stage-9 map = 73%,  reduce = 0%, Cumulative CPU 719.01 sec

2020-04-23 09:28:33 2020-04-23 09:28:33,907 Stage-9 map = 74%,  reduce = 0%, Cumulative CPU 726.0 sec

2020-04-23 09:28:34 2020-04-23 09:28:34,940 Stage-9 map = 75%,  reduce = 0%, Cumulative CPU 738.23 sec

2020-04-23 09:28:36 2020-04-23 09:28:36,983 Stage-9 map = 76%,  reduce = 0%, Cumulative CPU 742.4 sec

2020-04-23 09:28:37 2020-04-23 09:28:38,011 Stage-9 map = 77%,  reduce = 0%, Cumulative CPU 745.23 sec

2020-04-23 09:28:39 2020-04-23 09:28:40,064 Stage-9 map = 78%,  reduce = 0%, Cumulative CPU 751.72 sec

2020-04-23 09:28:48 2020-04-23 09:28:48,259 Stage-9 map = 79%,  reduce = 0%, Cumulative CPU 761.47 sec

2020-04-23 09:28:50 2020-04-23 09:28:50,307 Stage-9 map = 80%,  reduce = 0%, Cumulative CPU 777.52 sec

2020-04-23 09:28:53 2020-04-23 09:28:53,375 Stage-9 map = 81%,  reduce = 0%, Cumulative CPU 795.26 sec

2020-04-23 09:28:56 2020-04-23 09:28:56,447 Stage-9 map = 83%,  reduce = 0%, Cumulative CPU 836.22 sec

2020-04-23 09:28:59 2020-04-23 09:28:59,503 Stage-9 map = 84%,  reduce = 0%, Cumulative CPU 842.49 sec

2020-04-23 09:29:02 2020-04-23 09:29:02,570 Stage-9 map = 87%,  reduce = 0%, Cumulative CPU 861.4 sec

2020-04-23 09:29:05 2020-04-23 09:29:05,631 Stage-9 map = 88%,  reduce = 0%, Cumulative CPU 867.77 sec

2020-04-23 09:29:07 2020-04-23 09:29:07,674 Stage-9 map = 89%,  reduce = 0%, Cumulative CPU 873.98 sec

2020-04-23 09:29:08 2020-04-23 09:29:08,701 Stage-9 map = 90%,  reduce = 0%, Cumulative CPU 886.73 sec

2020-04-23 09:29:11 2020-04-23 09:29:11,763 Stage-9 map = 91%,  reduce = 0%, Cumulative CPU 893.12 sec

2020-04-23 09:29:13 2020-04-23 09:29:13,811 Stage-9 map = 92%,  reduce = 0%, Cumulative CPU 899.23 sec

2020-04-23 09:29:14 2020-04-23 09:29:14,837 Stage-9 map = 94%,  reduce = 0%, Cumulative CPU 912.42 sec

2020-04-23 09:29:19 2020-04-23 09:29:19,943 Stage-9 map = 96%,  reduce = 0%, Cumulative CPU 930.9 sec

2020-04-23 09:29:20 2020-04-23 09:29:20,968 Stage-9 map = 97%,  reduce = 0%, Cumulative CPU 937.08 sec

2020-04-23 09:29:22 2020-04-23 09:29:23,009 Stage-9 map = 99%,  reduce = 0%, Cumulative CPU 948.6 sec

2020-04-23 09:29:25 2020-04-23 09:29:26,086 Stage-9 map = 100%,  reduce = 0%, Cumulative CPU 953.93 sec

2020-04-23 09:29:26 MapReduce Total cumulative CPU time: 15 minutes 53 seconds 930 msec

2020-04-23 09:29:26 Ended Job = job_1587346943308_0484

2020-04-23 09:29:27 Stage-4 is filtered out by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is selected by condition resolver.

2020-04-23 09:29:29 Launching Job 3 out of 3

2020-04-23 09:29:29 Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-23 09:29:34 Starting Job = job_1587346943308_0485, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0485/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0485

2020-04-23 09:29:41 Hadoop job information for Stage-5: number of mappers: 76; number of reducers: 0

2020-04-23 09:29:41 2020-04-23 09:29:41,291 Stage-5 map = 0%,  reduce = 0%

2020-04-23 09:29:50 2020-04-23 09:29:50,535 Stage-5 map = 1%,  reduce = 0%, Cumulative CPU 6.91 sec

2020-04-23 09:29:52 2020-04-23 09:29:52,619 Stage-5 map = 3%,  reduce = 0%, Cumulative CPU 6.91 sec

2020-04-23 09:29:53 2020-04-23 09:29:53,643 Stage-5 map = 7%,  reduce = 0%, Cumulative CPU 40.39 sec

2020-04-23 09:29:58 2020-04-23 09:29:58,777 Stage-5 map = 8%,  reduce = 0%, Cumulative CPU 47.44 sec

2020-04-23 09:30:00 2020-04-23 09:30:00,833 Stage-5 map = 9%,  reduce = 0%, Cumulative CPU 54.86 sec

2020-04-23 09:30:01 2020-04-23 09:30:01,869 Stage-5 map = 12%,  reduce = 0%, Cumulative CPU 69.5 sec

2020-04-23 09:30:02 2020-04-23 09:30:02,908 Stage-5 map = 13%,  reduce = 0%, Cumulative CPU 77.37 sec

2020-04-23 09:30:04 2020-04-23 09:30:04,965 Stage-5 map = 14%,  reduce = 0%, Cumulative CPU 83.68 sec

2020-04-23 09:30:08 2020-04-23 09:30:09,072 Stage-5 map = 16%,  reduce = 0%, Cumulative CPU 90.34 sec

2020-04-23 09:30:09 2020-04-23 09:30:10,100 Stage-5 map = 18%,  reduce = 0%, Cumulative CPU 103.59 sec

2020-04-23 09:30:10 2020-04-23 09:30:11,130 Stage-5 map = 20%,  reduce = 0%, Cumulative CPU 111.17 sec

2020-04-23 09:30:11 2020-04-23 09:30:12,162 Stage-5 map = 21%,  reduce = 0%, Cumulative CPU 117.07 sec

2020-04-23 09:30:17 2020-04-23 09:30:17,319 Stage-5 map = 22%,  reduce = 0%, Cumulative CPU 124.86 sec

2020-04-23 09:30:18 2020-04-23 09:30:18,343 Stage-5 map = 24%,  reduce = 0%, Cumulative CPU 132.13 sec

2020-04-23 09:30:19 2020-04-23 09:30:19,362 Stage-5 map = 28%,  reduce = 0%, Cumulative CPU 154.2 sec

2020-04-23 09:30:25 2020-04-23 09:30:25,537 Stage-5 map = 29%,  reduce = 0%, Cumulative CPU 160.47 sec

2020-04-23 09:30:26 2020-04-23 09:30:26,564 Stage-5 map = 32%,  reduce = 0%, Cumulative CPU 174.42 sec

2020-04-23 09:30:27 2020-04-23 09:30:27,592 Stage-5 map = 34%,  reduce = 0%, Cumulative CPU 187.08 sec

2020-04-23 09:30:32 2020-04-23 09:30:32,719 Stage-5 map = 36%,  reduce = 0%, Cumulative CPU 193.63 sec

2020-04-23 09:30:34 2020-04-23 09:30:34,788 Stage-5 map = 38%,  reduce = 0%, Cumulative CPU 205.65 sec

2020-04-23 09:30:35 2020-04-23 09:30:35,813 Stage-5 map = 41%,  reduce = 0%, Cumulative CPU 218.1 sec

2020-04-23 09:30:38 2020-04-23 09:30:38,883 Stage-5 map = 42%,  reduce = 0%, Cumulative CPU 224.54 sec

2020-04-23 09:30:41 2020-04-23 09:30:41,961 Stage-5 map = 45%,  reduce = 0%, Cumulative CPU 236.4 sec

2020-04-23 09:30:43 2020-04-23 09:30:44,030 Stage-5 map = 47%,  reduce = 0%, Cumulative CPU 249.09 sec

2020-04-23 09:30:44 2020-04-23 09:30:45,057 Stage-5 map = 49%,  reduce = 0%, Cumulative CPU 254.69 sec

2020-04-23 09:30:50 2020-04-23 09:30:51,210 Stage-5 map = 51%,  reduce = 0%, Cumulative CPU 266.99 sec

2020-04-23 09:30:51 2020-04-23 09:30:52,235 Stage-5 map = 54%,  reduce = 0%, Cumulative CPU 279.35 sec

2020-04-23 09:30:53 2020-04-23 09:30:53,262 Stage-5 map = 55%,  reduce = 0%, Cumulative CPU 285.81 sec

2020-04-23 09:30:56 2020-04-23 09:30:56,337 Stage-5 map = 57%,  reduce = 0%, Cumulative CPU 289.17 sec

2020-04-23 09:30:57 2020-04-23 09:30:57,364 Stage-5 map = 58%,  reduce = 0%, Cumulative CPU 293.18 sec

2020-04-23 09:30:58 2020-04-23 09:30:58,389 Stage-5 map = 61%,  reduce = 0%, Cumulative CPU 300.59 sec

2020-04-23 09:30:59 2020-04-23 09:30:59,433 Stage-5 map = 62%,  reduce = 0%, Cumulative CPU 304.38 sec

2020-04-23 09:31:02 2020-04-23 09:31:02,557 Stage-5 map = 63%,  reduce = 0%, Cumulative CPU 307.65 sec

2020-04-23 09:31:03 2020-04-23 09:31:03,578 Stage-5 map = 64%,  reduce = 0%, Cumulative CPU 312.35 sec

2020-04-23 09:31:04 2020-04-23 09:31:04,598 Stage-5 map = 67%,  reduce = 0%, Cumulative CPU 319.53 sec

2020-04-23 09:31:05 2020-04-23 09:31:05,626 Stage-5 map = 68%,  reduce = 0%, Cumulative CPU 322.88 sec

2020-04-23 09:31:07 2020-04-23 09:31:07,689 Stage-5 map = 70%,  reduce = 0%, Cumulative CPU 326.83 sec

2020-04-23 09:31:08 2020-04-23 09:31:08,720 Stage-5 map = 71%,  reduce = 0%, Cumulative CPU 330.38 sec

2020-04-23 09:31:09 2020-04-23 09:31:09,754 Stage-5 map = 74%,  reduce = 0%, Cumulative CPU 337.47 sec

2020-04-23 09:31:11 2020-04-23 09:31:11,806 Stage-5 map = 75%,  reduce = 0%, Cumulative CPU 340.91 sec

2020-04-23 09:31:12 2020-04-23 09:31:12,846 Stage-5 map = 76%,  reduce = 0%, Cumulative CPU 344.59 sec

2020-04-23 09:31:15 2020-04-23 09:31:15,930 Stage-5 map = 80%,  reduce = 0%, Cumulative CPU 353.56 sec

2020-04-23 09:31:16 2020-04-23 09:31:16,956 Stage-5 map = 83%,  reduce = 0%, Cumulative CPU 359.5 sec

2020-04-23 09:31:20 2020-04-23 09:31:21,061 Stage-5 map = 87%,  reduce = 0%, Cumulative CPU 370.36 sec

2020-04-23 09:31:21 2020-04-23 09:31:22,086 Stage-5 map = 88%,  reduce = 0%, Cumulative CPU 373.34 sec

2020-04-23 09:31:23 2020-04-23 09:31:24,133 Stage-5 map = 89%,  reduce = 0%, Cumulative CPU 376.48 sec

2020-04-23 09:31:25 2020-04-23 09:31:26,207 Stage-5 map = 91%,  reduce = 0%, Cumulative CPU 379.93 sec

2020-04-23 09:31:27 2020-04-23 09:31:27,235 Stage-5 map = 95%,  reduce = 0%, Cumulative CPU 389.5 sec

2020-04-23 09:31:29 2020-04-23 09:31:29,279 Stage-5 map = 96%,  reduce = 0%, Cumulative CPU 392.76 sec

2020-04-23 09:31:31 2020-04-23 09:31:31,339 Stage-5 map = 99%,  reduce = 0%, Cumulative CPU 398.78 sec

2020-04-23 09:31:32 2020-04-23 09:31:32,363 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 401.24 sec

2020-04-23 09:31:33 MapReduce Total cumulative CPU time: 6 minutes 41 seconds 240 msec
Ended Job = job_1587346943308_0485

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operateResumePoint/y=2020/m=03/d=23
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operateResumePoint/y=2020/m=03/d=25
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operateResumePoint/y=2020/m=03/d=30
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operateResumePoint/y=2020/m=04/d=01
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operateResumePoint/y=2020/m=04/d=08

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationDetails/y=2020/m=03/d=23
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationDetails/y=2020/m=03/d=25

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationDetails/y=2020/m=03/d=30
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationDetails/y=2020/m=04/d=01

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationDetails/y=2020/m=04/d=08
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=23

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=25

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=26

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=27

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=28

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=29

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=30

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=03/d=31

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=04/d=01

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=04/d=02

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=04/d=03

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=04/d=04

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=04/d=05
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=04/d=06
Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=04/d=07

2020-04-23 09:31:33 Moving data to directory hdfs://hadoop-cluster/data/hive/events_type_log/.hive-staging_hive_2020-04-23_09-25-40_439_5187586188087448594-1/-ext-10000/events_type=operationPage/y=2020/m=04/d=08

2020-04-23 09:31:35 Loading data to table yn_hadoop.events_type_log partition (events_type=null, y=null, m=null, d=null)

2020-04-23 09:31:35 


2020-04-23 09:31:43 	 Time taken to load dynamic partitions: 7.957 seconds
	 Time taken for adding to write entity : 0.013 seconds

2020-04-23 09:31:47 MapReduce Jobs Launched: 
Stage-Stage-9: Map: 19   Cumulative CPU: 953.93 sec   HDFS Read: 5020059399 HDFS Write: 2726384388 SUCCESS
Stage-Stage-5: Map: 76   Cumulative CPU: 401.24 sec   HDFS Read: 1504427676 HDFS Write: 1503590964 SUCCESS
Total MapReduce CPU Time Spent: 22 minutes 35 seconds 170 msec
OK

2020-04-23 09:31:47 Time taken: 367.607 seconds

2020-04-23 09:31:48 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:31:48 yn_logs进行分类处理已完成
2020-04-23 09:31:48 开始处理对日统计数据
2020-04-23 09:31:48 # query hive不支持- -create-hive-table
sqoop import \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--table user_info \
--driver com.mysql.jdbc.Driver \
--fields-terminated-by '\t' \
--delete-target-dir \
--num-mappers 1 \
--hive-import \
--hive-database yn_hadoop \
--hive-table user_info \
--hive-overwrite
2020-04-23 09:31:48 Last login: Thu Apr 23 09:25:29 2020 from 10.10.30.233

2020-04-23 09:31:48 su hadoop

2020-04-23 09:31:48 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:31:48 [hadoop@hadoop-01 root]$ 
2020-04-23 09:31:48 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:31:48 # query hive不支持- -create-hive-table
[hadoop@hadoop-01 sx_hadoop_script]$ sqoop import \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --table user_info \
> --driver com.mysql.jdbc.Driver \
> --fields-terminated-by '\t' \
> --delete-target-dir \
> --num-mappers 1 \
> --hive-import \
> --hive-database yn_hadoop \
> --hive-table user_info \
> --hive-overwrite

2020-04-23 09:31:48 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:31:49 2020-04-23 09:31:49,387 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-23 09:31:49,413 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:31:49 2020-04-23 09:31:49,460 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
2020-04-23 09:31:49,468 INFO manager.SqlManager: Using default fetchSize of 1000
2020-04-23 09:31:49,471 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:31:49 2020-04-23 09:31:49,687 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-23 09:31:49,691 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-23 09:31:49 2020-04-23 09:31:49,707 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:31:50 注: /tmp/sqoop-hadoop/compile/045c80e6d5f11ea7ba9d8780b2c570ef/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:31:50,604 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/045c80e6d5f11ea7ba9d8780b2c570ef/user_info.jar

2020-04-23 09:31:51 2020-04-23 09:31:51,306 INFO tool.ImportTool: Destination directory user_info is not present, hence not deleting.
2020-04-23 09:31:51,311 INFO mapreduce.ImportJobBase: Beginning import of user_info
2020-04-23 09:31:51,311 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2020-04-23 09:31:51,315 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
2020-04-23 09:31:51,316 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-23 09:31:51,325 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:31:51 2020-04-23 09:31:51,763 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0486

2020-04-23 09:31:53 2020-04-23 09:31:53,541 INFO db.DBInputFormat: Using read commited transaction isolation

2020-04-23 09:31:53 2020-04-23 09:31:53,576 INFO mapreduce.JobSubmitter: number of splits:1

2020-04-23 09:31:53 2020-04-23 09:31:53,774 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0486
2020-04-23 09:31:53,776 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:31:53 2020-04-23 09:31:53,950 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:31:53,950 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:31:53 2020-04-23 09:31:53,995 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0486

2020-04-23 09:31:53 2020-04-23 09:31:54,020 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0486/
2020-04-23 09:31:54,021 INFO mapreduce.Job: Running job: job_1587346943308_0486

2020-04-23 09:31:59 2020-04-23 09:32:00,105 INFO mapreduce.Job: Job job_1587346943308_0486 running in uber mode : false
2020-04-23 09:32:00,107 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:32:04 2020-04-23 09:32:05,176 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:32:05,188 INFO mapreduce.Job: Job job_1587346943308_0486 completed successfully

2020-04-23 09:32:05 2020-04-23 09:32:05,301 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=231942
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=618572
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2632
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2632
		Total vcore-milliseconds taken by all map tasks=2632
		Total megabyte-milliseconds taken by all map tasks=2695168
	Map-Reduce Framework
		Map input records=15888
		Map output records=15888
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=57
		CPU time spen
2020-04-23 09:32:05 t (ms)=1170
		Physical memory (bytes) snapshot=244432896
		Virtual memory (bytes) snapshot=2446524416
		Total committed heap usage (bytes)=192937984
		Peak Map Physical memory (bytes)=244432896
		Peak Map Virtual memory (bytes)=2446524416
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=618572
2020-04-23 09:32:05,309 INFO mapreduce.ImportJobBase: Transferred 604.0742 KB in 13.975 seconds (43.2252 KB/sec)

2020-04-23 09:32:05 2020-04-23 09:32:05,313 INFO mapreduce.ImportJobBase: Retrieved 15888 records.
2020-04-23 09:32:05,313 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user_info

2020-04-23 09:32:05 2020-04-23 09:32:05,325 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0
2020-04-23 09:32:05,327 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM user_info AS t WHERE 1=0

2020-04-23 09:32:05 2020-04-23 09:32:05,345 INFO hive.HiveImport: Loading uploaded data into Hive
2020-04-23 09:32:05,353 INFO conf.HiveConf: Found configuration file file:/usr/local/hive-3.1.2/conf/hive-site.xml

2020-04-23 09:32:05 2020-04-23 09:32:05,519 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-23 09:32:05 2020-04-23 09:32:06,091 INFO hive.HiveImport: which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:32:05 2020-04-23 09:32:06,113 INFO hive.HiveImport: WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.

2020-04-23 09:32:06 2020-04-23 09:32:06,541 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
2020-04-23 09:32:06,541 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-23 09:32:06,541 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2020-04-23 09:32:06,541 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2020-04-23 09:32:06,551 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:32:08 2020-04-23 09:32:08,600 INFO hive.HiveImport: Hive Session ID = 3fa9eed5-eefc-496e-ba45-bc826c2bdd65

2020-04-23 09:32:08 2020-04-23 09:32:08,649 INFO hive.HiveImport: 
2020-04-23 09:32:08,649 INFO hive.HiveImport: Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:32:15 2020-04-23 09:32:15,725 INFO hive.HiveImport: Hive Session ID = 423dab39-3469-4550-814f-11554c8a5715

2020-04-23 09:32:16 2020-04-23 09:32:17,199 INFO hive.HiveImport: OK
2020-04-23 09:32:17,210 INFO hive.HiveImport: Time taken: 1.407 seconds

2020-04-23 09:32:17 2020-04-23 09:32:17,491 INFO hive.HiveImport: Loading data to table yn_hadoop.user_info

2020-04-23 09:32:17 2020-04-23 09:32:18,057 INFO hive.HiveImport: OK
2020-04-23 09:32:18,068 INFO hive.HiveImport: Time taken: 0.835 seconds

2020-04-23 09:32:18 2020-04-23 09:32:18,578 INFO hive.HiveImport: Hive import complete.

2020-04-23 09:32:18 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:32:18 Last login: Thu Apr 23 09:31:48 2020 from 10.10.30.233

2020-04-23 09:32:18 su hadoop

2020-04-23 09:32:18 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:32:18 [hadoop@hadoop-01 root]$ 
2020-04-23 09:32:18 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:32:18 cat hive/statistic/RegionbyDay.hive

2020-04-23 09:32:19 
-- 按天统计专区信息
-- 进入数据库
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
-- 1、按天去重 日访问用户信息
insert overwrite table visit_user_page_day partition(y,m,d)  select param["parentColumnId"] as parent_column_id,user_type,user_id ,'' area_code,min(create_time) as create_time,y,m,d from events_type_log
where events_type in ('operationDetails','operationPage') and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param["parentColumnId"],user_type,user_id,y,m,d cluster by user_id,user_type;




--3、 按天去除重复播放用户
insert overwrite table play_user_day partition(y,m,d)  select param['parentColumnId'] as parent_column_id,user_type,user_id ,'' area_code,sum(1) as play_count,sum(cast(nvl(param['timePosition'],'0') as bigint)) as duration,y,m,d from even
2020-04-23 09:32:19 ts_type_log
where events_type='operateResumePoint' and param['operateType']='add' and concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by param['parentColumnId'],user_type,user_id,y,m,d cluster by user_id,user_type;



-- 创建临时存放统计数据的表；
-- 统计日页面访问用户数，统计日播放用户数，统计日播放次数
-- 统计页面访问用户数
truncate table app_visit_count_day;
insert  overwrite table app_visit_count_day
select a.t as t_date,a.parent_column_id,a.user_type,sum(a.page_user_count) as page_user_count,sum(play_user_count) as play_user_count,sum(a.play_count) as play_count,sum(a.duration) as duration from (
select concat(y,'-',m,'-',d) as t,parent_column_id,user_type, count(1) as page_user_count,0 as play_user_count,0 as duration,0 as play_count from visit_user_page_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
UNION ALL
se
2020-04-23 09:32:19 lect concat(y,'-',m,'-',d) as t,parent_column_id,user_type,0 as page_user_count,count(1) as play_user_count, sum(duration) as duration,sum(play_count) as play_count from play_user_day where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by y,m,d,parent_column_id,user_type
) a group by a.t,a.parent_column_id,a.user_type;

-- 统计 播放次数
-- select sum(cast(nvl(data['timePosition'],'0') as bigint)) as duration,count(1) as play_count from events_type_log where y=2020 and m='04' and d='01' and data['operateType']='add';


[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:32:19 Last login: Thu Apr 23 09:32:19 2020 from 10.10.30.233

2020-04-23 09:32:19 su hadoop

2020-04-23 09:32:19 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:32:19 [hadoop@hadoop-01 root]$ 
2020-04-23 09:32:19 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:32:19 hive -d startDate=2020-03-23 -d endDate=202 0-04-09 -f hive/statistic/RegionbyDay.hive

2020-04-23 09:32:19 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:32:20 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:32:22 Hive Session ID = 6b4d5a6b-ae56-4ecf-bb66-d55ccce7c596

Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:32:29 Hive Session ID = 1c55b137-c4e9-4a14-91d7-b0eace7a3aa6

2020-04-23 09:32:30 OK
Time taken: 0.791 seconds

2020-04-23 09:32:31 Query ID = hadoop_20200423093230_58d3cdbe-1d08-4eb7-b3b1-6d3b2050bb57
Total jobs = 3
Launching Job 1 out of 3

2020-04-23 09:32:31 Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:32:34 Starting Job = job_1587346943308_0487, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0487/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0487

2020-04-23 09:32:40 Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 6

2020-04-23 09:32:40 2020-04-23 09:32:40,588 Stage-1 map = 0%,  reduce = 0%

2020-04-23 09:32:55 2020-04-23 09:32:55,272 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 14.98 sec

2020-04-23 09:32:58 2020-04-23 09:32:58,448 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 30.95 sec

2020-04-23 09:32:59 2020-04-23 09:32:59,523 Stage-1 map = 57%,  reduce = 0%, Cumulative CPU 61.95 sec

2020-04-23 09:33:00 2020-04-23 09:33:00,574 Stage-1 map = 72%,  reduce = 0%, Cumulative CPU 89.24 sec

2020-04-23 09:33:02 2020-04-23 09:33:02,655 Stage-1 map = 81%,  reduce = 0%, Cumulative CPU 91.27 sec

2020-04-23 09:33:03 2020-04-23 09:33:03,682 Stage-1 map = 90%,  reduce = 0%, Cumulative CPU 95.0 sec

2020-04-23 09:33:04 2020-04-23 09:33:04,736 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 99.17 sec

2020-04-23 09:33:07 2020-04-23 09:33:07,878 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 119.68 sec

2020-04-23 09:33:08 2020-04-23 09:33:08,911 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 141.58 sec

2020-04-23 09:33:09 MapReduce Total cumulative CPU time: 2 minutes 21 seconds 580 msec

2020-04-23 09:33:09 Ended Job = job_1587346943308_0487

2020-04-23 09:33:09 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:33:10 Starting Job = job_1587346943308_0488, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0488/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0488

2020-04-23 09:33:20 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1

2020-04-23 09:33:20 2020-04-23 09:33:20,880 Stage-2 map = 0%,  reduce = 0%

2020-04-23 09:33:28 2020-04-23 09:33:29,146 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 7.78 sec

2020-04-23 09:33:36 2020-04-23 09:33:36,381 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 14.63 sec

2020-04-23 09:33:37 MapReduce Total cumulative CPU time: 14 seconds 630 msec
Ended Job = job_1587346943308_0488

2020-04-23 09:33:37 Loading data to table yn_hadoop.visit_user_page_day partition (y=null, m=null, d=null)

2020-04-23 09:33:37 


2020-04-23 09:33:39 	 Time taken to load dynamic partitions: 1.967 seconds
	 Time taken for adding to write entity : 0.002 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:33:39 Starting Job = job_1587346943308_0489, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0489/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0489

2020-04-23 09:33:48 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-23 09:33:48,286 Stage-4 map = 0%,  reduce = 0%

2020-04-23 09:33:54 2020-04-23 09:33:54,475 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.5 sec

2020-04-23 09:34:01 2020-04-23 09:34:01,709 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.35 sec

2020-04-23 09:34:02 MapReduce Total cumulative CPU time: 4 seconds 350 msec
Ended Job = job_1587346943308_0489

2020-04-23 09:34:04 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 6  Reduce: 6   Cumulative CPU: 141.58 sec   HDFS Read: 1534544294 HDFS Write: 11709724 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 14.63 sec   HDFS Read: 11724261 HDFS Write: 6944906 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.35 sec   HDFS Read: 43517 HDFS Write: 43694 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 40 seconds 560 msec
OK
Time taken: 93.784 seconds

2020-04-23 09:34:04 Query ID = hadoop_20200423093404_15db12be-7910-43a5-acd7-74b62549b519
Total jobs = 3
Launching Job 1 out of 3

2020-04-23 09:34:04 Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:34:04 Starting Job = job_1587346943308_0490, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0490/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0490

2020-04-23 09:34:14 Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 3

2020-04-23 09:34:14 2020-04-23 09:34:14,371 Stage-1 map = 0%,  reduce = 0%

2020-04-23 09:34:26 2020-04-23 09:34:26,825 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 11.1 sec

2020-04-23 09:34:29 2020-04-23 09:34:29,905 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 26.45 sec

2020-04-23 09:34:30 2020-04-23 09:34:30,932 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 41.23 sec

2020-04-23 09:34:33 2020-04-23 09:34:34,038 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 51.47 sec

2020-04-23 09:34:34 2020-04-23 09:34:35,064 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 56.04 sec

2020-04-23 09:34:36 MapReduce Total cumulative CPU time: 56 seconds 40 msec

2020-04-23 09:34:36 Ended Job = job_1587346943308_0490

2020-04-23 09:34:36 Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:34:37 Starting Job = job_1587346943308_0491, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0491/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0491

2020-04-23 09:34:47 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1

2020-04-23 09:34:47 2020-04-23 09:34:47,854 Stage-2 map = 0%,  reduce = 0%

2020-04-23 09:34:54 2020-04-23 09:34:55,137 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 3.39 sec

2020-04-23 09:34:55 2020-04-23 09:34:56,169 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 7.96 sec

2020-04-23 09:35:04 2020-04-23 09:35:04,417 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 14.83 sec

2020-04-23 09:35:05 MapReduce Total cumulative CPU time: 14 seconds 830 msec
Ended Job = job_1587346943308_0491

2020-04-23 09:35:05 Loading data to table yn_hadoop.play_user_day partition (y=null, m=null, d=null)

2020-04-23 09:35:05 


2020-04-23 09:35:06 	 Time taken to load dynamic partitions: 1.354 seconds
	 Time taken for adding to write entity : 0.0 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:35:07 Starting Job = job_1587346943308_0492, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0492/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0492

2020-04-23 09:35:15 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-23 09:35:15 2020-04-23 09:35:15,603 Stage-4 map = 0%,  reduce = 0%

2020-04-23 09:35:21 2020-04-23 09:35:21,782 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec

2020-04-23 09:35:28 2020-04-23 09:35:29,006 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.58 sec

2020-04-23 09:35:30 MapReduce Total cumulative CPU time: 4 seconds 580 msec
Ended Job = job_1587346943308_0492

2020-04-23 09:35:31 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 3   Cumulative CPU: 56.04 sec   HDFS Read: 702839120 HDFS Write: 7001942 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 14.83 sec   HDFS Read: 7017062 HDFS Write: 3767363 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.58 sec   HDFS Read: 54155 HDFS Write: 53485 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 15 seconds 450 msec
OK
Time taken: 87.832 seconds

2020-04-23 09:35:32 OK
Time taken: 0.096 seconds

2020-04-23 09:35:33 Query ID = hadoop_20200423093532_a2c78645-1fbe-4ee8-a2e2-09f35d7e1f9b
Total jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:35:34 Starting Job = job_1587346943308_0493, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0493/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0493

2020-04-23 09:35:41 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-23 09:35:41 2020-04-23 09:35:41,778 Stage-1 map = 0%,  reduce = 0%

2020-04-23 09:35:49 2020-04-23 09:35:50,022 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 8.57 sec

2020-04-23 09:35:54 2020-04-23 09:35:55,168 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.1 sec

2020-04-23 09:35:55 MapReduce Total cumulative CPU time: 12 seconds 100 msec

2020-04-23 09:35:55 Ended Job = job_1587346943308_0493

2020-04-23 09:35:55 Launching Job 2 out of 4

2020-04-23 09:35:55 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:35:56 Starting Job = job_1587346943308_0494, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0494/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0494

2020-04-23 09:36:07 Hadoop job information for Stage-5: number of mappers: 2; number of reducers: 1

2020-04-23 09:36:07 2020-04-23 09:36:07,360 Stage-5 map = 0%,  reduce = 0%

2020-04-23 09:36:15 2020-04-23 09:36:15,614 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 8.54 sec

2020-04-23 09:36:22 2020-04-23 09:36:22,816 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 12.17 sec

2020-04-23 09:36:24 MapReduce Total cumulative CPU time: 12 seconds 170 msec

2020-04-23 09:36:24 Ended Job = job_1587346943308_0494

2020-04-23 09:36:24 Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:36:25 Starting Job = job_1587346943308_0495, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0495/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0495

2020-04-23 09:36:35 Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2020-04-23 09:36:35,631 Stage-2 map = 0%,  reduce = 0%

2020-04-23 09:36:41 2020-04-23 09:36:41,821 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 1.87 sec

2020-04-23 09:36:42 2020-04-23 09:36:42,852 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.68 sec

2020-04-23 09:36:47 2020-04-23 09:36:48,021 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.63 sec

2020-04-23 09:36:48 MapReduce Total cumulative CPU time: 7 seconds 630 msec
Ended Job = job_1587346943308_0495
Loading data to table yn_hadoop.app_visit_count_day

2020-04-23 09:36:48 Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:36:49 Starting Job = job_1587346943308_0496, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0496/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0496

2020-04-23 09:36:59 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

2020-04-23 09:36:59 2020-04-23 09:36:59,855 Stage-4 map = 0%,  reduce = 0%

2020-04-23 09:37:05 2020-04-23 09:37:06,035 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.87 sec

2020-04-23 09:37:10 2020-04-23 09:37:11,197 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.52 sec

2020-04-23 09:37:11 MapReduce Total cumulative CPU time: 4 seconds 520 msec
Ended Job = job_1587346943308_0496

2020-04-23 09:37:12 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 12.1 sec   HDFS Read: 6944510 HDFS Write: 869 SUCCESS
Stage-Stage-5: Map: 2  Reduce: 1   Cumulative CPU: 12.17 sec   HDFS Read: 3758648 HDFS Write: 946 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 7.63 sec   HDFS Read: 21290 HDFS Write: 1566 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.52 sec   HDFS Read: 13677 HDFS Write: 823 SUCCESS
Total MapReduce CPU Time Spent: 36 seconds 420 msec
OK
Time taken: 100.201 seconds

2020-04-23 09:37:12 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:37:12 Last login: Thu Apr 23 09:32:19 2020 from 10.10.30.233

2020-04-23 09:37:12 su hadoop

2020-04-23 09:37:12 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:37:13 [hadoop@hadoop-01 root]$ 
2020-04-23 09:37:13 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:37:13 cat hive/statistic/add_user_day.hive
-- 2、天新增用户数据
use yn_hadoop;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false; --控制是否启用查询执行的向量模式
insert overwrite table add_page_user_day partition(y,m,d)
select vud.parent_column_id,vud.user_type,vud.user_id,vud.area_code,vud.create_time,substr(vud.create_time,1,4) y,substr(vud.create_time,6,2) m,substr(vud.create_time,9,2) d   from(
  select parent_column_id,user_type,user_id,area_code,min(create_time) as create_time from visit_user_page_day
  where concat(y,'-',m,'-',d)>='${startDate}' and concat(y,'-',m,'-',d)<='${endDate}' group by parent_column_id,user_type,user_id,area_code
) vud
 left join user_info aud on aud.user_id=vud.user_id and aud.user_type=vud.user_type and  aud.parent_column_id=vud.parent_column_id
 where aud.user_id is null;
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:37:13 Last login: Thu Apr 23 09:37:13 2020 from 10.10.30.233

2020-04-23 09:37:13 su hadoop

2020-04-23 09:37:13 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:37:13 [hadoop@hadoop-01 root]$ 
2020-04-23 09:37:13 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:37:13 hive -d startDate=2020-03-23 -d endDate=202 0-04-09 -f hive/statistic/add_user_day.hive

2020-04-23 09:37:13 which: no hbase in (/usr/java/jdk1.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop-3.1.2/bin:/usr/local/hadoop-3.1.2/sbin:/usr/local/hive-3.1.2/bin:/usr/local/pig-0.17.0/bin:/usr/local/sqoop-1.4.7/bin::/usr/local/flume/bin:/usr/local/kafka/bin:/root/bin)

2020-04-23 09:37:14 SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2020-04-23 09:37:16 Hive Session ID = 90137b4b-44e1-44d8-b877-77633e84744c

2020-04-23 09:37:16 
Logging initialized using configuration in file:/usr/local/hive-3.1.2/conf/hive-log4j2.properties Async: true

2020-04-23 09:37:23 Hive Session ID = c0c56cdb-e0f2-4d00-86a0-1a99e4547b2b

2020-04-23 09:37:24 OK
Time taken: 0.809 seconds

2020-04-23 09:37:27 Query ID = hadoop_20200423093724_644c12db-fca4-4880-a85a-6dd22aba289e
Total jobs = 3
Launching Job 1 out of 3

2020-04-23 09:37:27 Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:37:28 Starting Job = job_1587346943308_0497, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0497/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0497

2020-04-23 09:37:35 Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

2020-04-23 09:37:35 2020-04-23 09:37:35,356 Stage-1 map = 0%,  reduce = 0%

2020-04-23 09:37:43 2020-04-23 09:37:43,735 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.17 sec

2020-04-23 09:37:50 2020-04-23 09:37:51,013 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.48 sec

2020-04-23 09:37:51 MapReduce Total cumulative CPU time: 16 seconds 480 msec

2020-04-23 09:37:51 Ended Job = job_1587346943308_0497

2020-04-23 09:37:59 Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator

2020-04-23 09:38:00 Starting Job = job_1587346943308_0498, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0498/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0498

2020-04-23 09:38:05 Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 0

2020-04-23 09:38:05 2020-04-23 09:38:06,010 Stage-6 map = 0%,  reduce = 0%

2020-04-23 09:38:14 2020-04-23 09:38:14,296 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 5.57 sec

2020-04-23 09:38:15 MapReduce Total cumulative CPU time: 5 seconds 570 msec
Ended Job = job_1587346943308_0498

2020-04-23 09:38:15 Loading data to table yn_hadoop.add_page_user_day partition (y=null, m=null, d=null)

2020-04-23 09:38:15 


2020-04-23 09:38:16 	 Time taken to load dynamic partitions: 1.728 seconds
	 Time taken for adding to write entity : 0.003 seconds
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

2020-04-23 09:38:17 Starting Job = job_1587346943308_0499, Tracking URL = http://hadoop-01:8088/proxy/application_1587346943308_0499/
Kill Command = /usr/local/hadoop-3.1.2/bin/mapred job  -kill job_1587346943308_0499

2020-04-23 09:38:25 Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2020-04-23 09:38:25,917 Stage-4 map = 0%,  reduce = 0%

2020-04-23 09:38:31 2020-04-23 09:38:32,125 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.53 sec

2020-04-23 09:38:38 2020-04-23 09:38:38,318 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.39 sec

2020-04-23 09:38:39 MapReduce Total cumulative CPU time: 4 seconds 390 msec

2020-04-23 09:38:39 Ended Job = job_1587346943308_0499

2020-04-23 09:38:40 MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 16.48 sec   HDFS Read: 6944988 HDFS Write: 2528598 SUCCESS
Stage-Stage-6: Map: 1   Cumulative CPU: 5.57 sec   HDFS Read: 2538826 HDFS Write: 1168557 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.39 sec   HDFS Read: 39442 HDFS Write: 37846 SUCCESS
Total MapReduce CPU Time Spent: 26 seconds 440 msec
OK

2020-04-23 09:38:40 Time taken: 76.475 seconds

2020-04-23 09:38:41 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:38:41 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/reports/app/visit_count/day \
--table app_visit_count_day \
--columns t_date,parent_column_id,user_type,page_user_count,play_user_count,play_count,duration \
--update-key t_date,parent_column_id,user_type \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:38:41 Last login: Thu Apr 23 09:37:13 2020 from 10.10.30.233

2020-04-23 09:38:41 su hadoop

2020-04-23 09:38:41 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:38:41 [hadoop@hadoop-01 root]$ 
2020-04-23 09:38:41 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:38:41 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/reports/app/visit_count/day \
> --table app_visit_count_day \
> --columns t_date,parent_column_id,user_type,page_user_count,play_user_count,pl ay_count,duration \
> --update-key t_date,parent_column_id,user_type \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:38:41 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:38:42 2020-04-23 09:38:42,282 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-23 09:38:42 2020-04-23 09:38:42,309 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:38:42 2020-04-23 09:38:42,387 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:38:42,391 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:38:42 2020-04-23 09:38:42,594 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `app_visit_count_day` AS t LIMIT 1

2020-04-23 09:38:42 2020-04-23 09:38:42,609 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `app_visit_count_day` AS t LIMIT 1

2020-04-23 09:38:42 2020-04-23 09:38:42,615 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:38:43 注: /tmp/sqoop-hadoop/compile/8941378551a8e27b1db2c954aa4cab05/app_visit_count_day.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:38:43,647 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/8941378551a8e27b1db2c954aa4cab05/app_visit_count_day.jar
2020-04-23 09:38:43,655 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:38:43,655 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:38:43,655 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:38:43,655 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:38:43,655 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-23 09:38:43,662 INFO mapreduce.ExportJobBase: Beginning export of app_visit_count_day
2020-04-23 09:38:43,662 INFO Confi
2020-04-23 09:38:43 guration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:38:43 2020-04-23 09:38:43,768 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:38:44 2020-04-23 09:38:44,515 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:38:44,517 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:38:44,517 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:38:44 2020-04-23 09:38:44,958 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0500

2020-04-23 09:38:46 2020-04-23 09:38:46,664 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:38:46,667 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:38:46 2020-04-23 09:38:46,721 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:38:46 2020-04-23 09:38:46,751 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:38:46 2020-04-23 09:38:46,868 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0500
2020-04-23 09:38:46,870 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:38:46 2020-04-23 09:38:47,078 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:38:47,079 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:38:46 2020-04-23 09:38:47,144 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0500

2020-04-23 09:38:46 2020-04-23 09:38:47,174 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0500/
2020-04-23 09:38:47,175 INFO mapreduce.Job: Running job: job_1587346943308_0500

2020-04-23 09:38:53 2020-04-23 09:38:53,264 INFO mapreduce.Job: Job job_1587346943308_0500 running in uber mode : false
2020-04-23 09:38:53,267 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:39:00 2020-04-23 09:39:00,346 INFO mapreduce.Job:  map 50% reduce 0%

2020-04-23 09:39:01 2020-04-23 09:39:01,352 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:39:01,362 INFO mapreduce.Job: Job job_1587346943308_0500 completed successfully

2020-04-23 09:39:01 2020-04-23 09:39:01,475 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928672
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=2873
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=21162
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=21162
		Total vcore-milliseconds taken by all map tasks=21162
		Total megabyte-milliseconds taken by all map tasks=21669888
	Map-Reduce Framework
		Map input records=18
		Map output records=18
		Input split bytes=681
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=821
		CPU time spent (
2020-04-23 09:39:01 ms)=3010
		Physical memory (bytes) snapshot=962908160
		Virtual memory (bytes) snapshot=9768677376
		Total committed heap usage (bytes)=745537536
		Peak Map Physical memory (bytes)=242315264
		Peak Map Virtual memory (bytes)=2445692928
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:39:01,482 INFO mapreduce.ExportJobBase: Transferred 2.8057 KB in 16.9556 seconds (169.4421 bytes/sec)
2020-04-23 09:39:01,486 INFO mapreduce.ExportJobBase: Exported 18 records.

2020-04-23 09:39:01 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:39:01 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:39:01 Last login: Thu Apr 23 09:38:41 2020 from 10.10.30.233

2020-04-23 09:39:01 su hadoop

2020-04-23 09:39:01 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:39:01 [hadoop@hadoop-01 root]$ 
2020-04-23 09:39:01 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:39:01 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=23/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:39:01 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:39:02 2020-04-23 09:39:02,751 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-23 09:39:02 2020-04-23 09:39:02,778 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:39:02 2020-04-23 09:39:02,858 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:39:02,861 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:39:02 2020-04-23 09:39:03,109 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:39:03,124 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:39:02 2020-04-23 09:39:03,129 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:39:03 注: /tmp/sqoop-hadoop/compile/3a7864b19a1c07cfea8b507ccc6cf710/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:39:04,093 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/3a7864b19a1c07cfea8b507ccc6cf710/user_info.jar
2020-04-23 09:39:04,101 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:39:04,101 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:39:04,101 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:39:04,101 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:39:04,101 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-23 09:39:03 2020-04-23 09:39:04,107 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:39:04,107 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:39:03 2020-04-23 09:39:04,210 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:39:04 2020-04-23 09:39:04,998 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:39:05,000 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:39:05,000 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:39:05 2020-04-23 09:39:05,417 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0501

2020-04-23 09:39:06 2020-04-23 09:39:06,675 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:39:06,678 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:39:06 2020-04-23 09:39:06,724 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:39:06 2020-04-23 09:39:06,759 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:39:06 2020-04-23 09:39:06,857 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0501
2020-04-23 09:39:06,859 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:39:06 2020-04-23 09:39:07,028 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:39:07,028 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:39:06 2020-04-23 09:39:07,074 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0501

2020-04-23 09:39:06 2020-04-23 09:39:07,099 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0501/
2020-04-23 09:39:07,099 INFO mapreduce.Job: Running job: job_1587346943308_0501

2020-04-23 09:39:12 2020-04-23 09:39:13,168 INFO mapreduce.Job: Job job_1587346943308_0501 running in uber mode : false
2020-04-23 09:39:13,170 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:39:20 2020-04-23 09:39:20,251 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:39:20,261 INFO mapreduce.Job: Job job_1587346943308_0501 completed successfully

2020-04-23 09:39:20 2020-04-23 09:39:20,371 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=503452
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20734
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20734
		Total vcore-milliseconds taken by all map tasks=20734
		Total megabyte-milliseconds taken by all map tasks=21231616
	Map-Reduce Framework
		Map input records=11333
		Map output records=11333
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=829
		CPU time
2020-04-23 09:39:20  spent (ms)=3860
		Physical memory (bytes) snapshot=965828608
		Virtual memory (bytes) snapshot=9779224576
		Total committed heap usage (bytes)=751304704
		Peak Map Physical memory (bytes)=244498432
		Peak Map Virtual memory (bytes)=2445660160
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:39:20,378 INFO mapreduce.ExportJobBase: Transferred 491.6523 KB in 15.3696 seconds (31.9887 KB/sec)
2020-04-23 09:39:20,382 INFO mapreduce.ExportJobBase: Exported 11333 records.

2020-04-23 09:39:20 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:39:20 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:39:20 Last login: Thu Apr 23 09:39:02 2020 from 10.10.30.233

2020-04-23 09:39:20 su hadoop

2020-04-23 09:39:20 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:39:20 [hadoop@hadoop-01 root]$ 
2020-04-23 09:39:20 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:39:20 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=24/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:39:20 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:39:21 2020-04-23 09:39:21,630 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-23 09:39:21,656 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:39:21 2020-04-23 09:39:21,734 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:39:21,737 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:39:21 2020-04-23 09:39:21,939 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:39:21,956 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:39:21 2020-04-23 09:39:21,961 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:39:22 注: /tmp/sqoop-hadoop/compile/55ff2093e786b7bf6bd7459b303abba6/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。

2020-04-23 09:39:22 2020-04-23 09:39:22,936 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/55ff2093e786b7bf6bd7459b303abba6/user_info.jar
2020-04-23 09:39:22,944 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:39:22,944 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:39:22,944 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:39:22,944 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:39:22,944 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-23 09:39:22 2020-04-23 09:39:22,950 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:39:22,950 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:39:22 2020-04-23 09:39:23,052 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:39:23 2020-04-23 09:39:23,811 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:39:23,812 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:39:23,813 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:39:23 2020-04-23 09:39:24,229 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0502

2020-04-23 09:39:24 2020-04-23 09:39:25,067 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:39:25,069 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:39:24 2020-04-23 09:39:25,114 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:39:24 2020-04-23 09:39:25,144 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:39:25 2020-04-23 09:39:25,218 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0502
2020-04-23 09:39:25,220 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:39:25 2020-04-23 09:39:25,414 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:39:25,414 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:39:25 2020-04-23 09:39:25,467 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0502

2020-04-23 09:39:25 2020-04-23 09:39:25,493 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0502/
2020-04-23 09:39:25,493 INFO mapreduce.Job: Running job: job_1587346943308_0502

2020-04-23 09:39:31 2020-04-23 09:39:31,575 INFO mapreduce.Job: Job job_1587346943308_0502 running in uber mode : false
2020-04-23 09:39:31,576 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:39:38 2020-04-23 09:39:38,763 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-23 09:39:39 2020-04-23 09:39:39,778 INFO mapreduce.Job: Job job_1587346943308_0502 completed successfully

2020-04-23 09:39:39 2020-04-23 09:39:39,893 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=395458
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20398
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20398
		Total vcore-milliseconds taken by all map tasks=20398
		Total megabyte-milliseconds taken by all map tasks=20887552
	Map-Reduce Framework
		Map input records=4555
		Map output records=4555
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=749
		CPU time s
2020-04-23 09:39:39 pent (ms)=3550
		Physical memory (bytes) snapshot=959041536
		Virtual memory (bytes) snapshot=9783144448
		Total committed heap usage (bytes)=756547584
		Peak Map Physical memory (bytes)=242847744
		Peak Map Virtual memory (bytes)=2448560128
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:39:39,900 INFO mapreduce.ExportJobBase: Transferred 386.1895 KB in 16.0778 seconds (24.02 KB/sec)
2020-04-23 09:39:39,904 INFO mapreduce.ExportJobBase: Exported 4555 records.

2020-04-23 09:39:40 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:39:40 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=25/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:39:40 Last login: Thu Apr 23 09:39:21 2020 from 10.10.30.233

2020-04-23 09:39:40 su hadoop

2020-04-23 09:39:40 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:39:40 [hadoop@hadoop-01 root]$ 
2020-04-23 09:39:40 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:39:40 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=25/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:39:40 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:39:40 2020-04-23 09:39:41,180 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-23 09:39:41,205 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:39:41 2020-04-23 09:39:41,284 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:39:41,287 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:39:41 2020-04-23 09:39:41,489 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:39:41,503 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:39:41,509 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:39:42 注: /tmp/sqoop-hadoop/compile/649060632b07cb502a1d67a51fde27d2/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:39:42,528 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/649060632b07cb502a1d67a51fde27d2/user_info.jar
2020-04-23 09:39:42,535 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:39:42,535 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:39:42,535 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:39:42,535 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:39:42,535 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-23 09:39:42 2020-04-23 09:39:42,540 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:39:42,541 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:39:42 2020-04-23 09:39:42,644 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:39:43 2020-04-23 09:39:43,395 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:39:43,397 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:39:43,397 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:39:43 2020-04-23 09:39:43,814 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0503

2020-04-23 09:39:45 2020-04-23 09:39:45,957 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:39:45,959 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:39:45 2020-04-23 09:39:46,009 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:39:45 2020-04-23 09:39:46,037 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:39:46 2020-04-23 09:39:46,518 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0503
2020-04-23 09:39:46,521 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:39:46 2020-04-23 09:39:46,717 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:39:46,717 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:39:46 2020-04-23 09:39:46,763 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0503
2020-04-23 09:39:46,790 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0503/
2020-04-23 09:39:46,790 INFO mapreduce.Job: Running job: job_1587346943308_0503

2020-04-23 09:39:52 2020-04-23 09:39:52,878 INFO mapreduce.Job: Job job_1587346943308_0503 running in uber mode : false
2020-04-23 09:39:52,880 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:39:59 2020-04-23 09:40:00,026 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:40:00,036 INFO mapreduce.Job: Job job_1587346943308_0503 completed successfully

2020-04-23 09:39:59 2020-04-23 09:40:00,154 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=346440
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20345
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20345
		Total vcore-milliseconds taken by all map tasks=20345
		Total megabyte-milliseconds taken by all map tasks=20833280
	Map-Reduce Framework
		Map input records=3393
		Map output records=3393
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=713
		CPU time s
2020-04-23 09:39:59 pent (ms)=3430
		Physical memory (bytes) snapshot=923484160
		Virtual memory (bytes) snapshot=9780363264
		Total committed heap usage (bytes)=709361664
		Peak Map Physical memory (bytes)=247394304
		Peak Map Virtual memory (bytes)=2446438400
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:40:00,161 INFO mapreduce.ExportJobBase: Transferred 338.3203 KB in 16.7549 seconds (20.1924 KB/sec)
2020-04-23 09:40:00,165 INFO mapreduce.ExportJobBase: Exported 3393 records.

2020-04-23 09:40:00 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:40:00 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=26/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:40:00 Last login: Thu Apr 23 09:39:40 2020 from 10.10.30.233

2020-04-23 09:40:00 su hadoop

2020-04-23 09:40:00 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:40:00 [hadoop@hadoop-01 root]$ 
2020-04-23 09:40:00 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:40:00 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=26/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:40:00 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:40:01 2020-04-23 09:40:01,491 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-23 09:40:01 2020-04-23 09:40:01,530 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:40:01 2020-04-23 09:40:01,615 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:40:01,618 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:40:01 2020-04-23 09:40:01,827 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:40:01,851 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:40:01,855 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:40:02 注: /tmp/sqoop-hadoop/compile/6b1856ed0fdb286a6c56aaa290b29b98/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:40:02,818 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/6b1856ed0fdb286a6c56aaa290b29b98/user_info.jar
2020-04-23 09:40:02,825 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:40:02,825 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:40:02,825 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:40:02,825 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:40:02,825 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-23 09:40:02,831 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:40:02,831 INFO Configuration.deprecation: mapred.j
2020-04-23 09:40:02 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:40:02 2020-04-23 09:40:02,934 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:40:03 2020-04-23 09:40:03,689 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:40:03,691 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:40:03,691 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:40:03 2020-04-23 09:40:04,110 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0504

2020-04-23 09:40:05 2020-04-23 09:40:05,361 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:40:05,364 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:40:05 2020-04-23 09:40:05,411 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:40:05 2020-04-23 09:40:05,445 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:40:05 2020-04-23 09:40:05,928 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0504
2020-04-23 09:40:05,931 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:40:05 2020-04-23 09:40:06,121 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:40:06,121 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:40:05 2020-04-23 09:40:06,169 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0504

2020-04-23 09:40:05 2020-04-23 09:40:06,196 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0504/
2020-04-23 09:40:06,196 INFO mapreduce.Job: Running job: job_1587346943308_0504

2020-04-23 09:40:12 2020-04-23 09:40:12,284 INFO mapreduce.Job: Job job_1587346943308_0504 running in uber mode : false
2020-04-23 09:40:12,285 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:40:19 2020-04-23 09:40:19,364 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:40:19,373 INFO mapreduce.Job: Job job_1587346943308_0504 completed successfully

2020-04-23 09:40:19 2020-04-23 09:40:19,488 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=287072
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=21068
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=21068
		Total vcore-milliseconds taken by all map tasks=21068
		Total megabyte-milliseconds taken by all map tasks=21573632
	Map-Reduce Framework
		Map input records=2802
		Map output records=2802
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=699
		CPU time s
2020-04-23 09:40:19 pent (ms)=3290
		Physical memory (bytes) snapshot=962248704
		Virtual memory (bytes) snapshot=9768226816
		Total committed heap usage (bytes)=723517440
		Peak Map Physical memory (bytes)=259530752
		Peak Map Virtual memory (bytes)=2445316096
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:40:19,496 INFO mapreduce.ExportJobBase: Transferred 280.3438 KB in 15.7955 seconds (17.7483 KB/sec)
2020-04-23 09:40:19,500 INFO mapreduce.ExportJobBase: Exported 2802 records.

2020-04-23 09:40:19 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:40:19 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=27/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:40:19 Last login: Thu Apr 23 09:40:00 2020 from 10.10.30.233

2020-04-23 09:40:19 su hadoop

2020-04-23 09:40:19 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:40:19 [hadoop@hadoop-01 root]$ 
2020-04-23 09:40:19 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:40:19 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=27/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:40:19 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:40:20 2020-04-23 09:40:20,749 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-23 09:40:20 2020-04-23 09:40:20,774 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:40:20 2020-04-23 09:40:20,851 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:40:20,854 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:40:20 2020-04-23 09:40:21,054 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:40:20 2020-04-23 09:40:21,070 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:40:20 2020-04-23 09:40:21,076 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:40:21 注: /tmp/sqoop-hadoop/compile/afdb097c01e4ecdbce6e50a54e87e25c/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:40:22,030 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/afdb097c01e4ecdbce6e50a54e87e25c/user_info.jar
2020-04-23 09:40:22,038 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:40:22,038 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:40:22,038 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:40:22,038 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:40:22,038 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-23 09:40:22,044 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:40:22,044 INFO Configuration.deprecation: mapred.j
2020-04-23 09:40:21 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:40:21 2020-04-23 09:40:22,151 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:40:22 2020-04-23 09:40:22,981 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:40:22,982 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:40:22,983 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:40:23 2020-04-23 09:40:23,416 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0505

2020-04-23 09:40:24 2020-04-23 09:40:25,137 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:40:24 2020-04-23 09:40:25,140 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:40:24 2020-04-23 09:40:25,202 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:40:24 2020-04-23 09:40:25,230 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:40:25 2020-04-23 09:40:25,302 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0505
2020-04-23 09:40:25,305 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:40:25 2020-04-23 09:40:25,512 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:40:25,512 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:40:25 2020-04-23 09:40:25,562 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0505

2020-04-23 09:40:25 2020-04-23 09:40:25,590 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0505/
2020-04-23 09:40:25,590 INFO mapreduce.Job: Running job: job_1587346943308_0505

2020-04-23 09:40:31 2020-04-23 09:40:31,675 INFO mapreduce.Job: Job job_1587346943308_0505 running in uber mode : false
2020-04-23 09:40:31,677 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:40:38 2020-04-23 09:40:38,869 INFO mapreduce.Job:  map 75% reduce 0%

2020-04-23 09:40:39 2020-04-23 09:40:39,875 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:40:39,885 INFO mapreduce.Job: Job job_1587346943308_0505 completed successfully

2020-04-23 09:40:39 2020-04-23 09:40:39,998 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=283490
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20143
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20143
		Total vcore-milliseconds taken by all map tasks=20143
		Total megabyte-milliseconds taken by all map tasks=20626432
	Map-Reduce Framework
		Map input records=2767
		Map output records=2767
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=906
		CPU time s
2020-04-23 09:40:39 pent (ms)=3350
		Physical memory (bytes) snapshot=974835712
		Virtual memory (bytes) snapshot=9785823232
		Total committed heap usage (bytes)=756547584
		Peak Map Physical memory (bytes)=246824960
		Peak Map Virtual memory (bytes)=2449637376
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:40:40,005 INFO mapreduce.ExportJobBase: Transferred 276.8457 KB in 17.0139 seconds (16.2718 KB/sec)
2020-04-23 09:40:40,010 INFO mapreduce.ExportJobBase: Exported 2767 records.

2020-04-23 09:40:40 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:40:40 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=28/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:40:40 Last login: Thu Apr 23 09:40:20 2020 from 10.10.30.233

2020-04-23 09:40:40 su hadoop

2020-04-23 09:40:40 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:40:40 [hadoop@hadoop-01 root]$ 
2020-04-23 09:40:40 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:40:40 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=28/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:40:40 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:40:41 2020-04-23 09:40:41,278 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-23 09:40:41 2020-04-23 09:40:41,313 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:40:41 2020-04-23 09:40:41,418 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:40:41,423 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:40:41 2020-04-23 09:40:41,664 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:40:41,685 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:40:41,691 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:40:42 注: /tmp/sqoop-hadoop/compile/e6c99865c7fe09446a85e44c2b7db346/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:40:42,643 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/e6c99865c7fe09446a85e44c2b7db346/user_info.jar
2020-04-23 09:40:42,650 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:40:42,650 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:40:42,650 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:40:42,650 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:40:42,650 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-23 09:40:42,655 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:40:42,655 INFO Configuration.deprecation: mapred.j
2020-04-23 09:40:42 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:40:42 2020-04-23 09:40:42,764 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:40:43 2020-04-23 09:40:43,572 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:40:43,574 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:40:43,575 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:40:43 2020-04-23 09:40:44,156 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0506

2020-04-23 09:40:46 2020-04-23 09:40:46,262 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:40:46,264 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:40:46 2020-04-23 09:40:46,312 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:40:46 2020-04-23 09:40:46,340 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:40:46 2020-04-23 09:40:46,418 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0506
2020-04-23 09:40:46,421 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:40:46 2020-04-23 09:40:46,631 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:40:46,631 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-04-23 09:40:46,684 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0506

2020-04-23 09:40:46 2020-04-23 09:40:46,711 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0506/
2020-04-23 09:40:46,711 INFO mapreduce.Job: Running job: job_1587346943308_0506

2020-04-23 09:40:52 2020-04-23 09:40:52,790 INFO mapreduce.Job: Job job_1587346943308_0506 running in uber mode : false
2020-04-23 09:40:52,792 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:41:00 2020-04-23 09:41:00,882 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:41:00,892 INFO mapreduce.Job: Job job_1587346943308_0506 completed successfully

2020-04-23 09:41:00 2020-04-23 09:41:01,003 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=273578
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20397
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20397
		Total vcore-milliseconds taken by all map tasks=20397
		Total megabyte-milliseconds taken by all map tasks=20886528
	Map-Reduce Framework
		Map input records=2670
		Map output records=2670
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=807
		CPU time s
2020-04-23 09:41:00 pent (ms)=3370
		Physical memory (bytes) snapshot=966152192
		Virtual memory (bytes) snapshot=9782767616
		Total committed heap usage (bytes)=749731840
		Peak Map Physical memory (bytes)=243429376
		Peak Map Virtual memory (bytes)=2447831040
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:41:01,011 INFO mapreduce.ExportJobBase: Transferred 267.166 KB in 17.4251 seconds (15.3322 KB/sec)
2020-04-23 09:41:01,016 INFO mapreduce.ExportJobBase: Exported 2670 records.

2020-04-23 09:41:01 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:41:01 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=29/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:41:01 Last login: Thu Apr 23 09:40:40 2020 from 10.10.30.233

2020-04-23 09:41:01 su hadoop

2020-04-23 09:41:01 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:41:01 [hadoop@hadoop-01 root]$ 
2020-04-23 09:41:01 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:41:01 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=29/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:41:01 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:41:02 2020-04-23 09:41:02,295 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-23 09:41:02,321 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:41:02 2020-04-23 09:41:02,395 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:41:02,399 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:41:02 2020-04-23 09:41:02,602 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:41:02,623 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:41:02 2020-04-23 09:41:02,630 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:41:03 注: /tmp/sqoop-hadoop/compile/6ce2894cc26192a4619b6cbb4f5e40b0/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:41:03,785 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/6ce2894cc26192a4619b6cbb4f5e40b0/user_info.jar
2020-04-23 09:41:03,792 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:41:03,792 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:41:03,792 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:41:03,792 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:41:03,792 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-23 09:41:03,798 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:41:03,798 INFO Configuration.deprecation: mapred.j
2020-04-23 09:41:03 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:41:03 2020-04-23 09:41:03,899 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:41:04 2020-04-23 09:41:04,631 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:41:04,633 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:41:04,633 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:41:04 2020-04-23 09:41:05,077 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0507

2020-04-23 09:41:06 2020-04-23 09:41:06,810 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:41:06,812 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:41:06 2020-04-23 09:41:06,860 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:41:06 2020-04-23 09:41:06,888 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:41:06 2020-04-23 09:41:06,963 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0507
2020-04-23 09:41:06,964 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:41:06 2020-04-23 09:41:07,134 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:41:07,134 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:41:06 2020-04-23 09:41:07,191 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0507

2020-04-23 09:41:06 2020-04-23 09:41:07,225 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0507/
2020-04-23 09:41:07,225 INFO mapreduce.Job: Running job: job_1587346943308_0507

2020-04-23 09:41:12 2020-04-23 09:41:12,300 INFO mapreduce.Job: Job job_1587346943308_0507 running in uber mode : false
2020-04-23 09:41:12,302 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:41:19 2020-04-23 09:41:19,526 INFO mapreduce.Job:  map 75% reduce 0%

2020-04-23 09:41:20 2020-04-23 09:41:20,533 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:41:20,545 INFO mapreduce.Job: Job job_1587346943308_0507 completed successfully

2020-04-23 09:41:20 2020-04-23 09:41:20,652 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=230255
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=19662
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=19662
		Total vcore-milliseconds taken by all map tasks=19662
		Total megabyte-milliseconds taken by all map tasks=20133888
	Map-Reduce Framework
		Map input records=2246
		Map output records=2246
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=803
		CPU time s
2020-04-23 09:41:20 pent (ms)=3270
		Physical memory (bytes) snapshot=951943168
		Virtual memory (bytes) snapshot=9782525952
		Total committed heap usage (bytes)=754450432
		Peak Map Physical memory (bytes)=241274880
		Peak Map Virtual memory (bytes)=2449756160
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:41:20,660 INFO mapreduce.ExportJobBase: Transferred 224.8584 KB in 16.0171 seconds (14.0386 KB/sec)
2020-04-23 09:41:20,664 INFO mapreduce.ExportJobBase: Exported 2246 records.

2020-04-23 09:41:20 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:41:20 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=30/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:41:21 Last login: Thu Apr 23 09:41:01 2020 from 10.10.30.233

2020-04-23 09:41:21 su hadoop

2020-04-23 09:41:21 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:41:21 [hadoop@hadoop-01 root]$ 
2020-04-23 09:41:21 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:41:21 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=30/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:41:21 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:41:21 2020-04-23 09:41:21,960 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-23 09:41:21,986 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:41:21 2020-04-23 09:41:22,068 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.

2020-04-23 09:41:21 2020-04-23 09:41:22,071 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:41:22 2020-04-23 09:41:22,314 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:41:22,330 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:41:22,336 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:41:23 注: /tmp/sqoop-hadoop/compile/f5f59da4d7b6152a7bf3f3d967dbe878/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:41:23,296 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/f5f59da4d7b6152a7bf3f3d967dbe878/user_info.jar
2020-04-23 09:41:23,303 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:41:23,303 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:41:23,303 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:41:23,303 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:41:23,303 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-23 09:41:23,308 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:41:23,309 INFO Configuration.deprecation: mapred.j
2020-04-23 09:41:23 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:41:23 2020-04-23 09:41:23,412 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:41:23 2020-04-23 09:41:24,150 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:41:24,152 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:41:24,152 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:41:24 2020-04-23 09:41:24,601 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0508

2020-04-23 09:41:25 2020-04-23 09:41:25,427 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:41:25,429 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:41:25 2020-04-23 09:41:25,479 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:41:25 2020-04-23 09:41:25,509 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:41:25 2020-04-23 09:41:25,590 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0508
2020-04-23 09:41:25,591 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:41:25 2020-04-23 09:41:25,772 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:41:25,773 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:41:25 2020-04-23 09:41:25,819 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0508

2020-04-23 09:41:25 2020-04-23 09:41:25,845 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0508/
2020-04-23 09:41:25,846 INFO mapreduce.Job: Running job: job_1587346943308_0508

2020-04-23 09:41:31 2020-04-23 09:41:31,934 INFO mapreduce.Job: Job job_1587346943308_0508 running in uber mode : false
2020-04-23 09:41:31,936 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:41:38 2020-04-23 09:41:39,133 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-23 09:41:38 2020-04-23 09:41:39,143 INFO mapreduce.Job: Job job_1587346943308_0508 completed successfully

2020-04-23 09:41:39 2020-04-23 09:41:39,269 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=189474
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=19251
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=19251
		Total vcore-milliseconds taken by all map tasks=19251
		Total megabyte-milliseconds taken by all map tasks=19713024
	Map-Reduce Framework
		Map input records=1847
		Map output records=1847
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=685
		CPU time s
2020-04-23 09:41:39 pent (ms)=3100
		Physical memory (bytes) snapshot=924688384
		Virtual memory (bytes) snapshot=9782435840
		Total committed heap usage (bytes)=716701696
		Peak Map Physical memory (bytes)=243126272
		Peak Map Virtual memory (bytes)=2446422016
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:41:39,275 INFO mapreduce.ExportJobBase: Transferred 185.0332 KB in 15.1153 seconds (12.2415 KB/sec)
2020-04-23 09:41:39,278 INFO mapreduce.ExportJobBase: Exported 1847 records.

2020-04-23 09:41:39 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:41:39 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=03/d=31/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:41:39 Last login: Thu Apr 23 09:41:21 2020 from 10.10.30.233

2020-04-23 09:41:39 su hadoop

2020-04-23 09:41:39 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:41:39 [hadoop@hadoop-01 root]$ 
2020-04-23 09:41:39 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:41:39 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=03/d=31/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:41:39 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:41:40 2020-04-23 09:41:40,563 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-23 09:41:40 2020-04-23 09:41:40,588 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:41:40 2020-04-23 09:41:40,667 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:41:40,670 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:41:40 2020-04-23 09:41:40,909 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:41:40 2020-04-23 09:41:40,924 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:41:40 2020-04-23 09:41:40,930 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:41:41 注: /tmp/sqoop-hadoop/compile/e95360dfa614422f765947d55375c5e3/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:41:41,880 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/e95360dfa614422f765947d55375c5e3/user_info.jar
2020-04-23 09:41:41,890 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:41:41,890 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:41:41,890 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:41:41,890 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:41:41,890 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-23 09:41:41,897 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:41:41,898 INFO Configuration.deprecation: mapred.j
2020-04-23 09:41:41 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:41:41 2020-04-23 09:41:42,013 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:41:42 2020-04-23 09:41:42,756 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:41:42,758 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:41:42,758 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:41:42 2020-04-23 09:41:43,181 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0509

2020-04-23 09:41:44 2020-04-23 09:41:44,480 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:41:44,483 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:41:44 2020-04-23 09:41:44,529 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:41:44 2020-04-23 09:41:44,557 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:41:44 2020-04-23 09:41:44,646 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0509
2020-04-23 09:41:44,648 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:41:44 2020-04-23 09:41:44,810 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:41:44,810 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:41:44 2020-04-23 09:41:44,862 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0509

2020-04-23 09:41:44 2020-04-23 09:41:44,888 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0509/
2020-04-23 09:41:44,888 INFO mapreduce.Job: Running job: job_1587346943308_0509

2020-04-23 09:41:50 2020-04-23 09:41:50,978 INFO mapreduce.Job: Job job_1587346943308_0509 running in uber mode : false
2020-04-23 09:41:50,980 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:41:57 2020-04-23 09:41:58,181 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:41:58,192 INFO mapreduce.Job: Job job_1587346943308_0509 completed successfully

2020-04-23 09:41:58 2020-04-23 09:41:58,298 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=167360
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=19251
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=19251
		Total vcore-milliseconds taken by all map tasks=19251
		Total megabyte-milliseconds taken by all map tasks=19713024
	Map-Reduce Framework
		Map input records=1713
		Map output records=1713
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=728
		CPU time s
2020-04-23 09:41:58 pent (ms)=2950
		Physical memory (bytes) snapshot=957022208
		Virtual memory (bytes) snapshot=9780469760
		Total committed heap usage (bytes)=760741888
		Peak Map Physical memory (bytes)=242409472
		Peak Map Virtual memory (bytes)=2446700544
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:41:58,303 INFO mapreduce.ExportJobBase: Transferred 163.4375 KB in 15.5374 seconds (10.519 KB/sec)
2020-04-23 09:41:58,306 INFO mapreduce.ExportJobBase: Exported 1713 records.

2020-04-23 09:41:58 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:41:58 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=04/d=01/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:41:58 Last login: Thu Apr 23 09:41:39 2020 from 10.10.30.233

2020-04-23 09:41:58 su hadoop

2020-04-23 09:41:58 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:41:58 [hadoop@hadoop-01 root]$ 
2020-04-23 09:41:58 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:41:58 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=04/d=01/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:41:58 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:41:59 2020-04-23 09:41:59,514 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-23 09:41:59 2020-04-23 09:41:59,539 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:41:59 2020-04-23 09:41:59,616 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:41:59,620 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:41:59 2020-04-23 09:41:59,822 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:41:59 2020-04-23 09:41:59,837 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:41:59 2020-04-23 09:41:59,842 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:42:00 注: /tmp/sqoop-hadoop/compile/f695b5d876b8bda9e8840c72fc904ce0/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:42:00,825 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/f695b5d876b8bda9e8840c72fc904ce0/user_info.jar
2020-04-23 09:42:00,832 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:42:00,832 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:42:00,832 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:42:00,832 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:42:00,832 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-23 09:42:00,837 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:42:00,838 INFO Configuration.deprecation: mapred.j
2020-04-23 09:42:00 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:42:00 2020-04-23 09:42:00,947 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:42:01 2020-04-23 09:42:01,721 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:42:01,723 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:42:01,724 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:42:01 2020-04-23 09:42:02,153 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0510

2020-04-23 09:42:03 2020-04-23 09:42:03,414 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:42:03,416 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:42:03 2020-04-23 09:42:03,462 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:42:03 2020-04-23 09:42:03,490 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:42:03 2020-04-23 09:42:03,987 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0510
2020-04-23 09:42:03,989 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:42:03 2020-04-23 09:42:04,192 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:42:04,192 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:42:03 2020-04-23 09:42:04,236 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0510

2020-04-23 09:42:03 2020-04-23 09:42:04,269 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0510/

2020-04-23 09:42:04 2020-04-23 09:42:04,270 INFO mapreduce.Job: Running job: job_1587346943308_0510

2020-04-23 09:42:10 2020-04-23 09:42:10,364 INFO mapreduce.Job: Job job_1587346943308_0510 running in uber mode : false
2020-04-23 09:42:10,366 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:42:17 2020-04-23 09:42:17,446 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:42:17,457 INFO mapreduce.Job: Job job_1587346943308_0510 completed successfully

2020-04-23 09:42:17 2020-04-23 09:42:17,572 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=157511
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20007
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20007
		Total vcore-milliseconds taken by all map tasks=20007
		Total megabyte-milliseconds taken by all map tasks=20487168
	Map-Reduce Framework
		Map input records=1534
		Map output records=1534
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=719
		CPU time s
2020-04-23 09:42:17 pent (ms)=2990
		Physical memory (bytes) snapshot=935424000
		Virtual memory (bytes) snapshot=9777651712
		Total committed heap usage (bytes)=754450432
		Peak Map Physical memory (bytes)=242307072
		Peak Map Virtual memory (bytes)=2446041088
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0

2020-04-23 09:42:17 2020-04-23 09:42:17,579 INFO mapreduce.ExportJobBase: Transferred 153.8193 KB in 15.8467 seconds (9.7067 KB/sec)

2020-04-23 09:42:17 2020-04-23 09:42:17,583 INFO mapreduce.ExportJobBase: Exported 1534 records.

2020-04-23 09:42:17 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:42:17 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=04/d=02/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:42:17 Last login: Thu Apr 23 09:41:58 2020 from 10.10.30.233

2020-04-23 09:42:17 su hadoop

2020-04-23 09:42:17 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:42:17 [hadoop@hadoop-01 root]$ 
2020-04-23 09:42:17 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:42:18 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=04/d=02/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:42:18 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:42:18 2020-04-23 09:42:18,804 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-23 09:42:18,829 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:42:18 2020-04-23 09:42:18,910 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:42:18,913 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:42:18 2020-04-23 09:42:19,152 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:42:19,169 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:42:19,174 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:42:19 注: /tmp/sqoop-hadoop/compile/b8b4b8bf09867292384f33db17b697d1/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:42:20,148 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/b8b4b8bf09867292384f33db17b697d1/user_info.jar
2020-04-23 09:42:20,156 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:42:20,156 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:42:20,156 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:42:20,156 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:42:20,156 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-23 09:42:19 2020-04-23 09:42:20,162 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:42:20,163 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:42:20 2020-04-23 09:42:20,264 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:42:20 2020-04-23 09:42:21,053 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

2020-04-23 09:42:20 2020-04-23 09:42:21,055 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:42:21,055 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:42:21 2020-04-23 09:42:21,483 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0511

2020-04-23 09:42:22 2020-04-23 09:42:23,174 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:42:23,176 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:42:22 2020-04-23 09:42:23,225 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:42:22 2020-04-23 09:42:23,254 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:42:23 2020-04-23 09:42:23,332 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0511
2020-04-23 09:42:23,333 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:42:23 2020-04-23 09:42:23,494 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:42:23,495 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:42:23 2020-04-23 09:42:23,540 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0511

2020-04-23 09:42:23 2020-04-23 09:42:23,566 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0511/
2020-04-23 09:42:23,566 INFO mapreduce.Job: Running job: job_1587346943308_0511

2020-04-23 09:42:29 2020-04-23 09:42:29,665 INFO mapreduce.Job: Job job_1587346943308_0511 running in uber mode : false
2020-04-23 09:42:29,667 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:42:36 2020-04-23 09:42:36,753 INFO mapreduce.Job:  map 50% reduce 0%

2020-04-23 09:42:37 2020-04-23 09:42:37,759 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:42:37,769 INFO mapreduce.Job: Job job_1587346943308_0511 completed successfully

2020-04-23 09:42:37 2020-04-23 09:42:37,885 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=166815
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=22468
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=22468
		Total vcore-milliseconds taken by all map tasks=22468
		Total megabyte-milliseconds taken by all map tasks=23007232
	Map-Reduce Framework
		Map input records=1625
		Map output records=1625
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=829
		CPU time s
2020-04-23 09:42:37 pent (ms)=3020
		Physical memory (bytes) snapshot=949399552
		Virtual memory (bytes) snapshot=9780797440
		Total committed heap usage (bytes)=737148928
		Peak Map Physical memory (bytes)=241905664
		Peak Map Virtual memory (bytes)=2446151680
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:42:37,892 INFO mapreduce.ExportJobBase: Transferred 162.9053 KB in 16.8284 seconds (9.6804 KB/sec)
2020-04-23 09:42:37,896 INFO mapreduce.ExportJobBase: Exported 1625 records.

2020-04-23 09:42:37 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:42:38 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=04/d=03/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:42:38 Last login: Thu Apr 23 09:42:18 2020 from 10.10.30.233

2020-04-23 09:42:38 su hadoop

2020-04-23 09:42:38 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:42:38 [hadoop@hadoop-01 root]$ 
2020-04-23 09:42:38 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:42:38 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=04/d=03/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:42:38 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:42:38 2020-04-23 09:42:39,146 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-23 09:42:38 2020-04-23 09:42:39,171 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:42:38 2020-04-23 09:42:39,246 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.

2020-04-23 09:42:38 2020-04-23 09:42:39,249 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:42:39 2020-04-23 09:42:39,446 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:42:39,461 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:42:39,466 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:42:40 注: /tmp/sqoop-hadoop/compile/1080ff9d1b30180fb436eaafd74bc7b8/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:42:40,384 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/1080ff9d1b30180fb436eaafd74bc7b8/user_info.jar
2020-04-23 09:42:40,391 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:42:40,391 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:42:40,391 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:42:40,391 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:42:40,391 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-23 09:42:40 2020-04-23 09:42:40,398 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:42:40,398 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:42:40 2020-04-23 09:42:40,499 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:42:41 2020-04-23 09:42:41,345 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:42:41,347 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:42:41,348 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:42:41 2020-04-23 09:42:41,752 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0512

2020-04-23 09:42:42 2020-04-23 09:42:43,014 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:42:43,017 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:42:42 2020-04-23 09:42:43,065 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:42:42 2020-04-23 09:42:43,095 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:42:42 2020-04-23 09:42:43,193 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0512
2020-04-23 09:42:43,195 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:42:43 2020-04-23 09:42:43,401 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:42:43,401 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:42:43 2020-04-23 09:42:43,452 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0512

2020-04-23 09:42:43 2020-04-23 09:42:43,477 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0512/
2020-04-23 09:42:43,478 INFO mapreduce.Job: Running job: job_1587346943308_0512

2020-04-23 09:42:49 2020-04-23 09:42:49,564 INFO mapreduce.Job: Job job_1587346943308_0512 running in uber mode : false
2020-04-23 09:42:49,566 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:42:56 2020-04-23 09:42:56,641 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:42:56,652 INFO mapreduce.Job: Job job_1587346943308_0512 completed successfully

2020-04-23 09:42:56 2020-04-23 09:42:56,765 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=155110
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=21126
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=21126
		Total vcore-milliseconds taken by all map tasks=21126
		Total megabyte-milliseconds taken by all map tasks=21633024
	Map-Reduce Framework
		Map input records=1587
		Map output records=1587
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=852
		CPU time s
2020-04-23 09:42:56 pent (ms)=3140
		Physical memory (bytes) snapshot=957517824
		Virtual memory (bytes) snapshot=9788481536
		Total committed heap usage (bytes)=727711744
		Peak Map Physical memory (bytes)=249647104
		Peak Map Virtual memory (bytes)=2450894848
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:42:56,772 INFO mapreduce.ExportJobBase: Transferred 151.4746 KB in 15.4154 seconds (9.8262 KB/sec)
2020-04-23 09:42:56,776 INFO mapreduce.ExportJobBase: Exported 1587 records.

2020-04-23 09:42:56 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:42:56 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=04/d=04/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:42:57 Last login: Thu Apr 23 09:42:38 2020 from 10.10.30.233

2020-04-23 09:42:57 su hadoop

2020-04-23 09:42:57 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:42:57 [hadoop@hadoop-01 root]$ 
2020-04-23 09:42:57 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:42:57 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=04/d=04/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:42:57 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:42:57 2020-04-23 09:42:58,037 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-23 09:42:57 2020-04-23 09:42:58,062 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:42:57 2020-04-23 09:42:58,140 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:42:58,143 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:42:58 2020-04-23 09:42:58,356 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:42:58,393 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:42:58 2020-04-23 09:42:58,399 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:42:59 注: /tmp/sqoop-hadoop/compile/7781e8bf3763c958a3d03b64ab69dc6f/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:42:59,371 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/7781e8bf3763c958a3d03b64ab69dc6f/user_info.jar

2020-04-23 09:42:59 2020-04-23 09:42:59,381 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:42:59,381 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:42:59,381 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:42:59,381 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:42:59,381 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-23 09:42:59 2020-04-23 09:42:59,390 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:42:59,390 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:42:59 2020-04-23 09:42:59,516 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:43:00 2020-04-23 09:43:00,305 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:43:00,307 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:43:00,308 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:43:00 2020-04-23 09:43:00,785 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0513

2020-04-23 09:43:02 2020-04-23 09:43:02,884 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:43:02 2020-04-23 09:43:02,886 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:43:02 2020-04-23 09:43:02,937 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:43:02 2020-04-23 09:43:02,979 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:43:03 2020-04-23 09:43:03,486 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0513
2020-04-23 09:43:03,488 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:43:03 2020-04-23 09:43:03,693 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:43:03,693 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:43:03 2020-04-23 09:43:03,740 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0513

2020-04-23 09:43:03 2020-04-23 09:43:03,765 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0513/
2020-04-23 09:43:03,766 INFO mapreduce.Job: Running job: job_1587346943308_0513

2020-04-23 09:43:09 2020-04-23 09:43:09,841 INFO mapreduce.Job: Job job_1587346943308_0513 running in uber mode : false
2020-04-23 09:43:09,843 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:43:16 2020-04-23 09:43:16,923 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-23 09:43:17 2020-04-23 09:43:17,938 INFO mapreduce.Job: Job job_1587346943308_0513 completed successfully

2020-04-23 09:43:17 2020-04-23 09:43:18,025 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=227741
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20383
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20383
		Total vcore-milliseconds taken by all map tasks=20383
		Total megabyte-milliseconds taken by all map tasks=20872192
	Map-Reduce Framework
		Map input records=2221
		Map output records=2221
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=724
		CPU time s
2020-04-23 09:43:17 pent (ms)=3330
		Physical memory (bytes) snapshot=961511424
		Virtual memory (bytes) snapshot=9778798592
		Total committed heap usage (bytes)=755499008
		Peak Map Physical memory (bytes)=241319936
		Peak Map Virtual memory (bytes)=2446610432
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:43:18,031 INFO mapreduce.ExportJobBase: Transferred 222.4033 KB in 17.7126 seconds (12.5562 KB/sec)
2020-04-23 09:43:18,037 INFO mapreduce.ExportJobBase: Exported 2221 records.

2020-04-23 09:43:18 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:43:18 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=04/d=05/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:43:18 Last login: Thu Apr 23 09:42:57 2020 from 10.10.30.233

2020-04-23 09:43:18 su hadoop

2020-04-23 09:43:18 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:43:18 [hadoop@hadoop-01 root]$ 
2020-04-23 09:43:18 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:43:18 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=04/d=05/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:43:18 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:43:19 2020-04-23 09:43:19,284 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-23 09:43:19,309 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:43:19 2020-04-23 09:43:19,386 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.

2020-04-23 09:43:19 2020-04-23 09:43:19,389 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:43:19 2020-04-23 09:43:19,591 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:43:19,606 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:43:19,612 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:43:20 注: /tmp/sqoop-hadoop/compile/ab77e2dd103f2b6198922772bfa3ef96/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。

2020-04-23 09:43:20 2020-04-23 09:43:20,592 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/ab77e2dd103f2b6198922772bfa3ef96/user_info.jar
2020-04-23 09:43:20,598 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:43:20,598 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:43:20,598 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:43:20,598 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:43:20,598 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-23 09:43:20 2020-04-23 09:43:20,604 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:43:20,604 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:43:20 2020-04-23 09:43:20,709 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:43:21 2020-04-23 09:43:21,451 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:43:21,452 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:43:21 2020-04-23 09:43:21,453 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:43:21 2020-04-23 09:43:21,899 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0514

2020-04-23 09:43:23 2020-04-23 09:43:24,070 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:43:24,072 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:43:23 2020-04-23 09:43:24,121 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:43:23 2020-04-23 09:43:24,165 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:43:24 2020-04-23 09:43:24,257 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0514
2020-04-23 09:43:24,259 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:43:24 2020-04-23 09:43:24,428 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:43:24,429 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-04-23 09:43:24,475 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0514

2020-04-23 09:43:24 2020-04-23 09:43:24,504 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0514/
2020-04-23 09:43:24,505 INFO mapreduce.Job: Running job: job_1587346943308_0514

2020-04-23 09:43:30 2020-04-23 09:43:30,590 INFO mapreduce.Job: Job job_1587346943308_0514 running in uber mode : false
2020-04-23 09:43:30,592 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:43:37 2020-04-23 09:43:37,680 INFO mapreduce.Job:  map 100% reduce 0%

2020-04-23 09:43:37 2020-04-23 09:43:37,692 INFO mapreduce.Job: Job job_1587346943308_0514 completed successfully

2020-04-23 09:43:37 2020-04-23 09:43:37,793 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=147800
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20213
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20213
		Total vcore-milliseconds taken by all map tasks=20213
		Total megabyte-milliseconds taken by all map tasks=20698112
	Map-Reduce Framework
		Map input records=1439
		Map output records=1439
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=809
		CPU time s
2020-04-23 09:43:37 pent (ms)=3080
		Physical memory (bytes) snapshot=940249088
		Virtual memory (bytes) snapshot=9779974144
		Total committed heap usage (bytes)=753401856
		Peak Map Physical memory (bytes)=245526528
		Peak Map Virtual memory (bytes)=2446016512
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:43:37,798 INFO mapreduce.ExportJobBase: Transferred 144.3359 KB in 16.3381 seconds (8.8343 KB/sec)
2020-04-23 09:43:37,801 INFO mapreduce.ExportJobBase: Exported 1439 records.

2020-04-23 09:43:37 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:43:37 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=04/d=06/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:43:38 Last login: Thu Apr 23 09:43:18 2020 from 10.10.30.233

2020-04-23 09:43:38 su hadoop

2020-04-23 09:43:38 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:43:38 [hadoop@hadoop-01 root]$ 
2020-04-23 09:43:38 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:43:38 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=04/d=06/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:43:38 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:43:38 2020-04-23 09:43:39,057 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-23 09:43:39,081 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:43:38 2020-04-23 09:43:39,159 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.

2020-04-23 09:43:38 2020-04-23 09:43:39,162 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:43:39 2020-04-23 09:43:39,367 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:43:39,382 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1
2020-04-23 09:43:39,387 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:43:40 注: /tmp/sqoop-hadoop/compile/10f5f8295ee09761918e0ba822f10639/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:43:40,345 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/10f5f8295ee09761918e0ba822f10639/user_info.jar
2020-04-23 09:43:40,352 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:43:40,352 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:43:40,352 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:43:40,352 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:43:40,352 WARN manager.MySQLManager: documentation for additional limitations.
2020-04-23 09:43:40,358 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:43:40,358 INFO Configuration.deprecation: mapred.j
2020-04-23 09:43:40 ob.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:43:40 2020-04-23 09:43:40,464 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:43:41 2020-04-23 09:43:41,247 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:43:41,249 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:43:41,249 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:43:41 2020-04-23 09:43:41,678 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0515

2020-04-23 09:43:44 2020-04-23 09:43:44,281 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:43:44,284 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:43:44 2020-04-23 09:43:44,342 INFO mapreduce.JobSubmitter: number of splits:4
2020-04-23 09:43:44,370 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:43:44 2020-04-23 09:43:44,864 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0515
2020-04-23 09:43:44,866 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:43:44 2020-04-23 09:43:45,084 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:43:45,085 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:43:44 2020-04-23 09:43:45,140 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0515

2020-04-23 09:43:44 2020-04-23 09:43:45,174 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0515/
2020-04-23 09:43:45,174 INFO mapreduce.Job: Running job: job_1587346943308_0515

2020-04-23 09:43:51 2020-04-23 09:43:51,260 INFO mapreduce.Job: Job job_1587346943308_0515 running in uber mode : false
2020-04-23 09:43:51,262 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:43:58 2020-04-23 09:43:58,454 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:43:58,464 INFO mapreduce.Job: Job job_1587346943308_0515 completed successfully

2020-04-23 09:43:58 2020-04-23 09:43:58,579 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=127004
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20074
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20074
		Total vcore-milliseconds taken by all map tasks=20074
		Total megabyte-milliseconds taken by all map tasks=20555776
	Map-Reduce Framework
		Map input records=1236
		Map output records=1236
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=809
		CPU time s
2020-04-23 09:43:58 pent (ms)=3190
		Physical memory (bytes) snapshot=947888128
		Virtual memory (bytes) snapshot=9781374976
		Total committed heap usage (bytes)=734003200
		Peak Map Physical memory (bytes)=247005184
		Peak Map Virtual memory (bytes)=2446336000
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:43:58,586 INFO mapreduce.ExportJobBase: Transferred 124.0273 KB in 17.3275 seconds (7.1578 KB/sec)
2020-04-23 09:43:58,589 INFO mapreduce.ExportJobBase: Exported 1236 records.

2020-04-23 09:43:58 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:43:58 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=04/d=07/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:43:58 Last login: Thu Apr 23 09:43:38 2020 from 10.10.30.233

2020-04-23 09:43:58 su hadoop

2020-04-23 09:43:59 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:43:59 [hadoop@hadoop-01 root]$ 
2020-04-23 09:43:59 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:43:59 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=04/d=07/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:43:59 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:43:59 2020-04-23 09:43:59,867 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
2020-04-23 09:43:59,894 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:43:59 2020-04-23 09:43:59,973 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:43:59,977 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:43:59 2020-04-23 09:44:00,180 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:43:59 2020-04-23 09:44:00,207 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:43:59 2020-04-23 09:44:00,214 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:44:00 注: /tmp/sqoop-hadoop/compile/e1b47aed79f13474d7133b84038392de/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。

2020-04-23 09:44:00 2020-04-23 09:44:01,193 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/e1b47aed79f13474d7133b84038392de/user_info.jar
2020-04-23 09:44:01,201 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:44:01,201 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:44:01,201 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:44:01,201 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:44:01,201 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-23 09:44:00 2020-04-23 09:44:01,207 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:44:01,207 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:44:01 2020-04-23 09:44:01,314 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:44:01 2020-04-23 09:44:02,078 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:44:02,080 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:44:02,080 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:44:02 2020-04-23 09:44:02,525 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0516

2020-04-23 09:44:03 2020-04-23 09:44:03,781 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:44:03,784 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:44:03 2020-04-23 09:44:03,838 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:44:03 2020-04-23 09:44:03,869 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:44:04 2020-04-23 09:44:04,374 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0516
2020-04-23 09:44:04,376 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:44:04 2020-04-23 09:44:04,582 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:44:04,582 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:44:04 2020-04-23 09:44:04,635 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0516

2020-04-23 09:44:04 2020-04-23 09:44:04,663 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0516/
2020-04-23 09:44:04,663 INFO mapreduce.Job: Running job: job_1587346943308_0516

2020-04-23 09:44:10 2020-04-23 09:44:10,746 INFO mapreduce.Job: Job job_1587346943308_0516 running in uber mode : false
2020-04-23 09:44:10,748 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:44:17 2020-04-23 09:44:17,830 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:44:17,840 INFO mapreduce.Job: Job job_1587346943308_0516 completed successfully

2020-04-23 09:44:17 2020-04-23 09:44:17,957 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=114860
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20312
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20312
		Total vcore-milliseconds taken by all map tasks=20312
		Total megabyte-milliseconds taken by all map tasks=20799488
	Map-Reduce Framework
		Map input records=1117
		Map output records=1117
		Input split bytes=696
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=729
		CPU time s
2020-04-23 09:44:17 pent (ms)=3150
		Physical memory (bytes) snapshot=944709632
		Virtual memory (bytes) snapshot=9788321792
		Total committed heap usage (bytes)=729808896
		Peak Map Physical memory (bytes)=243290112
		Peak Map Virtual memory (bytes)=2448814080
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:44:17,964 INFO mapreduce.ExportJobBase: Transferred 112.168 KB in 15.8756 seconds (7.0654 KB/sec)
2020-04-23 09:44:17,967 INFO mapreduce.ExportJobBase: Exported 1117 records.

2020-04-23 09:44:18 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:44:18 sqoop export \
--connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
--username kfyw \
--password 123456 \
--export-dir /data/hive/user/page_add/y=2020/m=04/d=08/ \
--table user_info \
--columns parent_column_id,user_type,user_id,area_code,create_time \
--update-key parent_column_id,user_type,user_id \
--update-mode allowinsert \
-input-fields-terminated-by "\t"
2020-04-23 09:44:18 Last login: Thu Apr 23 09:43:59 2020 from 10.10.30.233

2020-04-23 09:44:18 su hadoop

2020-04-23 09:44:18 [root@hadoop-01 ~]# su hadoop

2020-04-23 09:44:18 [hadoop@hadoop-01 root]$ 
2020-04-23 09:44:18 cd /home/app/test/sx_hadoop_script/
[hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:44:18 sqoop export \
> --connect jdbc:mysql://192.168.15.50:3306/sx_hadoop?useUnicode=true \
> --username kfyw \
> --password 123456 \
> --export-dir /data/hive/user/page_add/y=2020/m=04/d=08/ \
> --table user_info \
> --columns parent_column_id,user_type,user_id,area_code,create_time \
> --update-key parent_column_id,user_type,user_id \
> --update-mode allowinsert \
> -input-fields-terminated-by "\t"

2020-04-23 09:44:18 Warning: /usr/local/sqoop-1.4.7/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop-1.4.7/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop-1.4.7/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop-1.4.7/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.

2020-04-23 09:44:18 2020-04-23 09:44:19,195 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7

2020-04-23 09:44:18 2020-04-23 09:44:19,222 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.

2020-04-23 09:44:19 2020-04-23 09:44:19,305 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
2020-04-23 09:44:19,308 INFO tool.CodeGenTool: Beginning code generation

2020-04-23 09:44:19 2020-04-23 09:44:19,550 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:44:19 2020-04-23 09:44:19,565 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user_info` AS t LIMIT 1

2020-04-23 09:44:19 2020-04-23 09:44:19,571 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop-3.1.2

2020-04-23 09:44:20 注: /tmp/sqoop-hadoop/compile/6f708f374316f08933508d126e3be491/user_info.java使用或覆盖了已过时的 API。
注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
2020-04-23 09:44:20,544 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/6f708f374316f08933508d126e3be491/user_info.jar

2020-04-23 09:44:20 2020-04-23 09:44:20,552 WARN manager.MySQLManager: MySQL Connector upsert functionality is using INSERT ON
2020-04-23 09:44:20,552 WARN manager.MySQLManager: DUPLICATE KEY UPDATE clause that relies on table's unique key.
2020-04-23 09:44:20,552 WARN manager.MySQLManager: Insert/update distinction is therefore independent on column
2020-04-23 09:44:20,552 WARN manager.MySQLManager: names specified in --update-key parameter. Please see MySQL
2020-04-23 09:44:20,552 WARN manager.MySQLManager: documentation for additional limitations.

2020-04-23 09:44:20 2020-04-23 09:44:20,558 INFO mapreduce.ExportJobBase: Beginning export of user_info
2020-04-23 09:44:20,558 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address

2020-04-23 09:44:20 2020-04-23 09:44:20,662 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar

2020-04-23 09:44:21 2020-04-23 09:44:21,473 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2020-04-23 09:44:21,475 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
2020-04-23 09:44:21,475 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

2020-04-23 09:44:21 2020-04-23 09:44:21,901 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587346943308_0517

2020-04-23 09:44:22 2020-04-23 09:44:23,212 INFO input.FileInputFormat: Total input files to process : 1
2020-04-23 09:44:23,215 INFO input.FileInputFormat: Total input files to process : 1

2020-04-23 09:44:23 2020-04-23 09:44:23,264 INFO mapreduce.JobSubmitter: number of splits:4

2020-04-23 09:44:23 2020-04-23 09:44:23,292 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative

2020-04-23 09:44:23 2020-04-23 09:44:23,398 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587346943308_0517
2020-04-23 09:44:23,400 INFO mapreduce.JobSubmitter: Executing with tokens: []

2020-04-23 09:44:23 2020-04-23 09:44:23,590 INFO conf.Configuration: resource-types.xml not found
2020-04-23 09:44:23,590 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.

2020-04-23 09:44:23 2020-04-23 09:44:23,653 INFO impl.YarnClientImpl: Submitted application application_1587346943308_0517

2020-04-23 09:44:23 2020-04-23 09:44:23,689 INFO mapreduce.Job: The url to track the job: http://hadoop-01:8088/proxy/application_1587346943308_0517/
2020-04-23 09:44:23,689 INFO mapreduce.Job: Running job: job_1587346943308_0517

2020-04-23 09:44:29 2020-04-23 09:44:29,760 INFO mapreduce.Job: Job job_1587346943308_0517 running in uber mode : false
2020-04-23 09:44:29,762 INFO mapreduce.Job:  map 0% reduce 0%

2020-04-23 09:44:36 2020-04-23 09:44:36,839 INFO mapreduce.Job:  map 100% reduce 0%
2020-04-23 09:44:36,847 INFO mapreduce.Job: Job job_1587346943308_0517 completed successfully

2020-04-23 09:44:36 2020-04-23 09:44:36,952 INFO mapreduce.Job: Counters: 32
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=928436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=108830
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20446
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20446
		Total vcore-milliseconds taken by all map tasks=20446
		Total megabyte-milliseconds taken by all map tasks=20936704
	Map-Reduce Framework
		Map input records=1112
		Map output records=1112
		Input split bytes=608
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=734
		CPU time s
2020-04-23 09:44:36 pent (ms)=3090
		Physical memory (bytes) snapshot=964366336
		Virtual memory (bytes) snapshot=9778143232
		Total committed heap usage (bytes)=748683264
		Peak Map Physical memory (bytes)=244080640
		Peak Map Virtual memory (bytes)=2446282752
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2020-04-23 09:44:36,959 INFO mapreduce.ExportJobBase: Transferred 106.2793 KB in 15.4746 seconds (6.868 KB/sec)
2020-04-23 09:44:36,963 INFO mapreduce.ExportJobBase: Exported 1112 records.

2020-04-23 09:44:37 [hadoop@hadoop-01 sx_hadoop_script]$ 
2020-04-23 09:44:37 日统计数据已完成
